{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 3*32*32 \n",
    "num_classes = 100\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifardataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cifardataset.CIFAR100(root='./data2', \n",
    "                                           coarse=True,\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = cifardataset.CIFAR100(root='./data2', \n",
    "                                          coarse=True,\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_9 = [i for i in train_dataset if i[2] == 9]\n",
    "test_dataset_9 = [i for i in test_dataset if i[2] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_9, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_9, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model, self).__init__() #calls nn.Module init()\n",
    "        #self.linear = nn.Linear (input_size, num_classes)\n",
    "        #Z = WtX + b\n",
    "        #input_size is the shape of X\n",
    "        #num_classes is the shape of Z\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, num_classes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "model = Model(input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "Epoch [1/10], Step [1/25], Loss: 1.6093\n",
      "h\n",
      "Epoch [1/10], Step [2/25], Loss: 1.6024\n",
      "h\n",
      "Epoch [1/10], Step [3/25], Loss: 1.6364\n",
      "h\n",
      "Epoch [1/10], Step [4/25], Loss: 1.5878\n",
      "h\n",
      "Epoch [1/10], Step [5/25], Loss: 1.6098\n",
      "h\n",
      "Epoch [1/10], Step [6/25], Loss: 1.5908\n",
      "h\n",
      "Epoch [1/10], Step [7/25], Loss: 1.6142\n",
      "h\n",
      "Epoch [1/10], Step [8/25], Loss: 1.5827\n",
      "h\n",
      "Epoch [1/10], Step [9/25], Loss: 1.5807\n",
      "h\n",
      "Epoch [1/10], Step [10/25], Loss: 1.5968\n",
      "h\n",
      "Epoch [1/10], Step [11/25], Loss: 1.5691\n",
      "h\n",
      "Epoch [1/10], Step [12/25], Loss: 1.5782\n",
      "h\n",
      "Epoch [1/10], Step [13/25], Loss: 1.5940\n",
      "h\n",
      "Epoch [1/10], Step [14/25], Loss: 1.5477\n",
      "h\n",
      "Epoch [1/10], Step [15/25], Loss: 1.5490\n",
      "h\n",
      "Epoch [1/10], Step [16/25], Loss: 1.5117\n",
      "h\n",
      "Epoch [1/10], Step [17/25], Loss: 1.6047\n",
      "h\n",
      "Epoch [1/10], Step [18/25], Loss: 1.5890\n",
      "h\n",
      "Epoch [1/10], Step [19/25], Loss: 1.5253\n",
      "h\n",
      "Epoch [1/10], Step [20/25], Loss: 1.5540\n",
      "h\n",
      "Epoch [1/10], Step [21/25], Loss: 1.5614\n",
      "h\n",
      "Epoch [1/10], Step [22/25], Loss: 1.5314\n",
      "h\n",
      "Epoch [1/10], Step [23/25], Loss: 1.5194\n",
      "h\n",
      "Epoch [1/10], Step [24/25], Loss: 1.5348\n",
      "h\n",
      "Epoch [1/10], Step [25/25], Loss: 1.5381\n",
      "h\n",
      "Epoch [2/10], Step [1/25], Loss: 1.4888\n",
      "h\n",
      "Epoch [2/10], Step [2/25], Loss: 1.4802\n",
      "h\n",
      "Epoch [2/10], Step [3/25], Loss: 1.4969\n",
      "h\n",
      "Epoch [2/10], Step [4/25], Loss: 1.5352\n",
      "h\n",
      "Epoch [2/10], Step [5/25], Loss: 1.5212\n",
      "h\n",
      "Epoch [2/10], Step [6/25], Loss: 1.4702\n",
      "h\n",
      "Epoch [2/10], Step [7/25], Loss: 1.5361\n",
      "h\n",
      "Epoch [2/10], Step [8/25], Loss: 1.4942\n",
      "h\n",
      "Epoch [2/10], Step [9/25], Loss: 1.5104\n",
      "h\n",
      "Epoch [2/10], Step [10/25], Loss: 1.5093\n",
      "h\n",
      "Epoch [2/10], Step [11/25], Loss: 1.5516\n",
      "h\n",
      "Epoch [2/10], Step [12/25], Loss: 1.5275\n",
      "h\n",
      "Epoch [2/10], Step [13/25], Loss: 1.4979\n",
      "h\n",
      "Epoch [2/10], Step [14/25], Loss: 1.4886\n",
      "h\n",
      "Epoch [2/10], Step [15/25], Loss: 1.5209\n",
      "h\n",
      "Epoch [2/10], Step [16/25], Loss: 1.4770\n",
      "h\n",
      "Epoch [2/10], Step [17/25], Loss: 1.5318\n",
      "h\n",
      "Epoch [2/10], Step [18/25], Loss: 1.4044\n",
      "h\n",
      "Epoch [2/10], Step [19/25], Loss: 1.4827\n",
      "h\n",
      "Epoch [2/10], Step [20/25], Loss: 1.4700\n",
      "h\n",
      "Epoch [2/10], Step [21/25], Loss: 1.4796\n",
      "h\n",
      "Epoch [2/10], Step [22/25], Loss: 1.4674\n",
      "h\n",
      "Epoch [2/10], Step [23/25], Loss: 1.4966\n",
      "h\n",
      "Epoch [2/10], Step [24/25], Loss: 1.5216\n",
      "h\n",
      "Epoch [2/10], Step [25/25], Loss: 1.5100\n",
      "h\n",
      "Epoch [3/10], Step [1/25], Loss: 1.5216\n",
      "h\n",
      "Epoch [3/10], Step [2/25], Loss: 1.4697\n",
      "h\n",
      "Epoch [3/10], Step [3/25], Loss: 1.4492\n",
      "h\n",
      "Epoch [3/10], Step [4/25], Loss: 1.4515\n",
      "h\n",
      "Epoch [3/10], Step [5/25], Loss: 1.4927\n",
      "h\n",
      "Epoch [3/10], Step [6/25], Loss: 1.4776\n",
      "h\n",
      "Epoch [3/10], Step [7/25], Loss: 1.4664\n",
      "h\n",
      "Epoch [3/10], Step [8/25], Loss: 1.4771\n",
      "h\n",
      "Epoch [3/10], Step [9/25], Loss: 1.4403\n",
      "h\n",
      "Epoch [3/10], Step [10/25], Loss: 1.4508\n",
      "h\n",
      "Epoch [3/10], Step [11/25], Loss: 1.4686\n",
      "h\n",
      "Epoch [3/10], Step [12/25], Loss: 1.5270\n",
      "h\n",
      "Epoch [3/10], Step [13/25], Loss: 1.4713\n",
      "h\n",
      "Epoch [3/10], Step [14/25], Loss: 1.4562\n",
      "h\n",
      "Epoch [3/10], Step [15/25], Loss: 1.4502\n",
      "h\n",
      "Epoch [3/10], Step [16/25], Loss: 1.4727\n",
      "h\n",
      "Epoch [3/10], Step [17/25], Loss: 1.4096\n",
      "h\n",
      "Epoch [3/10], Step [18/25], Loss: 1.4355\n",
      "h\n",
      "Epoch [3/10], Step [19/25], Loss: 1.3937\n",
      "h\n",
      "Epoch [3/10], Step [20/25], Loss: 1.4615\n",
      "h\n",
      "Epoch [3/10], Step [21/25], Loss: 1.4571\n",
      "h\n",
      "Epoch [3/10], Step [22/25], Loss: 1.3793\n",
      "h\n",
      "Epoch [3/10], Step [23/25], Loss: 1.4170\n",
      "h\n",
      "Epoch [3/10], Step [24/25], Loss: 1.4437\n",
      "h\n",
      "Epoch [3/10], Step [25/25], Loss: 1.4316\n",
      "h\n",
      "Epoch [4/10], Step [1/25], Loss: 1.4568\n",
      "h\n",
      "Epoch [4/10], Step [2/25], Loss: 1.4221\n",
      "h\n",
      "Epoch [4/10], Step [3/25], Loss: 1.4230\n",
      "h\n",
      "Epoch [4/10], Step [4/25], Loss: 1.4336\n",
      "h\n",
      "Epoch [4/10], Step [5/25], Loss: 1.4345\n",
      "h\n",
      "Epoch [4/10], Step [6/25], Loss: 1.4294\n",
      "h\n",
      "Epoch [4/10], Step [7/25], Loss: 1.4269\n",
      "h\n",
      "Epoch [4/10], Step [8/25], Loss: 1.3822\n",
      "h\n",
      "Epoch [4/10], Step [9/25], Loss: 1.4385\n",
      "h\n",
      "Epoch [4/10], Step [10/25], Loss: 1.3894\n",
      "h\n",
      "Epoch [4/10], Step [11/25], Loss: 1.4559\n",
      "h\n",
      "Epoch [4/10], Step [12/25], Loss: 1.4338\n",
      "h\n",
      "Epoch [4/10], Step [13/25], Loss: 1.4441\n",
      "h\n",
      "Epoch [4/10], Step [14/25], Loss: 1.4608\n",
      "h\n",
      "Epoch [4/10], Step [15/25], Loss: 1.4177\n",
      "h\n",
      "Epoch [4/10], Step [16/25], Loss: 1.4121\n",
      "h\n",
      "Epoch [4/10], Step [17/25], Loss: 1.4243\n",
      "h\n",
      "Epoch [4/10], Step [18/25], Loss: 1.4027\n",
      "h\n",
      "Epoch [4/10], Step [19/25], Loss: 1.4209\n",
      "h\n",
      "Epoch [4/10], Step [20/25], Loss: 1.4313\n",
      "h\n",
      "Epoch [4/10], Step [21/25], Loss: 1.3924\n",
      "h\n",
      "Epoch [4/10], Step [22/25], Loss: 1.3453\n",
      "h\n",
      "Epoch [4/10], Step [23/25], Loss: 1.4328\n",
      "h\n",
      "Epoch [4/10], Step [24/25], Loss: 1.4365\n",
      "h\n",
      "Epoch [4/10], Step [25/25], Loss: 1.5085\n",
      "h\n",
      "Epoch [5/10], Step [1/25], Loss: 1.4064\n",
      "h\n",
      "Epoch [5/10], Step [2/25], Loss: 1.4113\n",
      "h\n",
      "Epoch [5/10], Step [3/25], Loss: 1.4812\n",
      "h\n",
      "Epoch [5/10], Step [4/25], Loss: 1.4054\n",
      "h\n",
      "Epoch [5/10], Step [5/25], Loss: 1.3981\n",
      "h\n",
      "Epoch [5/10], Step [6/25], Loss: 1.3630\n",
      "h\n",
      "Epoch [5/10], Step [7/25], Loss: 1.4296\n",
      "h\n",
      "Epoch [5/10], Step [8/25], Loss: 1.4713\n",
      "h\n",
      "Epoch [5/10], Step [9/25], Loss: 1.4188\n",
      "h\n",
      "Epoch [5/10], Step [10/25], Loss: 1.4272\n",
      "h\n",
      "Epoch [5/10], Step [11/25], Loss: 1.3991\n",
      "h\n",
      "Epoch [5/10], Step [12/25], Loss: 1.4878\n",
      "h\n",
      "Epoch [5/10], Step [13/25], Loss: 1.3565\n",
      "h\n",
      "Epoch [5/10], Step [14/25], Loss: 1.4517\n",
      "h\n",
      "Epoch [5/10], Step [15/25], Loss: 1.4032\n",
      "h\n",
      "Epoch [5/10], Step [16/25], Loss: 1.4312\n",
      "h\n",
      "Epoch [5/10], Step [17/25], Loss: 1.3916\n",
      "h\n",
      "Epoch [5/10], Step [18/25], Loss: 1.4448\n",
      "h\n",
      "Epoch [5/10], Step [19/25], Loss: 1.3945\n",
      "h\n",
      "Epoch [5/10], Step [20/25], Loss: 1.4562\n",
      "h\n",
      "Epoch [5/10], Step [21/25], Loss: 1.4250\n",
      "h\n",
      "Epoch [5/10], Step [22/25], Loss: 1.4181\n",
      "h\n",
      "Epoch [5/10], Step [23/25], Loss: 1.3803\n",
      "h\n",
      "Epoch [5/10], Step [24/25], Loss: 1.4243\n",
      "h\n",
      "Epoch [5/10], Step [25/25], Loss: 1.4337\n",
      "h\n",
      "Epoch [6/10], Step [1/25], Loss: 1.3127\n",
      "h\n",
      "Epoch [6/10], Step [2/25], Loss: 1.4376\n",
      "h\n",
      "Epoch [6/10], Step [3/25], Loss: 1.4330\n",
      "h\n",
      "Epoch [6/10], Step [4/25], Loss: 1.4393\n",
      "h\n",
      "Epoch [6/10], Step [5/25], Loss: 1.4432\n",
      "h\n",
      "Epoch [6/10], Step [6/25], Loss: 1.4186\n",
      "h\n",
      "Epoch [6/10], Step [7/25], Loss: 1.3824\n",
      "h\n",
      "Epoch [6/10], Step [8/25], Loss: 1.4041\n",
      "h\n",
      "Epoch [6/10], Step [9/25], Loss: 1.4167\n",
      "h\n",
      "Epoch [6/10], Step [10/25], Loss: 1.3314\n",
      "h\n",
      "Epoch [6/10], Step [11/25], Loss: 1.4195\n",
      "h\n",
      "Epoch [6/10], Step [12/25], Loss: 1.3426\n",
      "h\n",
      "Epoch [6/10], Step [13/25], Loss: 1.4141\n",
      "h\n",
      "Epoch [6/10], Step [14/25], Loss: 1.4318\n",
      "h\n",
      "Epoch [6/10], Step [15/25], Loss: 1.3304\n",
      "h\n",
      "Epoch [6/10], Step [16/25], Loss: 1.4153\n",
      "h\n",
      "Epoch [6/10], Step [17/25], Loss: 1.3812\n",
      "h\n",
      "Epoch [6/10], Step [18/25], Loss: 1.3450\n",
      "h\n",
      "Epoch [6/10], Step [19/25], Loss: 1.4285\n",
      "h\n",
      "Epoch [6/10], Step [20/25], Loss: 1.2583\n",
      "h\n",
      "Epoch [6/10], Step [21/25], Loss: 1.3431\n",
      "h\n",
      "Epoch [6/10], Step [22/25], Loss: 1.5459\n",
      "h\n",
      "Epoch [6/10], Step [23/25], Loss: 1.3598\n",
      "h\n",
      "Epoch [6/10], Step [24/25], Loss: 1.3937\n",
      "h\n",
      "Epoch [6/10], Step [25/25], Loss: 1.3383\n",
      "h\n",
      "Epoch [7/10], Step [1/25], Loss: 1.3824\n",
      "h\n",
      "Epoch [7/10], Step [2/25], Loss: 1.3929\n",
      "h\n",
      "Epoch [7/10], Step [3/25], Loss: 1.3981\n",
      "h\n",
      "Epoch [7/10], Step [4/25], Loss: 1.3529\n",
      "h\n",
      "Epoch [7/10], Step [5/25], Loss: 1.3516\n",
      "h\n",
      "Epoch [7/10], Step [6/25], Loss: 1.4013\n",
      "h\n",
      "Epoch [7/10], Step [7/25], Loss: 1.3987\n",
      "h\n",
      "Epoch [7/10], Step [8/25], Loss: 1.3421\n",
      "h\n",
      "Epoch [7/10], Step [9/25], Loss: 1.3355\n",
      "h\n",
      "Epoch [7/10], Step [10/25], Loss: 1.4082\n",
      "h\n",
      "Epoch [7/10], Step [11/25], Loss: 1.3547\n",
      "h\n",
      "Epoch [7/10], Step [12/25], Loss: 1.3522\n",
      "h\n",
      "Epoch [7/10], Step [13/25], Loss: 1.3179\n",
      "h\n",
      "Epoch [7/10], Step [14/25], Loss: 1.4000\n",
      "h\n",
      "Epoch [7/10], Step [15/25], Loss: 1.3808\n",
      "h\n",
      "Epoch [7/10], Step [16/25], Loss: 1.4225\n",
      "h\n",
      "Epoch [7/10], Step [17/25], Loss: 1.4361\n",
      "h\n",
      "Epoch [7/10], Step [18/25], Loss: 1.4177\n",
      "h\n",
      "Epoch [7/10], Step [19/25], Loss: 1.3337\n",
      "h\n",
      "Epoch [7/10], Step [20/25], Loss: 1.4279\n",
      "h\n",
      "Epoch [7/10], Step [21/25], Loss: 1.4429\n",
      "h\n",
      "Epoch [7/10], Step [22/25], Loss: 1.3067\n",
      "h\n",
      "Epoch [7/10], Step [23/25], Loss: 1.3597\n",
      "h\n",
      "Epoch [7/10], Step [24/25], Loss: 1.3624\n",
      "h\n",
      "Epoch [7/10], Step [25/25], Loss: 1.4158\n",
      "h\n",
      "Epoch [8/10], Step [1/25], Loss: 1.4952\n",
      "h\n",
      "Epoch [8/10], Step [2/25], Loss: 1.3873\n",
      "h\n",
      "Epoch [8/10], Step [3/25], Loss: 1.3525\n",
      "h\n",
      "Epoch [8/10], Step [4/25], Loss: 1.4233\n",
      "h\n",
      "Epoch [8/10], Step [5/25], Loss: 1.4344\n",
      "h\n",
      "Epoch [8/10], Step [6/25], Loss: 1.3540\n",
      "h\n",
      "Epoch [8/10], Step [7/25], Loss: 1.3379\n",
      "h\n",
      "Epoch [8/10], Step [8/25], Loss: 1.4042\n",
      "h\n",
      "Epoch [8/10], Step [9/25], Loss: 1.4286\n",
      "h\n",
      "Epoch [8/10], Step [10/25], Loss: 1.4284\n",
      "h\n",
      "Epoch [8/10], Step [11/25], Loss: 1.3794\n",
      "h\n",
      "Epoch [8/10], Step [12/25], Loss: 1.4315\n",
      "h\n",
      "Epoch [8/10], Step [13/25], Loss: 1.4007\n",
      "h\n",
      "Epoch [8/10], Step [14/25], Loss: 1.4075\n",
      "h\n",
      "Epoch [8/10], Step [15/25], Loss: 1.4062\n",
      "h\n",
      "Epoch [8/10], Step [16/25], Loss: 1.3565\n",
      "h\n",
      "Epoch [8/10], Step [17/25], Loss: 1.3475\n",
      "h\n",
      "Epoch [8/10], Step [18/25], Loss: 1.4342\n",
      "h\n",
      "Epoch [8/10], Step [19/25], Loss: 1.3248\n",
      "h\n",
      "Epoch [8/10], Step [20/25], Loss: 1.3625\n",
      "h\n",
      "Epoch [8/10], Step [21/25], Loss: 1.3610\n",
      "h\n",
      "Epoch [8/10], Step [22/25], Loss: 1.3420\n",
      "h\n",
      "Epoch [8/10], Step [23/25], Loss: 1.3209\n",
      "h\n",
      "Epoch [8/10], Step [24/25], Loss: 1.4061\n",
      "h\n",
      "Epoch [8/10], Step [25/25], Loss: 1.3360\n",
      "h\n",
      "Epoch [9/10], Step [1/25], Loss: 1.2894\n",
      "h\n",
      "Epoch [9/10], Step [2/25], Loss: 1.3513\n",
      "h\n",
      "Epoch [9/10], Step [3/25], Loss: 1.3280\n",
      "h\n",
      "Epoch [9/10], Step [4/25], Loss: 1.3599\n",
      "h\n",
      "Epoch [9/10], Step [5/25], Loss: 1.3693\n",
      "h\n",
      "Epoch [9/10], Step [6/25], Loss: 1.3685\n",
      "h\n",
      "Epoch [9/10], Step [7/25], Loss: 1.3947\n",
      "h\n",
      "Epoch [9/10], Step [8/25], Loss: 1.3530\n",
      "h\n",
      "Epoch [9/10], Step [9/25], Loss: 1.3935\n",
      "h\n",
      "Epoch [9/10], Step [10/25], Loss: 1.3995\n",
      "h\n",
      "Epoch [9/10], Step [11/25], Loss: 1.3474\n",
      "h\n",
      "Epoch [9/10], Step [12/25], Loss: 1.3583\n",
      "h\n",
      "Epoch [9/10], Step [13/25], Loss: 1.3807\n",
      "h\n",
      "Epoch [9/10], Step [14/25], Loss: 1.3268\n",
      "h\n",
      "Epoch [9/10], Step [15/25], Loss: 1.4159\n",
      "h\n",
      "Epoch [9/10], Step [16/25], Loss: 1.3717\n",
      "h\n",
      "Epoch [9/10], Step [17/25], Loss: 1.2413\n",
      "h\n",
      "Epoch [9/10], Step [18/25], Loss: 1.3748\n",
      "h\n",
      "Epoch [9/10], Step [19/25], Loss: 1.3334\n",
      "h\n",
      "Epoch [9/10], Step [20/25], Loss: 1.3577\n",
      "h\n",
      "Epoch [9/10], Step [21/25], Loss: 1.3863\n",
      "h\n",
      "Epoch [9/10], Step [22/25], Loss: 1.4008\n",
      "h\n",
      "Epoch [9/10], Step [23/25], Loss: 1.3291\n",
      "h\n",
      "Epoch [9/10], Step [24/25], Loss: 1.3562\n",
      "h\n",
      "Epoch [9/10], Step [25/25], Loss: 1.4023\n",
      "h\n",
      "Epoch [10/10], Step [1/25], Loss: 1.3451\n",
      "h\n",
      "Epoch [10/10], Step [2/25], Loss: 1.3529\n",
      "h\n",
      "Epoch [10/10], Step [3/25], Loss: 1.4010\n",
      "h\n",
      "Epoch [10/10], Step [4/25], Loss: 1.3590\n",
      "h\n",
      "Epoch [10/10], Step [5/25], Loss: 1.3085\n",
      "h\n",
      "Epoch [10/10], Step [6/25], Loss: 1.3644\n",
      "h\n",
      "Epoch [10/10], Step [7/25], Loss: 1.3045\n",
      "h\n",
      "Epoch [10/10], Step [8/25], Loss: 1.3823\n",
      "h\n",
      "Epoch [10/10], Step [9/25], Loss: 1.3706\n",
      "h\n",
      "Epoch [10/10], Step [10/25], Loss: 1.3592\n",
      "h\n",
      "Epoch [10/10], Step [11/25], Loss: 1.3302\n",
      "h\n",
      "Epoch [10/10], Step [12/25], Loss: 1.3045\n",
      "h\n",
      "Epoch [10/10], Step [13/25], Loss: 1.3541\n",
      "h\n",
      "Epoch [10/10], Step [14/25], Loss: 1.3122\n",
      "h\n",
      "Epoch [10/10], Step [15/25], Loss: 1.4301\n",
      "h\n",
      "Epoch [10/10], Step [16/25], Loss: 1.3284\n",
      "h\n",
      "Epoch [10/10], Step [17/25], Loss: 1.3064\n",
      "h\n",
      "Epoch [10/10], Step [18/25], Loss: 1.4053\n",
      "h\n",
      "Epoch [10/10], Step [19/25], Loss: 1.3482\n",
      "h\n",
      "Epoch [10/10], Step [20/25], Loss: 1.3160\n",
      "h\n",
      "Epoch [10/10], Step [21/25], Loss: 1.2833\n",
      "h\n",
      "Epoch [10/10], Step [22/25], Loss: 1.2908\n",
      "h\n",
      "Epoch [10/10], Step [23/25], Loss: 1.4301\n",
      "h\n",
      "Epoch [10/10], Step [24/25], Loss: 1.2976\n",
      "h\n",
      "Epoch [10/10], Step [25/25], Loss: 1.3885\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "figure = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels, coarse_label) in enumerate(train_loader):  \n",
    "        epochVec = []\n",
    "        epochLoss = []\n",
    "\n",
    "        print ('h')\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels[labels == 12] = 0\n",
    "        labels[labels == 17] = 1\n",
    "        labels[labels == 37] = 2\n",
    "        labels[labels == 68] = 3\n",
    "        labels[labels == 76] = 4\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion (outputs, labels)\n",
    "        epochLoss.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() #.step = update rule we rote \n",
    "        \n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epochAvg = sum(epochLoss)/len(epochLoss)\n",
    "    epochVec = [(epoch + 1), epochAvg]\n",
    "    figure.append(epochVec)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 1]\n",
      " [4 8]]\n",
      "Precision for the 500 test images: 88.88888888888889 %\n",
      "Accuracy for the 500 test images: 49.0 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels, coarse_label in test_loader:\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels[labels == 12] = 0\n",
    "        labels[labels == 17] = 1\n",
    "        labels[labels == 37] = 2\n",
    "        labels[labels == 68] = 3\n",
    "        labels[labels == 76] = 4\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1) # max returns (value ,index)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    confusion = confusion_matrix(labels,predicted, labels=[0,1])\n",
    "    tp = confusion[1][1]\n",
    "    fp = confusion[0][1]\n",
    "    fn = confusion[1][0]\n",
    "    tn = confusion[0][0]\n",
    "    print(confusion)\n",
    "    precision = tp/(tp+fp) *100\n",
    "    \n",
    "\n",
    "    print(f'Precision for the 500 test images: {precision} %')\n",
    "    print(f'Accuracy for the 500 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.        ,  3.        ,  4.        ,  5.        ,\n",
       "         6.        ,  7.        ,  8.        ,  9.        , 10.        ],\n",
       "       [ 1.53806591,  1.50998199,  1.4316467 ,  1.50850403,  1.43372416,\n",
       "         1.33828843,  1.41584611,  1.33598125,  1.4023    ,  1.38846576]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure = np.array(figure)\n",
    "figure = figure.transpose()\n",
    "figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10.0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(figure[0], figure[1])\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss by Epoch\")\n",
    "plt.xlim([0,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_classes = 100\n",
    "num_epochs = 10\n",
    "batch_size = 4 #mini-batch gradient descent\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifardataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cifardataset.CIFAR100(root='./data2', \n",
    "                                           coarse=True,\n",
    "                                           train=True, \n",
    "                                           transform=transform,  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = cifardataset.CIFAR100(root='./data2', \n",
    "                                          coarse=True,\n",
    "                                          train=False, \n",
    "                                          transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_9 = [i for i in train_dataset if i[2] == 9]\n",
    "test_dataset_9 = [i for i in test_dataset if i[2] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_9, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_9, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,8,5) #(in-channels, out-channels, kernel size)\n",
    "        self.fc1 = nn.Linear(8*28*28,1080) #calclate 8*28*28 on own, 8 filters, result = same as example \n",
    "        self.fc2 = nn.Linear(1080,100)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, 8*28*28) #tensor operation of reshape (-1, N)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of [0]: 17.0 %\n",
      "Accuracy of [1]: 88.0 %\n",
      "Accuracy of [2]: 13.0 %\n",
      "Accuracy of [3]: 63.0 %\n",
      "Accuracy of [4]: 65.0 %\n",
      "Accuracy of [5]: 0 %\n",
      "Accuracy of [6]: 0 %\n",
      "Accuracy of [7]: 0 %\n",
      "Accuracy of [8]: 0 %\n",
      "Accuracy of [9]: 0 %\n",
      "Accuracy of [10]: 0 %\n",
      "Accuracy of [11]: 0 %\n",
      "Accuracy of [12]: 0 %\n",
      "Accuracy of [13]: 0 %\n",
      "Accuracy of [14]: 0 %\n",
      "Accuracy of [15]: 0 %\n",
      "Accuracy of [16]: 0 %\n",
      "Accuracy of [17]: 0 %\n",
      "Accuracy of [18]: 0 %\n",
      "Accuracy of [19]: 0 %\n",
      "Accuracy of [20]: 0 %\n",
      "Accuracy of [21]: 0 %\n",
      "Accuracy of [22]: 0 %\n",
      "Accuracy of [23]: 0 %\n",
      "Accuracy of [24]: 0 %\n",
      "Accuracy of [25]: 0 %\n",
      "Accuracy of [26]: 0 %\n",
      "Accuracy of [27]: 0 %\n",
      "Accuracy of [28]: 0 %\n",
      "Accuracy of [29]: 0 %\n",
      "Accuracy of [30]: 0 %\n",
      "Accuracy of [31]: 0 %\n",
      "Accuracy of [32]: 0 %\n",
      "Accuracy of [33]: 0 %\n",
      "Accuracy of [34]: 0 %\n",
      "Accuracy of [35]: 0 %\n",
      "Accuracy of [36]: 0 %\n",
      "Accuracy of [37]: 0 %\n",
      "Accuracy of [38]: 0 %\n",
      "Accuracy of [39]: 0 %\n",
      "Accuracy of [40]: 0 %\n",
      "Accuracy of [41]: 0 %\n",
      "Accuracy of [42]: 0 %\n",
      "Accuracy of [43]: 0 %\n",
      "Accuracy of [44]: 0 %\n",
      "Accuracy of [45]: 0 %\n",
      "Accuracy of [46]: 0 %\n",
      "Accuracy of [47]: 0 %\n",
      "Accuracy of [48]: 0 %\n",
      "Accuracy of [49]: 0 %\n",
      "Accuracy of [50]: 0 %\n",
      "Accuracy of [51]: 0 %\n",
      "Accuracy of [52]: 0 %\n",
      "Accuracy of [53]: 0 %\n",
      "Accuracy of [54]: 0 %\n",
      "Accuracy of [55]: 0 %\n",
      "Accuracy of [56]: 0 %\n",
      "Accuracy of [57]: 0 %\n",
      "Accuracy of [58]: 0 %\n",
      "Accuracy of [59]: 0 %\n",
      "Accuracy of [60]: 0 %\n",
      "Accuracy of [61]: 0 %\n",
      "Accuracy of [62]: 0 %\n",
      "Accuracy of [63]: 0 %\n",
      "Accuracy of [64]: 0 %\n",
      "Accuracy of [65]: 0 %\n",
      "Accuracy of [66]: 0 %\n",
      "Accuracy of [67]: 0 %\n",
      "Accuracy of [68]: 0 %\n",
      "Accuracy of [69]: 0 %\n",
      "Accuracy of [70]: 0 %\n",
      "Accuracy of [71]: 0 %\n",
      "Accuracy of [72]: 0 %\n",
      "Accuracy of [73]: 0 %\n",
      "Accuracy of [74]: 0 %\n",
      "Accuracy of [75]: 0 %\n",
      "Accuracy of [76]: 0 %\n",
      "Accuracy of [77]: 0 %\n",
      "Accuracy of [78]: 0 %\n",
      "Accuracy of [79]: 0 %\n",
      "Accuracy of [80]: 0 %\n",
      "Accuracy of [81]: 0 %\n",
      "Accuracy of [82]: 0 %\n",
      "Accuracy of [83]: 0 %\n",
      "Accuracy of [84]: 0 %\n",
      "Accuracy of [85]: 0 %\n",
      "Accuracy of [86]: 0 %\n",
      "Accuracy of [87]: 0 %\n",
      "Accuracy of [88]: 0 %\n",
      "Accuracy of [89]: 0 %\n",
      "Accuracy of [90]: 0 %\n",
      "Accuracy of [91]: 0 %\n",
      "Accuracy of [92]: 0 %\n",
      "Accuracy of [93]: 0 %\n",
      "Accuracy of [94]: 0 %\n",
      "Accuracy of [95]: 0 %\n",
      "Accuracy of [96]: 0 %\n",
      "Accuracy of [97]: 0 %\n",
      "Accuracy of [98]: 0 %\n",
      "Accuracy of [99]: 0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jusxp\\anaconda3\\envs\\498A\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(100)]\n",
    "    n_class_samples = [0 for i in range(100)]\n",
    "    for images, labels, coarse_label in test_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        labels[labels == 12] = 0\n",
    "        labels[labels == 17] = 1\n",
    "        labels[labels == 37] = 2\n",
    "        labels[labels == 68] = 3\n",
    "        labels[labels == 76] = 4\n",
    "        outputs = model(images)\n",
    "       \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "\n",
    "    for i in range(100):\n",
    "        if n_class_samples[i] == 0:\n",
    "            print(f'Accuracy of {[i]}: 0 %')\n",
    "        else:\n",
    "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/625], Loss: 1.2585\n",
      "Epoch [1/10], Step [2/625], Loss: 1.3845\n",
      "Epoch [1/10], Step [3/625], Loss: 1.5617\n",
      "Epoch [1/10], Step [4/625], Loss: 1.4248\n",
      "Epoch [1/10], Step [5/625], Loss: 1.3952\n",
      "Epoch [1/10], Step [6/625], Loss: 1.3974\n",
      "Epoch [1/10], Step [7/625], Loss: 1.5509\n",
      "Epoch [1/10], Step [8/625], Loss: 1.4012\n",
      "Epoch [1/10], Step [9/625], Loss: 1.4590\n",
      "Epoch [1/10], Step [10/625], Loss: 1.4244\n",
      "Epoch [1/10], Step [11/625], Loss: 1.6425\n",
      "Epoch [1/10], Step [12/625], Loss: 1.2308\n",
      "Epoch [1/10], Step [13/625], Loss: 1.1951\n",
      "Epoch [1/10], Step [14/625], Loss: 1.4864\n",
      "Epoch [1/10], Step [15/625], Loss: 1.3732\n",
      "Epoch [1/10], Step [16/625], Loss: 1.2859\n",
      "Epoch [1/10], Step [17/625], Loss: 1.1547\n",
      "Epoch [1/10], Step [18/625], Loss: 1.6423\n",
      "Epoch [1/10], Step [19/625], Loss: 1.3223\n",
      "Epoch [1/10], Step [20/625], Loss: 1.8866\n",
      "Epoch [1/10], Step [21/625], Loss: 1.7468\n",
      "Epoch [1/10], Step [22/625], Loss: 1.8008\n",
      "Epoch [1/10], Step [23/625], Loss: 1.1661\n",
      "Epoch [1/10], Step [24/625], Loss: 1.1446\n",
      "Epoch [1/10], Step [25/625], Loss: 1.3980\n",
      "Epoch [1/10], Step [26/625], Loss: 1.4014\n",
      "Epoch [1/10], Step [27/625], Loss: 1.1160\n",
      "Epoch [1/10], Step [28/625], Loss: 1.3562\n",
      "Epoch [1/10], Step [29/625], Loss: 1.4049\n",
      "Epoch [1/10], Step [30/625], Loss: 1.2526\n",
      "Epoch [1/10], Step [31/625], Loss: 1.1538\n",
      "Epoch [1/10], Step [32/625], Loss: 1.5354\n",
      "Epoch [1/10], Step [33/625], Loss: 1.4072\n",
      "Epoch [1/10], Step [34/625], Loss: 1.4032\n",
      "Epoch [1/10], Step [35/625], Loss: 1.1619\n",
      "Epoch [1/10], Step [36/625], Loss: 1.2925\n",
      "Epoch [1/10], Step [37/625], Loss: 1.4363\n",
      "Epoch [1/10], Step [38/625], Loss: 1.6310\n",
      "Epoch [1/10], Step [39/625], Loss: 1.6520\n",
      "Epoch [1/10], Step [40/625], Loss: 1.8058\n",
      "Epoch [1/10], Step [41/625], Loss: 1.3918\n",
      "Epoch [1/10], Step [42/625], Loss: 1.6522\n",
      "Epoch [1/10], Step [43/625], Loss: 1.3634\n",
      "Epoch [1/10], Step [44/625], Loss: 1.3119\n",
      "Epoch [1/10], Step [45/625], Loss: 1.1512\n",
      "Epoch [1/10], Step [46/625], Loss: 1.7895\n",
      "Epoch [1/10], Step [47/625], Loss: 1.1510\n",
      "Epoch [1/10], Step [48/625], Loss: 1.6556\n",
      "Epoch [1/10], Step [49/625], Loss: 1.8592\n",
      "Epoch [1/10], Step [50/625], Loss: 1.4190\n",
      "Epoch [1/10], Step [51/625], Loss: 1.3674\n",
      "Epoch [1/10], Step [52/625], Loss: 1.3974\n",
      "Epoch [1/10], Step [53/625], Loss: 1.3846\n",
      "Epoch [1/10], Step [54/625], Loss: 1.5208\n",
      "Epoch [1/10], Step [55/625], Loss: 1.1942\n",
      "Epoch [1/10], Step [56/625], Loss: 1.1594\n",
      "Epoch [1/10], Step [57/625], Loss: 1.4196\n",
      "Epoch [1/10], Step [58/625], Loss: 1.6459\n",
      "Epoch [1/10], Step [59/625], Loss: 1.3371\n",
      "Epoch [1/10], Step [60/625], Loss: 1.1677\n",
      "Epoch [1/10], Step [61/625], Loss: 1.8080\n",
      "Epoch [1/10], Step [62/625], Loss: 1.2017\n",
      "Epoch [1/10], Step [63/625], Loss: 1.3921\n",
      "Epoch [1/10], Step [64/625], Loss: 1.4950\n",
      "Epoch [1/10], Step [65/625], Loss: 1.6541\n",
      "Epoch [1/10], Step [66/625], Loss: 1.3078\n",
      "Epoch [1/10], Step [67/625], Loss: 1.4639\n",
      "Epoch [1/10], Step [68/625], Loss: 1.5429\n",
      "Epoch [1/10], Step [69/625], Loss: 0.9052\n",
      "Epoch [1/10], Step [70/625], Loss: 1.1545\n",
      "Epoch [1/10], Step [71/625], Loss: 1.8728\n",
      "Epoch [1/10], Step [72/625], Loss: 1.6397\n",
      "Epoch [1/10], Step [73/625], Loss: 1.6255\n",
      "Epoch [1/10], Step [74/625], Loss: 1.1137\n",
      "Epoch [1/10], Step [75/625], Loss: 1.7415\n",
      "Epoch [1/10], Step [76/625], Loss: 1.6489\n",
      "Epoch [1/10], Step [77/625], Loss: 0.9729\n",
      "Epoch [1/10], Step [78/625], Loss: 1.3820\n",
      "Epoch [1/10], Step [79/625], Loss: 1.4017\n",
      "Epoch [1/10], Step [80/625], Loss: 1.4019\n",
      "Epoch [1/10], Step [81/625], Loss: 1.8481\n",
      "Epoch [1/10], Step [82/625], Loss: 1.3408\n",
      "Epoch [1/10], Step [83/625], Loss: 1.3913\n",
      "Epoch [1/10], Step [84/625], Loss: 1.3235\n",
      "Epoch [1/10], Step [85/625], Loss: 1.7668\n",
      "Epoch [1/10], Step [86/625], Loss: 1.2286\n",
      "Epoch [1/10], Step [87/625], Loss: 1.3972\n",
      "Epoch [1/10], Step [88/625], Loss: 1.5450\n",
      "Epoch [1/10], Step [89/625], Loss: 1.1489\n",
      "Epoch [1/10], Step [90/625], Loss: 1.6288\n",
      "Epoch [1/10], Step [91/625], Loss: 1.4473\n",
      "Epoch [1/10], Step [92/625], Loss: 1.6564\n",
      "Epoch [1/10], Step [93/625], Loss: 1.6543\n",
      "Epoch [1/10], Step [94/625], Loss: 1.3897\n",
      "Epoch [1/10], Step [95/625], Loss: 1.8886\n",
      "Epoch [1/10], Step [96/625], Loss: 1.1019\n",
      "Epoch [1/10], Step [97/625], Loss: 1.5080\n",
      "Epoch [1/10], Step [98/625], Loss: 1.7505\n",
      "Epoch [1/10], Step [99/625], Loss: 0.9060\n",
      "Epoch [1/10], Step [100/625], Loss: 1.3928\n",
      "Epoch [1/10], Step [101/625], Loss: 1.1642\n",
      "Epoch [1/10], Step [102/625], Loss: 1.2939\n",
      "Epoch [1/10], Step [103/625], Loss: 1.4343\n",
      "Epoch [1/10], Step [104/625], Loss: 1.4083\n",
      "Epoch [1/10], Step [105/625], Loss: 1.2011\n",
      "Epoch [1/10], Step [106/625], Loss: 1.4393\n",
      "Epoch [1/10], Step [107/625], Loss: 1.4133\n",
      "Epoch [1/10], Step [108/625], Loss: 1.1500\n",
      "Epoch [1/10], Step [109/625], Loss: 1.4330\n",
      "Epoch [1/10], Step [110/625], Loss: 1.6303\n",
      "Epoch [1/10], Step [111/625], Loss: 1.3875\n",
      "Epoch [1/10], Step [112/625], Loss: 1.4051\n",
      "Epoch [1/10], Step [113/625], Loss: 1.4436\n",
      "Epoch [1/10], Step [114/625], Loss: 1.3980\n",
      "Epoch [1/10], Step [115/625], Loss: 1.2805\n",
      "Epoch [1/10], Step [116/625], Loss: 1.6535\n",
      "Epoch [1/10], Step [117/625], Loss: 0.9123\n",
      "Epoch [1/10], Step [118/625], Loss: 1.1301\n",
      "Epoch [1/10], Step [119/625], Loss: 1.3118\n",
      "Epoch [1/10], Step [120/625], Loss: 1.1916\n",
      "Epoch [1/10], Step [121/625], Loss: 1.3044\n",
      "Epoch [1/10], Step [122/625], Loss: 1.0808\n",
      "Epoch [1/10], Step [123/625], Loss: 1.1556\n",
      "Epoch [1/10], Step [124/625], Loss: 1.8848\n",
      "Epoch [1/10], Step [125/625], Loss: 1.4149\n",
      "Epoch [1/10], Step [126/625], Loss: 1.4564\n",
      "Epoch [1/10], Step [127/625], Loss: 1.2649\n",
      "Epoch [1/10], Step [128/625], Loss: 1.2102\n",
      "Epoch [1/10], Step [129/625], Loss: 1.2344\n",
      "Epoch [1/10], Step [130/625], Loss: 1.6296\n",
      "Epoch [1/10], Step [131/625], Loss: 1.8279\n",
      "Epoch [1/10], Step [132/625], Loss: 1.8531\n",
      "Epoch [1/10], Step [133/625], Loss: 1.5531\n",
      "Epoch [1/10], Step [134/625], Loss: 1.3834\n",
      "Epoch [1/10], Step [135/625], Loss: 1.7570\n",
      "Epoch [1/10], Step [136/625], Loss: 1.3866\n",
      "Epoch [1/10], Step [137/625], Loss: 1.2559\n",
      "Epoch [1/10], Step [138/625], Loss: 1.1950\n",
      "Epoch [1/10], Step [139/625], Loss: 1.2639\n",
      "Epoch [1/10], Step [140/625], Loss: 1.6975\n",
      "Epoch [1/10], Step [141/625], Loss: 1.8767\n",
      "Epoch [1/10], Step [142/625], Loss: 1.4172\n",
      "Epoch [1/10], Step [143/625], Loss: 1.8485\n",
      "Epoch [1/10], Step [144/625], Loss: 1.3352\n",
      "Epoch [1/10], Step [145/625], Loss: 1.3488\n",
      "Epoch [1/10], Step [146/625], Loss: 1.1427\n",
      "Epoch [1/10], Step [147/625], Loss: 1.4065\n",
      "Epoch [1/10], Step [148/625], Loss: 1.7602\n",
      "Epoch [1/10], Step [149/625], Loss: 1.6476\n",
      "Epoch [1/10], Step [150/625], Loss: 1.3868\n",
      "Epoch [1/10], Step [151/625], Loss: 1.4026\n",
      "Epoch [1/10], Step [152/625], Loss: 1.6339\n",
      "Epoch [1/10], Step [153/625], Loss: 1.6893\n",
      "Epoch [1/10], Step [154/625], Loss: 1.4024\n",
      "Epoch [1/10], Step [155/625], Loss: 1.5052\n",
      "Epoch [1/10], Step [156/625], Loss: 1.1582\n",
      "Epoch [1/10], Step [157/625], Loss: 1.3714\n",
      "Epoch [1/10], Step [158/625], Loss: 1.7427\n",
      "Epoch [1/10], Step [159/625], Loss: 1.3143\n",
      "Epoch [1/10], Step [160/625], Loss: 1.1019\n",
      "Epoch [1/10], Step [161/625], Loss: 1.3874\n",
      "Epoch [1/10], Step [162/625], Loss: 1.1471\n",
      "Epoch [1/10], Step [163/625], Loss: 1.1548\n",
      "Epoch [1/10], Step [164/625], Loss: 1.6541\n",
      "Epoch [1/10], Step [165/625], Loss: 1.8139\n",
      "Epoch [1/10], Step [166/625], Loss: 1.3508\n",
      "Epoch [1/10], Step [167/625], Loss: 1.6665\n",
      "Epoch [1/10], Step [168/625], Loss: 1.4071\n",
      "Epoch [1/10], Step [169/625], Loss: 1.5697\n",
      "Epoch [1/10], Step [170/625], Loss: 1.6372\n",
      "Epoch [1/10], Step [171/625], Loss: 1.4119\n",
      "Epoch [1/10], Step [172/625], Loss: 1.4077\n",
      "Epoch [1/10], Step [173/625], Loss: 1.3650\n",
      "Epoch [1/10], Step [174/625], Loss: 1.6373\n",
      "Epoch [1/10], Step [175/625], Loss: 1.6975\n",
      "Epoch [1/10], Step [176/625], Loss: 0.9982\n",
      "Epoch [1/10], Step [177/625], Loss: 1.4124\n",
      "Epoch [1/10], Step [178/625], Loss: 1.6453\n",
      "Epoch [1/10], Step [179/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [180/625], Loss: 1.8854\n",
      "Epoch [1/10], Step [181/625], Loss: 1.1696\n",
      "Epoch [1/10], Step [182/625], Loss: 1.6320\n",
      "Epoch [1/10], Step [183/625], Loss: 1.4034\n",
      "Epoch [1/10], Step [184/625], Loss: 1.5206\n",
      "Epoch [1/10], Step [185/625], Loss: 1.4086\n",
      "Epoch [1/10], Step [186/625], Loss: 1.2169\n",
      "Epoch [1/10], Step [187/625], Loss: 1.6336\n",
      "Epoch [1/10], Step [188/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [189/625], Loss: 1.3513\n",
      "Epoch [1/10], Step [190/625], Loss: 1.1457\n",
      "Epoch [1/10], Step [191/625], Loss: 1.6548\n",
      "Epoch [1/10], Step [192/625], Loss: 1.6072\n",
      "Epoch [1/10], Step [193/625], Loss: 1.8561\n",
      "Epoch [1/10], Step [194/625], Loss: 1.3503\n",
      "Epoch [1/10], Step [195/625], Loss: 1.3140\n",
      "Epoch [1/10], Step [196/625], Loss: 1.6438\n",
      "Epoch [1/10], Step [197/625], Loss: 1.4252\n",
      "Epoch [1/10], Step [198/625], Loss: 1.6423\n",
      "Epoch [1/10], Step [199/625], Loss: 1.3914\n",
      "Epoch [1/10], Step [200/625], Loss: 1.5111\n",
      "Epoch [1/10], Step [201/625], Loss: 1.4043\n",
      "Epoch [1/10], Step [202/625], Loss: 1.6546\n",
      "Epoch [1/10], Step [203/625], Loss: 1.4071\n",
      "Epoch [1/10], Step [204/625], Loss: 1.3353\n",
      "Epoch [1/10], Step [205/625], Loss: 1.4739\n",
      "Epoch [1/10], Step [206/625], Loss: 1.4199\n",
      "Epoch [1/10], Step [207/625], Loss: 1.8759\n",
      "Epoch [1/10], Step [208/625], Loss: 1.6322\n",
      "Epoch [1/10], Step [209/625], Loss: 1.3963\n",
      "Epoch [1/10], Step [210/625], Loss: 1.3843\n",
      "Epoch [1/10], Step [211/625], Loss: 1.6393\n",
      "Epoch [1/10], Step [212/625], Loss: 1.3717\n",
      "Epoch [1/10], Step [213/625], Loss: 1.2840\n",
      "Epoch [1/10], Step [214/625], Loss: 1.5672\n",
      "Epoch [1/10], Step [215/625], Loss: 1.9020\n",
      "Epoch [1/10], Step [216/625], Loss: 1.4080\n",
      "Epoch [1/10], Step [217/625], Loss: 1.8581\n",
      "Epoch [1/10], Step [218/625], Loss: 1.6609\n",
      "Epoch [1/10], Step [219/625], Loss: 1.2962\n",
      "Epoch [1/10], Step [220/625], Loss: 1.3411\n",
      "Epoch [1/10], Step [221/625], Loss: 1.3984\n",
      "Epoch [1/10], Step [222/625], Loss: 1.4010\n",
      "Epoch [1/10], Step [223/625], Loss: 1.6689\n",
      "Epoch [1/10], Step [224/625], Loss: 1.6495\n",
      "Epoch [1/10], Step [225/625], Loss: 1.4017\n",
      "Epoch [1/10], Step [226/625], Loss: 0.9182\n",
      "Epoch [1/10], Step [227/625], Loss: 1.6432\n",
      "Epoch [1/10], Step [228/625], Loss: 1.8904\n",
      "Epoch [1/10], Step [229/625], Loss: 1.4058\n",
      "Epoch [1/10], Step [230/625], Loss: 0.9051\n",
      "Epoch [1/10], Step [231/625], Loss: 1.5312\n",
      "Epoch [1/10], Step [232/625], Loss: 1.1550\n",
      "Epoch [1/10], Step [233/625], Loss: 1.1546\n",
      "Epoch [1/10], Step [234/625], Loss: 1.1627\n",
      "Epoch [1/10], Step [235/625], Loss: 1.7493\n",
      "Epoch [1/10], Step [236/625], Loss: 1.4309\n",
      "Epoch [1/10], Step [237/625], Loss: 1.1558\n",
      "Epoch [1/10], Step [238/625], Loss: 1.3963\n",
      "Epoch [1/10], Step [239/625], Loss: 1.4725\n",
      "Epoch [1/10], Step [240/625], Loss: 1.1678\n",
      "Epoch [1/10], Step [241/625], Loss: 1.3266\n",
      "Epoch [1/10], Step [242/625], Loss: 1.7088\n",
      "Epoch [1/10], Step [243/625], Loss: 1.6503\n",
      "Epoch [1/10], Step [244/625], Loss: 0.9049\n",
      "Epoch [1/10], Step [245/625], Loss: 1.3834\n",
      "Epoch [1/10], Step [246/625], Loss: 1.1167\n",
      "Epoch [1/10], Step [247/625], Loss: 1.4661\n",
      "Epoch [1/10], Step [248/625], Loss: 1.2065\n",
      "Epoch [1/10], Step [249/625], Loss: 1.7369\n",
      "Epoch [1/10], Step [250/625], Loss: 1.1529\n",
      "Epoch [1/10], Step [251/625], Loss: 1.2135\n",
      "Epoch [1/10], Step [252/625], Loss: 1.2806\n",
      "Epoch [1/10], Step [253/625], Loss: 1.6190\n",
      "Epoch [1/10], Step [254/625], Loss: 1.1906\n",
      "Epoch [1/10], Step [255/625], Loss: 1.1309\n",
      "Epoch [1/10], Step [256/625], Loss: 1.1548\n",
      "Epoch [1/10], Step [257/625], Loss: 1.6365\n",
      "Epoch [1/10], Step [258/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [259/625], Loss: 1.1819\n",
      "Epoch [1/10], Step [260/625], Loss: 1.4935\n",
      "Epoch [1/10], Step [261/625], Loss: 1.6484\n",
      "Epoch [1/10], Step [262/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [263/625], Loss: 1.8229\n",
      "Epoch [1/10], Step [264/625], Loss: 1.1567\n",
      "Epoch [1/10], Step [265/625], Loss: 1.8864\n",
      "Epoch [1/10], Step [266/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [267/625], Loss: 1.1269\n",
      "Epoch [1/10], Step [268/625], Loss: 1.0591\n",
      "Epoch [1/10], Step [269/625], Loss: 1.4743\n",
      "Epoch [1/10], Step [270/625], Loss: 1.1723\n",
      "Epoch [1/10], Step [271/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [272/625], Loss: 1.4123\n",
      "Epoch [1/10], Step [273/625], Loss: 1.6701\n",
      "Epoch [1/10], Step [274/625], Loss: 1.6509\n",
      "Epoch [1/10], Step [275/625], Loss: 1.2447\n",
      "Epoch [1/10], Step [276/625], Loss: 1.4087\n",
      "Epoch [1/10], Step [277/625], Loss: 1.6890\n",
      "Epoch [1/10], Step [278/625], Loss: 1.3646\n",
      "Epoch [1/10], Step [279/625], Loss: 1.4562\n",
      "Epoch [1/10], Step [280/625], Loss: 1.3991\n",
      "Epoch [1/10], Step [281/625], Loss: 1.6714\n",
      "Epoch [1/10], Step [282/625], Loss: 1.4049\n",
      "Epoch [1/10], Step [283/625], Loss: 1.1536\n",
      "Epoch [1/10], Step [284/625], Loss: 1.2297\n",
      "Epoch [1/10], Step [285/625], Loss: 1.4634\n",
      "Epoch [1/10], Step [286/625], Loss: 1.3989\n",
      "Epoch [1/10], Step [287/625], Loss: 1.4042\n",
      "Epoch [1/10], Step [288/625], Loss: 1.2564\n",
      "Epoch [1/10], Step [289/625], Loss: 1.0447\n",
      "Epoch [1/10], Step [290/625], Loss: 1.2494\n",
      "Epoch [1/10], Step [291/625], Loss: 1.1552\n",
      "Epoch [1/10], Step [292/625], Loss: 1.4778\n",
      "Epoch [1/10], Step [293/625], Loss: 1.6607\n",
      "Epoch [1/10], Step [294/625], Loss: 1.4036\n",
      "Epoch [1/10], Step [295/625], Loss: 1.3978\n",
      "Epoch [1/10], Step [296/625], Loss: 1.6459\n",
      "Epoch [1/10], Step [297/625], Loss: 1.4047\n",
      "Epoch [1/10], Step [298/625], Loss: 1.2765\n",
      "Epoch [1/10], Step [299/625], Loss: 0.9642\n",
      "Epoch [1/10], Step [300/625], Loss: 1.3536\n",
      "Epoch [1/10], Step [301/625], Loss: 1.4053\n",
      "Epoch [1/10], Step [302/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [303/625], Loss: 1.8886\n",
      "Epoch [1/10], Step [304/625], Loss: 1.6196\n",
      "Epoch [1/10], Step [305/625], Loss: 1.6546\n",
      "Epoch [1/10], Step [306/625], Loss: 1.3875\n",
      "Epoch [1/10], Step [307/625], Loss: 1.3939\n",
      "Epoch [1/10], Step [308/625], Loss: 1.8652\n",
      "Epoch [1/10], Step [309/625], Loss: 1.6337\n",
      "Epoch [1/10], Step [310/625], Loss: 1.4160\n",
      "Epoch [1/10], Step [311/625], Loss: 1.4144\n",
      "Epoch [1/10], Step [312/625], Loss: 1.6093\n",
      "Epoch [1/10], Step [313/625], Loss: 1.6435\n",
      "Epoch [1/10], Step [314/625], Loss: 1.3625\n",
      "Epoch [1/10], Step [315/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [316/625], Loss: 1.4322\n",
      "Epoch [1/10], Step [317/625], Loss: 0.9119\n",
      "Epoch [1/10], Step [318/625], Loss: 1.3255\n",
      "Epoch [1/10], Step [319/625], Loss: 1.1507\n",
      "Epoch [1/10], Step [320/625], Loss: 1.4008\n",
      "Epoch [1/10], Step [321/625], Loss: 1.4007\n",
      "Epoch [1/10], Step [322/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [323/625], Loss: 1.6871\n",
      "Epoch [1/10], Step [324/625], Loss: 1.1992\n",
      "Epoch [1/10], Step [325/625], Loss: 1.4353\n",
      "Epoch [1/10], Step [326/625], Loss: 1.3129\n",
      "Epoch [1/10], Step [327/625], Loss: 1.3958\n",
      "Epoch [1/10], Step [328/625], Loss: 1.6460\n",
      "Epoch [1/10], Step [329/625], Loss: 1.2397\n",
      "Epoch [1/10], Step [330/625], Loss: 1.6326\n",
      "Epoch [1/10], Step [331/625], Loss: 1.4177\n",
      "Epoch [1/10], Step [332/625], Loss: 1.0520\n",
      "Epoch [1/10], Step [333/625], Loss: 1.8808\n",
      "Epoch [1/10], Step [334/625], Loss: 0.9167\n",
      "Epoch [1/10], Step [335/625], Loss: 1.1544\n",
      "Epoch [1/10], Step [336/625], Loss: 1.2235\n",
      "Epoch [1/10], Step [337/625], Loss: 1.3943\n",
      "Epoch [1/10], Step [338/625], Loss: 1.5913\n",
      "Epoch [1/10], Step [339/625], Loss: 1.0178\n",
      "Epoch [1/10], Step [340/625], Loss: 1.6357\n",
      "Epoch [1/10], Step [341/625], Loss: 1.3923\n",
      "Epoch [1/10], Step [342/625], Loss: 1.9045\n",
      "Epoch [1/10], Step [343/625], Loss: 1.4045\n",
      "Epoch [1/10], Step [344/625], Loss: 1.6210\n",
      "Epoch [1/10], Step [345/625], Loss: 1.4052\n",
      "Epoch [1/10], Step [346/625], Loss: 1.4461\n",
      "Epoch [1/10], Step [347/625], Loss: 1.5863\n",
      "Epoch [1/10], Step [348/625], Loss: 1.6532\n",
      "Epoch [1/10], Step [349/625], Loss: 1.6386\n",
      "Epoch [1/10], Step [350/625], Loss: 1.1555\n",
      "Epoch [1/10], Step [351/625], Loss: 1.1494\n",
      "Epoch [1/10], Step [352/625], Loss: 1.4074\n",
      "Epoch [1/10], Step [353/625], Loss: 1.4046\n",
      "Epoch [1/10], Step [354/625], Loss: 1.4070\n",
      "Epoch [1/10], Step [355/625], Loss: 1.6173\n",
      "Epoch [1/10], Step [356/625], Loss: 1.4033\n",
      "Epoch [1/10], Step [357/625], Loss: 1.4049\n",
      "Epoch [1/10], Step [358/625], Loss: 1.1615\n",
      "Epoch [1/10], Step [359/625], Loss: 1.4295\n",
      "Epoch [1/10], Step [360/625], Loss: 1.1552\n",
      "Epoch [1/10], Step [361/625], Loss: 1.1455\n",
      "Epoch [1/10], Step [362/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [363/625], Loss: 1.6543\n",
      "Epoch [1/10], Step [364/625], Loss: 1.6731\n",
      "Epoch [1/10], Step [365/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [366/625], Loss: 1.5394\n",
      "Epoch [1/10], Step [367/625], Loss: 1.8725\n",
      "Epoch [1/10], Step [368/625], Loss: 1.4633\n",
      "Epoch [1/10], Step [369/625], Loss: 1.1583\n",
      "Epoch [1/10], Step [370/625], Loss: 1.3842\n",
      "Epoch [1/10], Step [371/625], Loss: 1.3951\n",
      "Epoch [1/10], Step [372/625], Loss: 1.3636\n",
      "Epoch [1/10], Step [373/625], Loss: 1.1580\n",
      "Epoch [1/10], Step [374/625], Loss: 1.5268\n",
      "Epoch [1/10], Step [375/625], Loss: 1.4043\n",
      "Epoch [1/10], Step [376/625], Loss: 1.4030\n",
      "Epoch [1/10], Step [377/625], Loss: 1.4027\n",
      "Epoch [1/10], Step [378/625], Loss: 1.6315\n",
      "Epoch [1/10], Step [379/625], Loss: 1.6544\n",
      "Epoch [1/10], Step [380/625], Loss: 1.6137\n",
      "Epoch [1/10], Step [381/625], Loss: 1.4194\n",
      "Epoch [1/10], Step [382/625], Loss: 1.1349\n",
      "Epoch [1/10], Step [383/625], Loss: 1.5592\n",
      "Epoch [1/10], Step [384/625], Loss: 1.8698\n",
      "Epoch [1/10], Step [385/625], Loss: 1.4109\n",
      "Epoch [1/10], Step [386/625], Loss: 1.3832\n",
      "Epoch [1/10], Step [387/625], Loss: 1.1541\n",
      "Epoch [1/10], Step [388/625], Loss: 1.6401\n",
      "Epoch [1/10], Step [389/625], Loss: 0.9434\n",
      "Epoch [1/10], Step [390/625], Loss: 1.6417\n",
      "Epoch [1/10], Step [391/625], Loss: 1.2849\n",
      "Epoch [1/10], Step [392/625], Loss: 1.4970\n",
      "Epoch [1/10], Step [393/625], Loss: 1.6598\n",
      "Epoch [1/10], Step [394/625], Loss: 1.3709\n",
      "Epoch [1/10], Step [395/625], Loss: 1.0848\n",
      "Epoch [1/10], Step [396/625], Loss: 1.2787\n",
      "Epoch [1/10], Step [397/625], Loss: 1.8273\n",
      "Epoch [1/10], Step [398/625], Loss: 1.3085\n",
      "Epoch [1/10], Step [399/625], Loss: 1.1842\n",
      "Epoch [1/10], Step [400/625], Loss: 1.1833\n",
      "Epoch [1/10], Step [401/625], Loss: 1.1324\n",
      "Epoch [1/10], Step [402/625], Loss: 1.4047\n",
      "Epoch [1/10], Step [403/625], Loss: 1.1576\n",
      "Epoch [1/10], Step [404/625], Loss: 1.4050\n",
      "Epoch [1/10], Step [405/625], Loss: 1.2350\n",
      "Epoch [1/10], Step [406/625], Loss: 1.6408\n",
      "Epoch [1/10], Step [407/625], Loss: 1.5545\n",
      "Epoch [1/10], Step [408/625], Loss: 1.4106\n",
      "Epoch [1/10], Step [409/625], Loss: 1.4043\n",
      "Epoch [1/10], Step [410/625], Loss: 0.9061\n",
      "Epoch [1/10], Step [411/625], Loss: 1.9012\n",
      "Epoch [1/10], Step [412/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [413/625], Loss: 1.3537\n",
      "Epoch [1/10], Step [414/625], Loss: 1.6385\n",
      "Epoch [1/10], Step [415/625], Loss: 1.4389\n",
      "Epoch [1/10], Step [416/625], Loss: 1.4226\n",
      "Epoch [1/10], Step [417/625], Loss: 1.3264\n",
      "Epoch [1/10], Step [418/625], Loss: 1.2175\n",
      "Epoch [1/10], Step [419/625], Loss: 1.4041\n",
      "Epoch [1/10], Step [420/625], Loss: 1.9042\n",
      "Epoch [1/10], Step [421/625], Loss: 0.9050\n",
      "Epoch [1/10], Step [422/625], Loss: 1.6564\n",
      "Epoch [1/10], Step [423/625], Loss: 1.6413\n",
      "Epoch [1/10], Step [424/625], Loss: 1.6398\n",
      "Epoch [1/10], Step [425/625], Loss: 1.1224\n",
      "Epoch [1/10], Step [426/625], Loss: 1.2090\n",
      "Epoch [1/10], Step [427/625], Loss: 1.5617\n",
      "Epoch [1/10], Step [428/625], Loss: 1.5597\n",
      "Epoch [1/10], Step [429/625], Loss: 1.5872\n",
      "Epoch [1/10], Step [430/625], Loss: 1.3044\n",
      "Epoch [1/10], Step [431/625], Loss: 1.7150\n",
      "Epoch [1/10], Step [432/625], Loss: 1.1706\n",
      "Epoch [1/10], Step [433/625], Loss: 1.4009\n",
      "Epoch [1/10], Step [434/625], Loss: 1.6646\n",
      "Epoch [1/10], Step [435/625], Loss: 1.6068\n",
      "Epoch [1/10], Step [436/625], Loss: 1.3958\n",
      "Epoch [1/10], Step [437/625], Loss: 1.1569\n",
      "Epoch [1/10], Step [438/625], Loss: 1.4408\n",
      "Epoch [1/10], Step [439/625], Loss: 0.9055\n",
      "Epoch [1/10], Step [440/625], Loss: 1.6092\n",
      "Epoch [1/10], Step [441/625], Loss: 1.1885\n",
      "Epoch [1/10], Step [442/625], Loss: 0.9254\n",
      "Epoch [1/10], Step [443/625], Loss: 1.0624\n",
      "Epoch [1/10], Step [444/625], Loss: 1.6542\n",
      "Epoch [1/10], Step [445/625], Loss: 1.2900\n",
      "Epoch [1/10], Step [446/625], Loss: 1.1886\n",
      "Epoch [1/10], Step [447/625], Loss: 0.9053\n",
      "Epoch [1/10], Step [448/625], Loss: 1.3253\n",
      "Epoch [1/10], Step [449/625], Loss: 1.4257\n",
      "Epoch [1/10], Step [450/625], Loss: 0.9048\n",
      "Epoch [1/10], Step [451/625], Loss: 1.3936\n",
      "Epoch [1/10], Step [452/625], Loss: 1.0464\n",
      "Epoch [1/10], Step [453/625], Loss: 1.2444\n",
      "Epoch [1/10], Step [454/625], Loss: 1.3967\n",
      "Epoch [1/10], Step [455/625], Loss: 1.3266\n",
      "Epoch [1/10], Step [456/625], Loss: 1.4105\n",
      "Epoch [1/10], Step [457/625], Loss: 1.1608\n",
      "Epoch [1/10], Step [458/625], Loss: 1.2218\n",
      "Epoch [1/10], Step [459/625], Loss: 1.7761\n",
      "Epoch [1/10], Step [460/625], Loss: 1.1548\n",
      "Epoch [1/10], Step [461/625], Loss: 1.4014\n",
      "Epoch [1/10], Step [462/625], Loss: 1.6279\n",
      "Epoch [1/10], Step [463/625], Loss: 1.4701\n",
      "Epoch [1/10], Step [464/625], Loss: 1.4249\n",
      "Epoch [1/10], Step [465/625], Loss: 1.8783\n",
      "Epoch [1/10], Step [466/625], Loss: 1.8796\n",
      "Epoch [1/10], Step [467/625], Loss: 1.0458\n",
      "Epoch [1/10], Step [468/625], Loss: 1.4005\n",
      "Epoch [1/10], Step [469/625], Loss: 1.3818\n",
      "Epoch [1/10], Step [470/625], Loss: 1.1279\n",
      "Epoch [1/10], Step [471/625], Loss: 1.2143\n",
      "Epoch [1/10], Step [472/625], Loss: 1.0790\n",
      "Epoch [1/10], Step [473/625], Loss: 1.7880\n",
      "Epoch [1/10], Step [474/625], Loss: 1.2326\n",
      "Epoch [1/10], Step [475/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [476/625], Loss: 1.8995\n",
      "Epoch [1/10], Step [477/625], Loss: 1.4817\n",
      "Epoch [1/10], Step [478/625], Loss: 1.4360\n",
      "Epoch [1/10], Step [479/625], Loss: 1.5487\n",
      "Epoch [1/10], Step [480/625], Loss: 1.1546\n",
      "Epoch [1/10], Step [481/625], Loss: 1.6559\n",
      "Epoch [1/10], Step [482/625], Loss: 1.3673\n",
      "Epoch [1/10], Step [483/625], Loss: 1.5585\n",
      "Epoch [1/10], Step [484/625], Loss: 1.2567\n",
      "Epoch [1/10], Step [485/625], Loss: 1.2185\n",
      "Epoch [1/10], Step [486/625], Loss: 1.5188\n",
      "Epoch [1/10], Step [487/625], Loss: 1.4548\n",
      "Epoch [1/10], Step [488/625], Loss: 1.2202\n",
      "Epoch [1/10], Step [489/625], Loss: 1.1541\n",
      "Epoch [1/10], Step [490/625], Loss: 1.3968\n",
      "Epoch [1/10], Step [491/625], Loss: 1.3929\n",
      "Epoch [1/10], Step [492/625], Loss: 1.3743\n",
      "Epoch [1/10], Step [493/625], Loss: 1.4707\n",
      "Epoch [1/10], Step [494/625], Loss: 0.9048\n",
      "Epoch [1/10], Step [495/625], Loss: 1.1548\n",
      "Epoch [1/10], Step [496/625], Loss: 1.5126\n",
      "Epoch [1/10], Step [497/625], Loss: 1.4048\n",
      "Epoch [1/10], Step [498/625], Loss: 1.4041\n",
      "Epoch [1/10], Step [499/625], Loss: 1.6703\n",
      "Epoch [1/10], Step [500/625], Loss: 1.7624\n",
      "Epoch [1/10], Step [501/625], Loss: 1.6211\n",
      "Epoch [1/10], Step [502/625], Loss: 1.2752\n",
      "Epoch [1/10], Step [503/625], Loss: 1.6742\n",
      "Epoch [1/10], Step [504/625], Loss: 1.4055\n",
      "Epoch [1/10], Step [505/625], Loss: 1.8780\n",
      "Epoch [1/10], Step [506/625], Loss: 1.4005\n",
      "Epoch [1/10], Step [507/625], Loss: 1.2001\n",
      "Epoch [1/10], Step [508/625], Loss: 1.2830\n",
      "Epoch [1/10], Step [509/625], Loss: 1.6546\n",
      "Epoch [1/10], Step [510/625], Loss: 1.6239\n",
      "Epoch [1/10], Step [511/625], Loss: 1.8781\n",
      "Epoch [1/10], Step [512/625], Loss: 1.3863\n",
      "Epoch [1/10], Step [513/625], Loss: 1.4001\n",
      "Epoch [1/10], Step [514/625], Loss: 1.1859\n",
      "Epoch [1/10], Step [515/625], Loss: 1.2384\n",
      "Epoch [1/10], Step [516/625], Loss: 1.3564\n",
      "Epoch [1/10], Step [517/625], Loss: 1.1015\n",
      "Epoch [1/10], Step [518/625], Loss: 1.2130\n",
      "Epoch [1/10], Step [519/625], Loss: 1.6566\n",
      "Epoch [1/10], Step [520/625], Loss: 1.4044\n",
      "Epoch [1/10], Step [521/625], Loss: 1.2841\n",
      "Epoch [1/10], Step [522/625], Loss: 1.6774\n",
      "Epoch [1/10], Step [523/625], Loss: 1.1560\n",
      "Epoch [1/10], Step [524/625], Loss: 1.5039\n",
      "Epoch [1/10], Step [525/625], Loss: 1.4047\n",
      "Epoch [1/10], Step [526/625], Loss: 1.3520\n",
      "Epoch [1/10], Step [527/625], Loss: 0.9059\n",
      "Epoch [1/10], Step [528/625], Loss: 1.6894\n",
      "Epoch [1/10], Step [529/625], Loss: 1.9027\n",
      "Epoch [1/10], Step [530/625], Loss: 1.8854\n",
      "Epoch [1/10], Step [531/625], Loss: 1.4053\n",
      "Epoch [1/10], Step [532/625], Loss: 1.1564\n",
      "Epoch [1/10], Step [533/625], Loss: 1.4499\n",
      "Epoch [1/10], Step [534/625], Loss: 1.6439\n",
      "Epoch [1/10], Step [535/625], Loss: 1.4302\n",
      "Epoch [1/10], Step [536/625], Loss: 1.3888\n",
      "Epoch [1/10], Step [537/625], Loss: 1.2101\n",
      "Epoch [1/10], Step [538/625], Loss: 1.4072\n",
      "Epoch [1/10], Step [539/625], Loss: 1.5272\n",
      "Epoch [1/10], Step [540/625], Loss: 1.4685\n",
      "Epoch [1/10], Step [541/625], Loss: 1.4179\n",
      "Epoch [1/10], Step [542/625], Loss: 1.6520\n",
      "Epoch [1/10], Step [543/625], Loss: 1.1071\n",
      "Epoch [1/10], Step [544/625], Loss: 1.4118\n",
      "Epoch [1/10], Step [545/625], Loss: 1.1589\n",
      "Epoch [1/10], Step [546/625], Loss: 1.6585\n",
      "Epoch [1/10], Step [547/625], Loss: 1.4119\n",
      "Epoch [1/10], Step [548/625], Loss: 1.1551\n",
      "Epoch [1/10], Step [549/625], Loss: 1.3910\n",
      "Epoch [1/10], Step [550/625], Loss: 1.4044\n",
      "Epoch [1/10], Step [551/625], Loss: 1.1535\n",
      "Epoch [1/10], Step [552/625], Loss: 1.8177\n",
      "Epoch [1/10], Step [553/625], Loss: 1.2007\n",
      "Epoch [1/10], Step [554/625], Loss: 1.1560\n",
      "Epoch [1/10], Step [555/625], Loss: 1.6538\n",
      "Epoch [1/10], Step [556/625], Loss: 1.5553\n",
      "Epoch [1/10], Step [557/625], Loss: 1.6548\n",
      "Epoch [1/10], Step [558/625], Loss: 0.9072\n",
      "Epoch [1/10], Step [559/625], Loss: 1.3140\n",
      "Epoch [1/10], Step [560/625], Loss: 1.3975\n",
      "Epoch [1/10], Step [561/625], Loss: 1.4352\n",
      "Epoch [1/10], Step [562/625], Loss: 1.6531\n",
      "Epoch [1/10], Step [563/625], Loss: 1.4030\n",
      "Epoch [1/10], Step [564/625], Loss: 1.3887\n",
      "Epoch [1/10], Step [565/625], Loss: 1.9031\n",
      "Epoch [1/10], Step [566/625], Loss: 1.2650\n",
      "Epoch [1/10], Step [567/625], Loss: 1.9035\n",
      "Epoch [1/10], Step [568/625], Loss: 1.6527\n",
      "Epoch [1/10], Step [569/625], Loss: 1.6424\n",
      "Epoch [1/10], Step [570/625], Loss: 1.6548\n",
      "Epoch [1/10], Step [571/625], Loss: 1.3480\n",
      "Epoch [1/10], Step [572/625], Loss: 1.4790\n",
      "Epoch [1/10], Step [573/625], Loss: 1.1330\n",
      "Epoch [1/10], Step [574/625], Loss: 1.4872\n",
      "Epoch [1/10], Step [575/625], Loss: 1.6470\n",
      "Epoch [1/10], Step [576/625], Loss: 1.5987\n",
      "Epoch [1/10], Step [577/625], Loss: 1.1612\n",
      "Epoch [1/10], Step [578/625], Loss: 1.3053\n",
      "Epoch [1/10], Step [579/625], Loss: 1.1224\n",
      "Epoch [1/10], Step [580/625], Loss: 1.2063\n",
      "Epoch [1/10], Step [581/625], Loss: 1.6496\n",
      "Epoch [1/10], Step [582/625], Loss: 1.6572\n",
      "Epoch [1/10], Step [583/625], Loss: 1.6081\n",
      "Epoch [1/10], Step [584/625], Loss: 1.3893\n",
      "Epoch [1/10], Step [585/625], Loss: 1.1586\n",
      "Epoch [1/10], Step [586/625], Loss: 1.6060\n",
      "Epoch [1/10], Step [587/625], Loss: 1.2321\n",
      "Epoch [1/10], Step [588/625], Loss: 1.1549\n",
      "Epoch [1/10], Step [589/625], Loss: 1.6377\n",
      "Epoch [1/10], Step [590/625], Loss: 1.0922\n",
      "Epoch [1/10], Step [591/625], Loss: 1.5262\n",
      "Epoch [1/10], Step [592/625], Loss: 1.1835\n",
      "Epoch [1/10], Step [593/625], Loss: 1.6417\n",
      "Epoch [1/10], Step [594/625], Loss: 0.9153\n",
      "Epoch [1/10], Step [595/625], Loss: 1.4050\n",
      "Epoch [1/10], Step [596/625], Loss: 1.4056\n",
      "Epoch [1/10], Step [597/625], Loss: 1.4007\n",
      "Epoch [1/10], Step [598/625], Loss: 1.4055\n",
      "Epoch [1/10], Step [599/625], Loss: 1.4120\n",
      "Epoch [1/10], Step [600/625], Loss: 1.4237\n",
      "Epoch [1/10], Step [601/625], Loss: 1.6614\n",
      "Epoch [1/10], Step [602/625], Loss: 1.6548\n",
      "Epoch [1/10], Step [603/625], Loss: 1.3664\n",
      "Epoch [1/10], Step [604/625], Loss: 1.3992\n",
      "Epoch [1/10], Step [605/625], Loss: 0.9050\n",
      "Epoch [1/10], Step [606/625], Loss: 1.3574\n",
      "Epoch [1/10], Step [607/625], Loss: 1.3953\n",
      "Epoch [1/10], Step [608/625], Loss: 1.1551\n",
      "Epoch [1/10], Step [609/625], Loss: 1.4028\n",
      "Epoch [1/10], Step [610/625], Loss: 1.4017\n",
      "Epoch [1/10], Step [611/625], Loss: 1.6483\n",
      "Epoch [1/10], Step [612/625], Loss: 1.2952\n",
      "Epoch [1/10], Step [613/625], Loss: 1.4037\n",
      "Epoch [1/10], Step [614/625], Loss: 1.5790\n",
      "Epoch [1/10], Step [615/625], Loss: 1.8398\n",
      "Epoch [1/10], Step [616/625], Loss: 1.3587\n",
      "Epoch [1/10], Step [617/625], Loss: 1.5844\n",
      "Epoch [1/10], Step [618/625], Loss: 1.4552\n",
      "Epoch [1/10], Step [619/625], Loss: 1.1620\n",
      "Epoch [1/10], Step [620/625], Loss: 1.0261\n",
      "Epoch [1/10], Step [621/625], Loss: 1.4909\n",
      "Epoch [1/10], Step [622/625], Loss: 1.4023\n",
      "Epoch [1/10], Step [623/625], Loss: 1.1533\n",
      "Epoch [1/10], Step [624/625], Loss: 1.4063\n",
      "Epoch [1/10], Step [625/625], Loss: 1.1550\n",
      "Epoch [2/10], Step [1/625], Loss: 1.1570\n",
      "Epoch [2/10], Step [2/625], Loss: 1.6372\n",
      "Epoch [2/10], Step [3/625], Loss: 1.4027\n",
      "Epoch [2/10], Step [4/625], Loss: 1.4191\n",
      "Epoch [2/10], Step [5/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [6/625], Loss: 1.1792\n",
      "Epoch [2/10], Step [7/625], Loss: 1.3852\n",
      "Epoch [2/10], Step [8/625], Loss: 0.9078\n",
      "Epoch [2/10], Step [9/625], Loss: 1.3763\n",
      "Epoch [2/10], Step [10/625], Loss: 1.1728\n",
      "Epoch [2/10], Step [11/625], Loss: 1.1026\n",
      "Epoch [2/10], Step [12/625], Loss: 1.1572\n",
      "Epoch [2/10], Step [13/625], Loss: 1.0505\n",
      "Epoch [2/10], Step [14/625], Loss: 1.2859\n",
      "Epoch [2/10], Step [15/625], Loss: 1.2987\n",
      "Epoch [2/10], Step [16/625], Loss: 1.3328\n",
      "Epoch [2/10], Step [17/625], Loss: 1.6479\n",
      "Epoch [2/10], Step [18/625], Loss: 1.6467\n",
      "Epoch [2/10], Step [19/625], Loss: 1.1574\n",
      "Epoch [2/10], Step [20/625], Loss: 1.4495\n",
      "Epoch [2/10], Step [21/625], Loss: 1.2194\n",
      "Epoch [2/10], Step [22/625], Loss: 1.3446\n",
      "Epoch [2/10], Step [23/625], Loss: 1.1571\n",
      "Epoch [2/10], Step [24/625], Loss: 1.1795\n",
      "Epoch [2/10], Step [25/625], Loss: 1.8916\n",
      "Epoch [2/10], Step [26/625], Loss: 1.1846\n",
      "Epoch [2/10], Step [27/625], Loss: 0.9050\n",
      "Epoch [2/10], Step [28/625], Loss: 1.6521\n",
      "Epoch [2/10], Step [29/625], Loss: 1.3894\n",
      "Epoch [2/10], Step [30/625], Loss: 1.1572\n",
      "Epoch [2/10], Step [31/625], Loss: 1.4047\n",
      "Epoch [2/10], Step [32/625], Loss: 1.6438\n",
      "Epoch [2/10], Step [33/625], Loss: 1.6540\n",
      "Epoch [2/10], Step [34/625], Loss: 1.4008\n",
      "Epoch [2/10], Step [35/625], Loss: 0.9848\n",
      "Epoch [2/10], Step [36/625], Loss: 1.6498\n",
      "Epoch [2/10], Step [37/625], Loss: 1.6052\n",
      "Epoch [2/10], Step [38/625], Loss: 1.6405\n",
      "Epoch [2/10], Step [39/625], Loss: 1.3229\n",
      "Epoch [2/10], Step [40/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [41/625], Loss: 1.1676\n",
      "Epoch [2/10], Step [42/625], Loss: 1.4711\n",
      "Epoch [2/10], Step [43/625], Loss: 1.3947\n",
      "Epoch [2/10], Step [44/625], Loss: 1.4353\n",
      "Epoch [2/10], Step [45/625], Loss: 1.6176\n",
      "Epoch [2/10], Step [46/625], Loss: 1.1297\n",
      "Epoch [2/10], Step [47/625], Loss: 1.3056\n",
      "Epoch [2/10], Step [48/625], Loss: 1.4045\n",
      "Epoch [2/10], Step [49/625], Loss: 1.1599\n",
      "Epoch [2/10], Step [50/625], Loss: 1.1612\n",
      "Epoch [2/10], Step [51/625], Loss: 1.4033\n",
      "Epoch [2/10], Step [52/625], Loss: 1.4160\n",
      "Epoch [2/10], Step [53/625], Loss: 1.6527\n",
      "Epoch [2/10], Step [54/625], Loss: 1.3939\n",
      "Epoch [2/10], Step [55/625], Loss: 1.4032\n",
      "Epoch [2/10], Step [56/625], Loss: 1.4042\n",
      "Epoch [2/10], Step [57/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [58/625], Loss: 1.3810\n",
      "Epoch [2/10], Step [59/625], Loss: 0.9390\n",
      "Epoch [2/10], Step [60/625], Loss: 1.1466\n",
      "Epoch [2/10], Step [61/625], Loss: 1.4035\n",
      "Epoch [2/10], Step [62/625], Loss: 1.4712\n",
      "Epoch [2/10], Step [63/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [64/625], Loss: 1.2075\n",
      "Epoch [2/10], Step [65/625], Loss: 1.1546\n",
      "Epoch [2/10], Step [66/625], Loss: 1.1070\n",
      "Epoch [2/10], Step [67/625], Loss: 1.3249\n",
      "Epoch [2/10], Step [68/625], Loss: 0.9050\n",
      "Epoch [2/10], Step [69/625], Loss: 1.4187\n",
      "Epoch [2/10], Step [70/625], Loss: 1.8403\n",
      "Epoch [2/10], Step [71/625], Loss: 0.9054\n",
      "Epoch [2/10], Step [72/625], Loss: 1.0242\n",
      "Epoch [2/10], Step [73/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [74/625], Loss: 1.1525\n",
      "Epoch [2/10], Step [75/625], Loss: 1.1484\n",
      "Epoch [2/10], Step [76/625], Loss: 1.1628\n",
      "Epoch [2/10], Step [77/625], Loss: 1.7651\n",
      "Epoch [2/10], Step [78/625], Loss: 1.2264\n",
      "Epoch [2/10], Step [79/625], Loss: 1.1654\n",
      "Epoch [2/10], Step [80/625], Loss: 1.6434\n",
      "Epoch [2/10], Step [81/625], Loss: 1.5899\n",
      "Epoch [2/10], Step [82/625], Loss: 1.5700\n",
      "Epoch [2/10], Step [83/625], Loss: 1.3912\n",
      "Epoch [2/10], Step [84/625], Loss: 1.1765\n",
      "Epoch [2/10], Step [85/625], Loss: 1.1440\n",
      "Epoch [2/10], Step [86/625], Loss: 1.1962\n",
      "Epoch [2/10], Step [87/625], Loss: 1.1646\n",
      "Epoch [2/10], Step [88/625], Loss: 1.3119\n",
      "Epoch [2/10], Step [89/625], Loss: 1.6383\n",
      "Epoch [2/10], Step [90/625], Loss: 1.4046\n",
      "Epoch [2/10], Step [91/625], Loss: 1.5371\n",
      "Epoch [2/10], Step [92/625], Loss: 1.3020\n",
      "Epoch [2/10], Step [93/625], Loss: 1.1514\n",
      "Epoch [2/10], Step [94/625], Loss: 1.1591\n",
      "Epoch [2/10], Step [95/625], Loss: 1.3681\n",
      "Epoch [2/10], Step [96/625], Loss: 1.6391\n",
      "Epoch [2/10], Step [97/625], Loss: 1.1555\n",
      "Epoch [2/10], Step [98/625], Loss: 1.6551\n",
      "Epoch [2/10], Step [99/625], Loss: 1.3722\n",
      "Epoch [2/10], Step [100/625], Loss: 1.5240\n",
      "Epoch [2/10], Step [101/625], Loss: 1.3581\n",
      "Epoch [2/10], Step [102/625], Loss: 1.8845\n",
      "Epoch [2/10], Step [103/625], Loss: 1.1549\n",
      "Epoch [2/10], Step [104/625], Loss: 1.3896\n",
      "Epoch [2/10], Step [105/625], Loss: 1.6506\n",
      "Epoch [2/10], Step [106/625], Loss: 1.5541\n",
      "Epoch [2/10], Step [107/625], Loss: 1.1818\n",
      "Epoch [2/10], Step [108/625], Loss: 1.2740\n",
      "Epoch [2/10], Step [109/625], Loss: 1.4373\n",
      "Epoch [2/10], Step [110/625], Loss: 1.4114\n",
      "Epoch [2/10], Step [111/625], Loss: 1.2590\n",
      "Epoch [2/10], Step [112/625], Loss: 1.1551\n",
      "Epoch [2/10], Step [113/625], Loss: 1.1970\n",
      "Epoch [2/10], Step [114/625], Loss: 1.3276\n",
      "Epoch [2/10], Step [115/625], Loss: 1.2116\n",
      "Epoch [2/10], Step [116/625], Loss: 1.1715\n",
      "Epoch [2/10], Step [117/625], Loss: 1.4793\n",
      "Epoch [2/10], Step [118/625], Loss: 1.6579\n",
      "Epoch [2/10], Step [119/625], Loss: 1.3908\n",
      "Epoch [2/10], Step [120/625], Loss: 1.2543\n",
      "Epoch [2/10], Step [121/625], Loss: 1.2427\n",
      "Epoch [2/10], Step [122/625], Loss: 1.3379\n",
      "Epoch [2/10], Step [123/625], Loss: 1.6221\n",
      "Epoch [2/10], Step [124/625], Loss: 0.9049\n",
      "Epoch [2/10], Step [125/625], Loss: 1.3965\n",
      "Epoch [2/10], Step [126/625], Loss: 0.9056\n",
      "Epoch [2/10], Step [127/625], Loss: 1.1569\n",
      "Epoch [2/10], Step [128/625], Loss: 1.3878\n",
      "Epoch [2/10], Step [129/625], Loss: 1.1360\n",
      "Epoch [2/10], Step [130/625], Loss: 1.1909\n",
      "Epoch [2/10], Step [131/625], Loss: 1.4390\n",
      "Epoch [2/10], Step [132/625], Loss: 1.1540\n",
      "Epoch [2/10], Step [133/625], Loss: 1.3686\n",
      "Epoch [2/10], Step [134/625], Loss: 1.2972\n",
      "Epoch [2/10], Step [135/625], Loss: 1.0637\n",
      "Epoch [2/10], Step [136/625], Loss: 1.1582\n",
      "Epoch [2/10], Step [137/625], Loss: 1.3055\n",
      "Epoch [2/10], Step [138/625], Loss: 1.7351\n",
      "Epoch [2/10], Step [139/625], Loss: 1.1706\n",
      "Epoch [2/10], Step [140/625], Loss: 1.4125\n",
      "Epoch [2/10], Step [141/625], Loss: 1.4034\n",
      "Epoch [2/10], Step [142/625], Loss: 1.4777\n",
      "Epoch [2/10], Step [143/625], Loss: 1.1671\n",
      "Epoch [2/10], Step [144/625], Loss: 1.6672\n",
      "Epoch [2/10], Step [145/625], Loss: 1.1498\n",
      "Epoch [2/10], Step [146/625], Loss: 1.8736\n",
      "Epoch [2/10], Step [147/625], Loss: 1.8786\n",
      "Epoch [2/10], Step [148/625], Loss: 1.6324\n",
      "Epoch [2/10], Step [149/625], Loss: 1.6495\n",
      "Epoch [2/10], Step [150/625], Loss: 1.4040\n",
      "Epoch [2/10], Step [151/625], Loss: 1.4600\n",
      "Epoch [2/10], Step [152/625], Loss: 1.6187\n",
      "Epoch [2/10], Step [153/625], Loss: 1.3346\n",
      "Epoch [2/10], Step [154/625], Loss: 1.2950\n",
      "Epoch [2/10], Step [155/625], Loss: 1.4146\n",
      "Epoch [2/10], Step [156/625], Loss: 1.3570\n",
      "Epoch [2/10], Step [157/625], Loss: 1.1544\n",
      "Epoch [2/10], Step [158/625], Loss: 0.9049\n",
      "Epoch [2/10], Step [159/625], Loss: 1.6549\n",
      "Epoch [2/10], Step [160/625], Loss: 1.1636\n",
      "Epoch [2/10], Step [161/625], Loss: 1.6351\n",
      "Epoch [2/10], Step [162/625], Loss: 1.3481\n",
      "Epoch [2/10], Step [163/625], Loss: 1.3966\n",
      "Epoch [2/10], Step [164/625], Loss: 1.1487\n",
      "Epoch [2/10], Step [165/625], Loss: 1.7463\n",
      "Epoch [2/10], Step [166/625], Loss: 1.1551\n",
      "Epoch [2/10], Step [167/625], Loss: 1.3951\n",
      "Epoch [2/10], Step [168/625], Loss: 1.4057\n",
      "Epoch [2/10], Step [169/625], Loss: 1.6646\n",
      "Epoch [2/10], Step [170/625], Loss: 1.1557\n",
      "Epoch [2/10], Step [171/625], Loss: 1.3454\n",
      "Epoch [2/10], Step [172/625], Loss: 1.4296\n",
      "Epoch [2/10], Step [173/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [174/625], Loss: 1.6543\n",
      "Epoch [2/10], Step [175/625], Loss: 0.9337\n",
      "Epoch [2/10], Step [176/625], Loss: 1.6511\n",
      "Epoch [2/10], Step [177/625], Loss: 1.6422\n",
      "Epoch [2/10], Step [178/625], Loss: 1.8984\n",
      "Epoch [2/10], Step [179/625], Loss: 1.4333\n",
      "Epoch [2/10], Step [180/625], Loss: 1.1554\n",
      "Epoch [2/10], Step [181/625], Loss: 1.1846\n",
      "Epoch [2/10], Step [182/625], Loss: 1.2465\n",
      "Epoch [2/10], Step [183/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [184/625], Loss: 1.6767\n",
      "Epoch [2/10], Step [185/625], Loss: 1.6134\n",
      "Epoch [2/10], Step [186/625], Loss: 1.6597\n",
      "Epoch [2/10], Step [187/625], Loss: 1.6741\n",
      "Epoch [2/10], Step [188/625], Loss: 1.2009\n",
      "Epoch [2/10], Step [189/625], Loss: 1.6081\n",
      "Epoch [2/10], Step [190/625], Loss: 1.5236\n",
      "Epoch [2/10], Step [191/625], Loss: 1.4634\n",
      "Epoch [2/10], Step [192/625], Loss: 1.1717\n",
      "Epoch [2/10], Step [193/625], Loss: 1.4050\n",
      "Epoch [2/10], Step [194/625], Loss: 1.9045\n",
      "Epoch [2/10], Step [195/625], Loss: 1.6328\n",
      "Epoch [2/10], Step [196/625], Loss: 1.8998\n",
      "Epoch [2/10], Step [197/625], Loss: 1.3897\n",
      "Epoch [2/10], Step [198/625], Loss: 1.6518\n",
      "Epoch [2/10], Step [199/625], Loss: 1.3985\n",
      "Epoch [2/10], Step [200/625], Loss: 1.6282\n",
      "Epoch [2/10], Step [201/625], Loss: 1.2924\n",
      "Epoch [2/10], Step [202/625], Loss: 1.5965\n",
      "Epoch [2/10], Step [203/625], Loss: 1.4046\n",
      "Epoch [2/10], Step [204/625], Loss: 1.6725\n",
      "Epoch [2/10], Step [205/625], Loss: 1.4060\n",
      "Epoch [2/10], Step [206/625], Loss: 1.3826\n",
      "Epoch [2/10], Step [207/625], Loss: 1.7369\n",
      "Epoch [2/10], Step [208/625], Loss: 1.6546\n",
      "Epoch [2/10], Step [209/625], Loss: 1.6083\n",
      "Epoch [2/10], Step [210/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [211/625], Loss: 1.1965\n",
      "Epoch [2/10], Step [212/625], Loss: 1.4041\n",
      "Epoch [2/10], Step [213/625], Loss: 1.5787\n",
      "Epoch [2/10], Step [214/625], Loss: 1.1834\n",
      "Epoch [2/10], Step [215/625], Loss: 1.4278\n",
      "Epoch [2/10], Step [216/625], Loss: 1.3893\n",
      "Epoch [2/10], Step [217/625], Loss: 1.4026\n",
      "Epoch [2/10], Step [218/625], Loss: 1.3415\n",
      "Epoch [2/10], Step [219/625], Loss: 1.1570\n",
      "Epoch [2/10], Step [220/625], Loss: 1.6375\n",
      "Epoch [2/10], Step [221/625], Loss: 1.4044\n",
      "Epoch [2/10], Step [222/625], Loss: 1.4042\n",
      "Epoch [2/10], Step [223/625], Loss: 1.4165\n",
      "Epoch [2/10], Step [224/625], Loss: 1.6506\n",
      "Epoch [2/10], Step [225/625], Loss: 1.4045\n",
      "Epoch [2/10], Step [226/625], Loss: 1.4403\n",
      "Epoch [2/10], Step [227/625], Loss: 1.5904\n",
      "Epoch [2/10], Step [228/625], Loss: 1.6188\n",
      "Epoch [2/10], Step [229/625], Loss: 0.9374\n",
      "Epoch [2/10], Step [230/625], Loss: 1.3274\n",
      "Epoch [2/10], Step [231/625], Loss: 1.0408\n",
      "Epoch [2/10], Step [232/625], Loss: 0.9879\n",
      "Epoch [2/10], Step [233/625], Loss: 1.4121\n",
      "Epoch [2/10], Step [234/625], Loss: 1.1321\n",
      "Epoch [2/10], Step [235/625], Loss: 1.4067\n",
      "Epoch [2/10], Step [236/625], Loss: 1.5426\n",
      "Epoch [2/10], Step [237/625], Loss: 0.9546\n",
      "Epoch [2/10], Step [238/625], Loss: 1.6461\n",
      "Epoch [2/10], Step [239/625], Loss: 1.8855\n",
      "Epoch [2/10], Step [240/625], Loss: 1.4198\n",
      "Epoch [2/10], Step [241/625], Loss: 1.3849\n",
      "Epoch [2/10], Step [242/625], Loss: 1.3496\n",
      "Epoch [2/10], Step [243/625], Loss: 1.3875\n",
      "Epoch [2/10], Step [244/625], Loss: 0.9210\n",
      "Epoch [2/10], Step [245/625], Loss: 1.5115\n",
      "Epoch [2/10], Step [246/625], Loss: 1.6523\n",
      "Epoch [2/10], Step [247/625], Loss: 1.1653\n",
      "Epoch [2/10], Step [248/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [249/625], Loss: 1.1594\n",
      "Epoch [2/10], Step [250/625], Loss: 1.4278\n",
      "Epoch [2/10], Step [251/625], Loss: 1.1814\n",
      "Epoch [2/10], Step [252/625], Loss: 1.4452\n",
      "Epoch [2/10], Step [253/625], Loss: 1.2119\n",
      "Epoch [2/10], Step [254/625], Loss: 1.1545\n",
      "Epoch [2/10], Step [255/625], Loss: 1.1896\n",
      "Epoch [2/10], Step [256/625], Loss: 1.1545\n",
      "Epoch [2/10], Step [257/625], Loss: 1.4193\n",
      "Epoch [2/10], Step [258/625], Loss: 1.3904\n",
      "Epoch [2/10], Step [259/625], Loss: 1.6496\n",
      "Epoch [2/10], Step [260/625], Loss: 0.9627\n",
      "Epoch [2/10], Step [261/625], Loss: 1.7171\n",
      "Epoch [2/10], Step [262/625], Loss: 1.4023\n",
      "Epoch [2/10], Step [263/625], Loss: 1.5368\n",
      "Epoch [2/10], Step [264/625], Loss: 1.1458\n",
      "Epoch [2/10], Step [265/625], Loss: 1.3911\n",
      "Epoch [2/10], Step [266/625], Loss: 1.4063\n",
      "Epoch [2/10], Step [267/625], Loss: 1.1813\n",
      "Epoch [2/10], Step [268/625], Loss: 1.4037\n",
      "Epoch [2/10], Step [269/625], Loss: 1.3011\n",
      "Epoch [2/10], Step [270/625], Loss: 1.4025\n",
      "Epoch [2/10], Step [271/625], Loss: 1.2820\n",
      "Epoch [2/10], Step [272/625], Loss: 1.8386\n",
      "Epoch [2/10], Step [273/625], Loss: 0.9568\n",
      "Epoch [2/10], Step [274/625], Loss: 1.4418\n",
      "Epoch [2/10], Step [275/625], Loss: 1.6537\n",
      "Epoch [2/10], Step [276/625], Loss: 1.3857\n",
      "Epoch [2/10], Step [277/625], Loss: 1.1503\n",
      "Epoch [2/10], Step [278/625], Loss: 1.3995\n",
      "Epoch [2/10], Step [279/625], Loss: 0.9064\n",
      "Epoch [2/10], Step [280/625], Loss: 1.5128\n",
      "Epoch [2/10], Step [281/625], Loss: 1.3980\n",
      "Epoch [2/10], Step [282/625], Loss: 1.6544\n",
      "Epoch [2/10], Step [283/625], Loss: 1.6385\n",
      "Epoch [2/10], Step [284/625], Loss: 1.4278\n",
      "Epoch [2/10], Step [285/625], Loss: 1.8900\n",
      "Epoch [2/10], Step [286/625], Loss: 0.9055\n",
      "Epoch [2/10], Step [287/625], Loss: 1.1557\n",
      "Epoch [2/10], Step [288/625], Loss: 1.1549\n",
      "Epoch [2/10], Step [289/625], Loss: 0.9049\n",
      "Epoch [2/10], Step [290/625], Loss: 1.3998\n",
      "Epoch [2/10], Step [291/625], Loss: 1.1882\n",
      "Epoch [2/10], Step [292/625], Loss: 1.1401\n",
      "Epoch [2/10], Step [293/625], Loss: 1.6542\n",
      "Epoch [2/10], Step [294/625], Loss: 1.4054\n",
      "Epoch [2/10], Step [295/625], Loss: 0.9149\n",
      "Epoch [2/10], Step [296/625], Loss: 1.4074\n",
      "Epoch [2/10], Step [297/625], Loss: 1.6017\n",
      "Epoch [2/10], Step [298/625], Loss: 1.8785\n",
      "Epoch [2/10], Step [299/625], Loss: 1.1564\n",
      "Epoch [2/10], Step [300/625], Loss: 1.3700\n",
      "Epoch [2/10], Step [301/625], Loss: 1.6363\n",
      "Epoch [2/10], Step [302/625], Loss: 1.9005\n",
      "Epoch [2/10], Step [303/625], Loss: 1.3946\n",
      "Epoch [2/10], Step [304/625], Loss: 1.6390\n",
      "Epoch [2/10], Step [305/625], Loss: 1.9017\n",
      "Epoch [2/10], Step [306/625], Loss: 1.3016\n",
      "Epoch [2/10], Step [307/625], Loss: 1.6417\n",
      "Epoch [2/10], Step [308/625], Loss: 1.7282\n",
      "Epoch [2/10], Step [309/625], Loss: 1.4284\n",
      "Epoch [2/10], Step [310/625], Loss: 1.4046\n",
      "Epoch [2/10], Step [311/625], Loss: 1.6490\n",
      "Epoch [2/10], Step [312/625], Loss: 1.3887\n",
      "Epoch [2/10], Step [313/625], Loss: 1.6543\n",
      "Epoch [2/10], Step [314/625], Loss: 1.3888\n",
      "Epoch [2/10], Step [315/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [316/625], Loss: 1.1455\n",
      "Epoch [2/10], Step [317/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [318/625], Loss: 1.4400\n",
      "Epoch [2/10], Step [319/625], Loss: 1.1541\n",
      "Epoch [2/10], Step [320/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [321/625], Loss: 1.5943\n",
      "Epoch [2/10], Step [322/625], Loss: 1.3885\n",
      "Epoch [2/10], Step [323/625], Loss: 1.2906\n",
      "Epoch [2/10], Step [324/625], Loss: 1.4068\n",
      "Epoch [2/10], Step [325/625], Loss: 0.9048\n",
      "Epoch [2/10], Step [326/625], Loss: 1.4488\n",
      "Epoch [2/10], Step [327/625], Loss: 1.4009\n",
      "Epoch [2/10], Step [328/625], Loss: 1.6202\n",
      "Epoch [2/10], Step [329/625], Loss: 1.2948\n",
      "Epoch [2/10], Step [330/625], Loss: 1.4077\n",
      "Epoch [2/10], Step [331/625], Loss: 1.1753\n",
      "Epoch [2/10], Step [332/625], Loss: 1.4319\n",
      "Epoch [2/10], Step [333/625], Loss: 1.3425\n",
      "Epoch [2/10], Step [334/625], Loss: 1.1535\n",
      "Epoch [2/10], Step [335/625], Loss: 0.9048\n",
      "Epoch [2/10], Step [336/625], Loss: 1.5377\n",
      "Epoch [2/10], Step [337/625], Loss: 1.1550\n",
      "Epoch [2/10], Step [338/625], Loss: 1.3111\n",
      "Epoch [2/10], Step [339/625], Loss: 1.2214\n",
      "Epoch [2/10], Step [340/625], Loss: 1.1578\n",
      "Epoch [2/10], Step [341/625], Loss: 1.6561\n",
      "Epoch [2/10], Step [342/625], Loss: 1.5536\n",
      "Epoch [2/10], Step [343/625], Loss: 1.4329\n",
      "Epoch [2/10], Step [344/625], Loss: 1.4201\n",
      "Epoch [2/10], Step [345/625], Loss: 1.4090\n",
      "Epoch [2/10], Step [346/625], Loss: 1.5614\n",
      "Epoch [2/10], Step [347/625], Loss: 1.1549\n",
      "Epoch [2/10], Step [348/625], Loss: 1.0956\n",
      "Epoch [2/10], Step [349/625], Loss: 1.3860\n",
      "Epoch [2/10], Step [350/625], Loss: 1.1550\n",
      "Epoch [2/10], Step [351/625], Loss: 1.8047\n",
      "Epoch [2/10], Step [352/625], Loss: 1.3853\n",
      "Epoch [2/10], Step [353/625], Loss: 1.6529\n",
      "Epoch [2/10], Step [354/625], Loss: 0.9612\n",
      "Epoch [2/10], Step [355/625], Loss: 1.4056\n",
      "Epoch [2/10], Step [356/625], Loss: 0.9376\n",
      "Epoch [2/10], Step [357/625], Loss: 1.3968\n",
      "Epoch [2/10], Step [358/625], Loss: 1.4235\n",
      "Epoch [2/10], Step [359/625], Loss: 1.4039\n",
      "Epoch [2/10], Step [360/625], Loss: 1.4148\n",
      "Epoch [2/10], Step [361/625], Loss: 1.3514\n",
      "Epoch [2/10], Step [362/625], Loss: 1.6151\n",
      "Epoch [2/10], Step [363/625], Loss: 1.4797\n",
      "Epoch [2/10], Step [364/625], Loss: 1.8324\n",
      "Epoch [2/10], Step [365/625], Loss: 1.5701\n",
      "Epoch [2/10], Step [366/625], Loss: 1.1401\n",
      "Epoch [2/10], Step [367/625], Loss: 1.6265\n",
      "Epoch [2/10], Step [368/625], Loss: 1.1597\n",
      "Epoch [2/10], Step [369/625], Loss: 1.8147\n",
      "Epoch [2/10], Step [370/625], Loss: 1.5389\n",
      "Epoch [2/10], Step [371/625], Loss: 1.3942\n",
      "Epoch [2/10], Step [372/625], Loss: 1.4061\n",
      "Epoch [2/10], Step [373/625], Loss: 1.1547\n",
      "Epoch [2/10], Step [374/625], Loss: 1.1583\n",
      "Epoch [2/10], Step [375/625], Loss: 1.5160\n",
      "Epoch [2/10], Step [376/625], Loss: 1.4140\n",
      "Epoch [2/10], Step [377/625], Loss: 1.4030\n",
      "Epoch [2/10], Step [378/625], Loss: 1.3742\n",
      "Epoch [2/10], Step [379/625], Loss: 1.8954\n",
      "Epoch [2/10], Step [380/625], Loss: 1.6534\n",
      "Epoch [2/10], Step [381/625], Loss: 1.4201\n",
      "Epoch [2/10], Step [382/625], Loss: 1.2171\n",
      "Epoch [2/10], Step [383/625], Loss: 1.4127\n",
      "Epoch [2/10], Step [384/625], Loss: 1.1669\n",
      "Epoch [2/10], Step [385/625], Loss: 1.4317\n",
      "Epoch [2/10], Step [386/625], Loss: 1.7184\n",
      "Epoch [2/10], Step [387/625], Loss: 1.4216\n",
      "Epoch [2/10], Step [388/625], Loss: 1.6274\n",
      "Epoch [2/10], Step [389/625], Loss: 1.2100\n",
      "Epoch [2/10], Step [390/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [391/625], Loss: 1.4047\n",
      "Epoch [2/10], Step [392/625], Loss: 1.5803\n",
      "Epoch [2/10], Step [393/625], Loss: 0.9049\n",
      "Epoch [2/10], Step [394/625], Loss: 1.0962\n",
      "Epoch [2/10], Step [395/625], Loss: 1.4047\n",
      "Epoch [2/10], Step [396/625], Loss: 1.2392\n",
      "Epoch [2/10], Step [397/625], Loss: 1.3352\n",
      "Epoch [2/10], Step [398/625], Loss: 1.6587\n",
      "Epoch [2/10], Step [399/625], Loss: 1.1666\n",
      "Epoch [2/10], Step [400/625], Loss: 1.5751\n",
      "Epoch [2/10], Step [401/625], Loss: 1.2106\n",
      "Epoch [2/10], Step [402/625], Loss: 1.3690\n",
      "Epoch [2/10], Step [403/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [404/625], Loss: 1.6534\n",
      "Epoch [2/10], Step [405/625], Loss: 1.7169\n",
      "Epoch [2/10], Step [406/625], Loss: 1.4211\n",
      "Epoch [2/10], Step [407/625], Loss: 1.1557\n",
      "Epoch [2/10], Step [408/625], Loss: 1.3977\n",
      "Epoch [2/10], Step [409/625], Loss: 1.6336\n",
      "Epoch [2/10], Step [410/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [411/625], Loss: 1.4864\n",
      "Epoch [2/10], Step [412/625], Loss: 1.5851\n",
      "Epoch [2/10], Step [413/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [414/625], Loss: 1.4041\n",
      "Epoch [2/10], Step [415/625], Loss: 1.1557\n",
      "Epoch [2/10], Step [416/625], Loss: 1.3812\n",
      "Epoch [2/10], Step [417/625], Loss: 1.4047\n",
      "Epoch [2/10], Step [418/625], Loss: 1.1556\n",
      "Epoch [2/10], Step [419/625], Loss: 1.3867\n",
      "Epoch [2/10], Step [420/625], Loss: 1.3991\n",
      "Epoch [2/10], Step [421/625], Loss: 1.7552\n",
      "Epoch [2/10], Step [422/625], Loss: 1.7622\n",
      "Epoch [2/10], Step [423/625], Loss: 1.1416\n",
      "Epoch [2/10], Step [424/625], Loss: 1.2972\n",
      "Epoch [2/10], Step [425/625], Loss: 0.9681\n",
      "Epoch [2/10], Step [426/625], Loss: 1.1563\n",
      "Epoch [2/10], Step [427/625], Loss: 1.4082\n",
      "Epoch [2/10], Step [428/625], Loss: 1.4829\n",
      "Epoch [2/10], Step [429/625], Loss: 1.6525\n",
      "Epoch [2/10], Step [430/625], Loss: 1.6290\n",
      "Epoch [2/10], Step [431/625], Loss: 1.4941\n",
      "Epoch [2/10], Step [432/625], Loss: 1.1520\n",
      "Epoch [2/10], Step [433/625], Loss: 1.6235\n",
      "Epoch [2/10], Step [434/625], Loss: 1.6549\n",
      "Epoch [2/10], Step [435/625], Loss: 1.1171\n",
      "Epoch [2/10], Step [436/625], Loss: 1.5209\n",
      "Epoch [2/10], Step [437/625], Loss: 1.6478\n",
      "Epoch [2/10], Step [438/625], Loss: 1.9034\n",
      "Epoch [2/10], Step [439/625], Loss: 1.4120\n",
      "Epoch [2/10], Step [440/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [441/625], Loss: 1.4654\n",
      "Epoch [2/10], Step [442/625], Loss: 1.4111\n",
      "Epoch [2/10], Step [443/625], Loss: 1.1545\n",
      "Epoch [2/10], Step [444/625], Loss: 1.1551\n",
      "Epoch [2/10], Step [445/625], Loss: 1.3965\n",
      "Epoch [2/10], Step [446/625], Loss: 1.4862\n",
      "Epoch [2/10], Step [447/625], Loss: 1.8301\n",
      "Epoch [2/10], Step [448/625], Loss: 1.4068\n",
      "Epoch [2/10], Step [449/625], Loss: 1.4029\n",
      "Epoch [2/10], Step [450/625], Loss: 1.3660\n",
      "Epoch [2/10], Step [451/625], Loss: 0.9203\n",
      "Epoch [2/10], Step [452/625], Loss: 1.2327\n",
      "Epoch [2/10], Step [453/625], Loss: 1.4345\n",
      "Epoch [2/10], Step [454/625], Loss: 1.1617\n",
      "Epoch [2/10], Step [455/625], Loss: 1.1505\n",
      "Epoch [2/10], Step [456/625], Loss: 1.1568\n",
      "Epoch [2/10], Step [457/625], Loss: 1.0313\n",
      "Epoch [2/10], Step [458/625], Loss: 1.4803\n",
      "Epoch [2/10], Step [459/625], Loss: 1.3294\n",
      "Epoch [2/10], Step [460/625], Loss: 1.4050\n",
      "Epoch [2/10], Step [461/625], Loss: 1.2969\n",
      "Epoch [2/10], Step [462/625], Loss: 1.1502\n",
      "Epoch [2/10], Step [463/625], Loss: 1.1710\n",
      "Epoch [2/10], Step [464/625], Loss: 1.9046\n",
      "Epoch [2/10], Step [465/625], Loss: 1.5275\n",
      "Epoch [2/10], Step [466/625], Loss: 1.4346\n",
      "Epoch [2/10], Step [467/625], Loss: 1.4050\n",
      "Epoch [2/10], Step [468/625], Loss: 1.6539\n",
      "Epoch [2/10], Step [469/625], Loss: 1.0746\n",
      "Epoch [2/10], Step [470/625], Loss: 1.6548\n",
      "Epoch [2/10], Step [471/625], Loss: 1.4047\n",
      "Epoch [2/10], Step [472/625], Loss: 1.6047\n",
      "Epoch [2/10], Step [473/625], Loss: 1.4043\n",
      "Epoch [2/10], Step [474/625], Loss: 1.2215\n",
      "Epoch [2/10], Step [475/625], Loss: 1.3832\n",
      "Epoch [2/10], Step [476/625], Loss: 1.6519\n",
      "Epoch [2/10], Step [477/625], Loss: 1.8895\n",
      "Epoch [2/10], Step [478/625], Loss: 1.1564\n",
      "Epoch [2/10], Step [479/625], Loss: 1.1551\n",
      "Epoch [2/10], Step [480/625], Loss: 1.2094\n",
      "Epoch [2/10], Step [481/625], Loss: 1.0899\n",
      "Epoch [2/10], Step [482/625], Loss: 1.4054\n",
      "Epoch [2/10], Step [483/625], Loss: 1.3959\n",
      "Epoch [2/10], Step [484/625], Loss: 1.6375\n",
      "Epoch [2/10], Step [485/625], Loss: 1.4024\n",
      "Epoch [2/10], Step [486/625], Loss: 1.8026\n",
      "Epoch [2/10], Step [487/625], Loss: 1.4055\n",
      "Epoch [2/10], Step [488/625], Loss: 1.9046\n",
      "Epoch [2/10], Step [489/625], Loss: 1.9038\n",
      "Epoch [2/10], Step [490/625], Loss: 0.9048\n",
      "Epoch [2/10], Step [491/625], Loss: 1.6435\n",
      "Epoch [2/10], Step [492/625], Loss: 1.5460\n",
      "Epoch [2/10], Step [493/625], Loss: 1.1739\n",
      "Epoch [2/10], Step [494/625], Loss: 1.4010\n",
      "Epoch [2/10], Step [495/625], Loss: 1.8974\n",
      "Epoch [2/10], Step [496/625], Loss: 0.9061\n",
      "Epoch [2/10], Step [497/625], Loss: 1.1532\n",
      "Epoch [2/10], Step [498/625], Loss: 1.1552\n",
      "Epoch [2/10], Step [499/625], Loss: 1.4164\n",
      "Epoch [2/10], Step [500/625], Loss: 1.6551\n",
      "Epoch [2/10], Step [501/625], Loss: 1.4517\n",
      "Epoch [2/10], Step [502/625], Loss: 1.6536\n",
      "Epoch [2/10], Step [503/625], Loss: 1.4161\n",
      "Epoch [2/10], Step [504/625], Loss: 1.3343\n",
      "Epoch [2/10], Step [505/625], Loss: 1.3445\n",
      "Epoch [2/10], Step [506/625], Loss: 1.1425\n",
      "Epoch [2/10], Step [507/625], Loss: 1.2599\n",
      "Epoch [2/10], Step [508/625], Loss: 1.1579\n",
      "Epoch [2/10], Step [509/625], Loss: 1.6111\n",
      "Epoch [2/10], Step [510/625], Loss: 1.3863\n",
      "Epoch [2/10], Step [511/625], Loss: 1.4119\n",
      "Epoch [2/10], Step [512/625], Loss: 1.6175\n",
      "Epoch [2/10], Step [513/625], Loss: 1.1551\n",
      "Epoch [2/10], Step [514/625], Loss: 1.6387\n",
      "Epoch [2/10], Step [515/625], Loss: 1.6442\n",
      "Epoch [2/10], Step [516/625], Loss: 1.6439\n",
      "Epoch [2/10], Step [517/625], Loss: 1.3889\n",
      "Epoch [2/10], Step [518/625], Loss: 1.4985\n",
      "Epoch [2/10], Step [519/625], Loss: 1.4062\n",
      "Epoch [2/10], Step [520/625], Loss: 1.6775\n",
      "Epoch [2/10], Step [521/625], Loss: 1.1549\n",
      "Epoch [2/10], Step [522/625], Loss: 0.9050\n",
      "Epoch [2/10], Step [523/625], Loss: 1.3523\n",
      "Epoch [2/10], Step [524/625], Loss: 1.2176\n",
      "Epoch [2/10], Step [525/625], Loss: 1.9019\n",
      "Epoch [2/10], Step [526/625], Loss: 1.4083\n",
      "Epoch [2/10], Step [527/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [528/625], Loss: 0.9234\n",
      "Epoch [2/10], Step [529/625], Loss: 1.6928\n",
      "Epoch [2/10], Step [530/625], Loss: 1.4310\n",
      "Epoch [2/10], Step [531/625], Loss: 1.2327\n",
      "Epoch [2/10], Step [532/625], Loss: 1.5269\n",
      "Epoch [2/10], Step [533/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [534/625], Loss: 1.4019\n",
      "Epoch [2/10], Step [535/625], Loss: 1.2010\n",
      "Epoch [2/10], Step [536/625], Loss: 1.4053\n",
      "Epoch [2/10], Step [537/625], Loss: 1.6249\n",
      "Epoch [2/10], Step [538/625], Loss: 1.1656\n",
      "Epoch [2/10], Step [539/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [540/625], Loss: 1.0626\n",
      "Epoch [2/10], Step [541/625], Loss: 1.5023\n",
      "Epoch [2/10], Step [542/625], Loss: 1.6542\n",
      "Epoch [2/10], Step [543/625], Loss: 1.3250\n",
      "Epoch [2/10], Step [544/625], Loss: 1.6517\n",
      "Epoch [2/10], Step [545/625], Loss: 0.9114\n",
      "Epoch [2/10], Step [546/625], Loss: 1.1993\n",
      "Epoch [2/10], Step [547/625], Loss: 1.7104\n",
      "Epoch [2/10], Step [548/625], Loss: 1.6109\n",
      "Epoch [2/10], Step [549/625], Loss: 1.1562\n",
      "Epoch [2/10], Step [550/625], Loss: 1.1539\n",
      "Epoch [2/10], Step [551/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [552/625], Loss: 0.9049\n",
      "Epoch [2/10], Step [553/625], Loss: 1.3617\n",
      "Epoch [2/10], Step [554/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [555/625], Loss: 1.5130\n",
      "Epoch [2/10], Step [556/625], Loss: 1.4052\n",
      "Epoch [2/10], Step [557/625], Loss: 0.9354\n",
      "Epoch [2/10], Step [558/625], Loss: 1.4144\n",
      "Epoch [2/10], Step [559/625], Loss: 1.1387\n",
      "Epoch [2/10], Step [560/625], Loss: 1.1590\n",
      "Epoch [2/10], Step [561/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [562/625], Loss: 1.6242\n",
      "Epoch [2/10], Step [563/625], Loss: 1.6243\n",
      "Epoch [2/10], Step [564/625], Loss: 1.4084\n",
      "Epoch [2/10], Step [565/625], Loss: 1.1613\n",
      "Epoch [2/10], Step [566/625], Loss: 1.6591\n",
      "Epoch [2/10], Step [567/625], Loss: 1.4169\n",
      "Epoch [2/10], Step [568/625], Loss: 1.3952\n",
      "Epoch [2/10], Step [569/625], Loss: 1.6530\n",
      "Epoch [2/10], Step [570/625], Loss: 1.7923\n",
      "Epoch [2/10], Step [571/625], Loss: 1.1361\n",
      "Epoch [2/10], Step [572/625], Loss: 1.5937\n",
      "Epoch [2/10], Step [573/625], Loss: 1.6156\n",
      "Epoch [2/10], Step [574/625], Loss: 1.5006\n",
      "Epoch [2/10], Step [575/625], Loss: 1.1546\n",
      "Epoch [2/10], Step [576/625], Loss: 1.4667\n",
      "Epoch [2/10], Step [577/625], Loss: 1.4031\n",
      "Epoch [2/10], Step [578/625], Loss: 1.2528\n",
      "Epoch [2/10], Step [579/625], Loss: 1.1554\n",
      "Epoch [2/10], Step [580/625], Loss: 1.1577\n",
      "Epoch [2/10], Step [581/625], Loss: 1.4269\n",
      "Epoch [2/10], Step [582/625], Loss: 1.3128\n",
      "Epoch [2/10], Step [583/625], Loss: 1.4072\n",
      "Epoch [2/10], Step [584/625], Loss: 1.6499\n",
      "Epoch [2/10], Step [585/625], Loss: 1.2438\n",
      "Epoch [2/10], Step [586/625], Loss: 1.5375\n",
      "Epoch [2/10], Step [587/625], Loss: 1.1546\n",
      "Epoch [2/10], Step [588/625], Loss: 0.9048\n",
      "Epoch [2/10], Step [589/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [590/625], Loss: 1.4055\n",
      "Epoch [2/10], Step [591/625], Loss: 1.3917\n",
      "Epoch [2/10], Step [592/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [593/625], Loss: 1.3585\n",
      "Epoch [2/10], Step [594/625], Loss: 1.1875\n",
      "Epoch [2/10], Step [595/625], Loss: 1.5489\n",
      "Epoch [2/10], Step [596/625], Loss: 1.1505\n",
      "Epoch [2/10], Step [597/625], Loss: 1.3894\n",
      "Epoch [2/10], Step [598/625], Loss: 1.3968\n",
      "Epoch [2/10], Step [599/625], Loss: 1.4057\n",
      "Epoch [2/10], Step [600/625], Loss: 1.6685\n",
      "Epoch [2/10], Step [601/625], Loss: 1.6519\n",
      "Epoch [2/10], Step [602/625], Loss: 1.6629\n",
      "Epoch [2/10], Step [603/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [604/625], Loss: 1.1229\n",
      "Epoch [2/10], Step [605/625], Loss: 1.1548\n",
      "Epoch [2/10], Step [606/625], Loss: 1.4560\n",
      "Epoch [2/10], Step [607/625], Loss: 1.4338\n",
      "Epoch [2/10], Step [608/625], Loss: 0.9310\n",
      "Epoch [2/10], Step [609/625], Loss: 1.4048\n",
      "Epoch [2/10], Step [610/625], Loss: 1.6548\n",
      "Epoch [2/10], Step [611/625], Loss: 1.5095\n",
      "Epoch [2/10], Step [612/625], Loss: 1.1438\n",
      "Epoch [2/10], Step [613/625], Loss: 1.1749\n",
      "Epoch [2/10], Step [614/625], Loss: 1.1634\n",
      "Epoch [2/10], Step [615/625], Loss: 1.3990\n",
      "Epoch [2/10], Step [616/625], Loss: 1.4766\n",
      "Epoch [2/10], Step [617/625], Loss: 1.4046\n",
      "Epoch [2/10], Step [618/625], Loss: 1.6221\n",
      "Epoch [2/10], Step [619/625], Loss: 0.9050\n",
      "Epoch [2/10], Step [620/625], Loss: 1.6243\n",
      "Epoch [2/10], Step [621/625], Loss: 1.1544\n",
      "Epoch [2/10], Step [622/625], Loss: 1.1575\n",
      "Epoch [2/10], Step [623/625], Loss: 1.6586\n",
      "Epoch [2/10], Step [624/625], Loss: 1.5003\n",
      "Epoch [2/10], Step [625/625], Loss: 1.3970\n",
      "Epoch [3/10], Step [1/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [2/625], Loss: 1.3905\n",
      "Epoch [3/10], Step [3/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [4/625], Loss: 1.1530\n",
      "Epoch [3/10], Step [5/625], Loss: 1.4070\n",
      "Epoch [3/10], Step [6/625], Loss: 1.3991\n",
      "Epoch [3/10], Step [7/625], Loss: 1.3647\n",
      "Epoch [3/10], Step [8/625], Loss: 0.9225\n",
      "Epoch [3/10], Step [9/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [10/625], Loss: 0.9153\n",
      "Epoch [3/10], Step [11/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [12/625], Loss: 1.4120\n",
      "Epoch [3/10], Step [13/625], Loss: 1.4128\n",
      "Epoch [3/10], Step [14/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [15/625], Loss: 1.6450\n",
      "Epoch [3/10], Step [16/625], Loss: 1.1691\n",
      "Epoch [3/10], Step [17/625], Loss: 1.1524\n",
      "Epoch [3/10], Step [18/625], Loss: 1.6543\n",
      "Epoch [3/10], Step [19/625], Loss: 1.6509\n",
      "Epoch [3/10], Step [20/625], Loss: 1.1953\n",
      "Epoch [3/10], Step [21/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [22/625], Loss: 1.1701\n",
      "Epoch [3/10], Step [23/625], Loss: 0.9091\n",
      "Epoch [3/10], Step [24/625], Loss: 1.3046\n",
      "Epoch [3/10], Step [25/625], Loss: 1.5926\n",
      "Epoch [3/10], Step [26/625], Loss: 1.1621\n",
      "Epoch [3/10], Step [27/625], Loss: 1.3808\n",
      "Epoch [3/10], Step [28/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [29/625], Loss: 1.6501\n",
      "Epoch [3/10], Step [30/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [31/625], Loss: 1.4011\n",
      "Epoch [3/10], Step [32/625], Loss: 1.5988\n",
      "Epoch [3/10], Step [33/625], Loss: 1.1536\n",
      "Epoch [3/10], Step [34/625], Loss: 1.3716\n",
      "Epoch [3/10], Step [35/625], Loss: 1.4352\n",
      "Epoch [3/10], Step [36/625], Loss: 1.4017\n",
      "Epoch [3/10], Step [37/625], Loss: 0.9098\n",
      "Epoch [3/10], Step [38/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [39/625], Loss: 1.1521\n",
      "Epoch [3/10], Step [40/625], Loss: 1.1531\n",
      "Epoch [3/10], Step [41/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [42/625], Loss: 1.6549\n",
      "Epoch [3/10], Step [43/625], Loss: 1.5055\n",
      "Epoch [3/10], Step [44/625], Loss: 1.6382\n",
      "Epoch [3/10], Step [45/625], Loss: 1.1636\n",
      "Epoch [3/10], Step [46/625], Loss: 1.6370\n",
      "Epoch [3/10], Step [47/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [48/625], Loss: 1.4003\n",
      "Epoch [3/10], Step [49/625], Loss: 1.6511\n",
      "Epoch [3/10], Step [50/625], Loss: 1.5772\n",
      "Epoch [3/10], Step [51/625], Loss: 1.6387\n",
      "Epoch [3/10], Step [52/625], Loss: 1.4291\n",
      "Epoch [3/10], Step [53/625], Loss: 1.1494\n",
      "Epoch [3/10], Step [54/625], Loss: 1.1561\n",
      "Epoch [3/10], Step [55/625], Loss: 1.3798\n",
      "Epoch [3/10], Step [56/625], Loss: 1.4021\n",
      "Epoch [3/10], Step [57/625], Loss: 1.6520\n",
      "Epoch [3/10], Step [58/625], Loss: 1.1553\n",
      "Epoch [3/10], Step [59/625], Loss: 1.8222\n",
      "Epoch [3/10], Step [60/625], Loss: 1.8915\n",
      "Epoch [3/10], Step [61/625], Loss: 1.4121\n",
      "Epoch [3/10], Step [62/625], Loss: 1.6444\n",
      "Epoch [3/10], Step [63/625], Loss: 1.1080\n",
      "Epoch [3/10], Step [64/625], Loss: 1.2010\n",
      "Epoch [3/10], Step [65/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [66/625], Loss: 1.4842\n",
      "Epoch [3/10], Step [67/625], Loss: 1.1549\n",
      "Epoch [3/10], Step [68/625], Loss: 1.1496\n",
      "Epoch [3/10], Step [69/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [70/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [71/625], Loss: 1.5662\n",
      "Epoch [3/10], Step [72/625], Loss: 1.4441\n",
      "Epoch [3/10], Step [73/625], Loss: 1.3507\n",
      "Epoch [3/10], Step [74/625], Loss: 1.4034\n",
      "Epoch [3/10], Step [75/625], Loss: 1.6440\n",
      "Epoch [3/10], Step [76/625], Loss: 1.6503\n",
      "Epoch [3/10], Step [77/625], Loss: 1.1538\n",
      "Epoch [3/10], Step [78/625], Loss: 1.0486\n",
      "Epoch [3/10], Step [79/625], Loss: 1.1615\n",
      "Epoch [3/10], Step [80/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [81/625], Loss: 1.5635\n",
      "Epoch [3/10], Step [82/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [83/625], Loss: 1.1652\n",
      "Epoch [3/10], Step [84/625], Loss: 1.3750\n",
      "Epoch [3/10], Step [85/625], Loss: 1.2781\n",
      "Epoch [3/10], Step [86/625], Loss: 1.4436\n",
      "Epoch [3/10], Step [87/625], Loss: 1.2940\n",
      "Epoch [3/10], Step [88/625], Loss: 1.1787\n",
      "Epoch [3/10], Step [89/625], Loss: 1.1552\n",
      "Epoch [3/10], Step [90/625], Loss: 1.3833\n",
      "Epoch [3/10], Step [91/625], Loss: 1.6549\n",
      "Epoch [3/10], Step [92/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [93/625], Loss: 1.2184\n",
      "Epoch [3/10], Step [94/625], Loss: 0.9055\n",
      "Epoch [3/10], Step [95/625], Loss: 1.3981\n",
      "Epoch [3/10], Step [96/625], Loss: 1.8993\n",
      "Epoch [3/10], Step [97/625], Loss: 1.6197\n",
      "Epoch [3/10], Step [98/625], Loss: 1.2008\n",
      "Epoch [3/10], Step [99/625], Loss: 1.5189\n",
      "Epoch [3/10], Step [100/625], Loss: 1.5346\n",
      "Epoch [3/10], Step [101/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [102/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [103/625], Loss: 1.4059\n",
      "Epoch [3/10], Step [104/625], Loss: 1.1642\n",
      "Epoch [3/10], Step [105/625], Loss: 1.1547\n",
      "Epoch [3/10], Step [106/625], Loss: 1.4247\n",
      "Epoch [3/10], Step [107/625], Loss: 1.1462\n",
      "Epoch [3/10], Step [108/625], Loss: 1.1555\n",
      "Epoch [3/10], Step [109/625], Loss: 1.3109\n",
      "Epoch [3/10], Step [110/625], Loss: 1.4010\n",
      "Epoch [3/10], Step [111/625], Loss: 1.6477\n",
      "Epoch [3/10], Step [112/625], Loss: 1.3933\n",
      "Epoch [3/10], Step [113/625], Loss: 1.2082\n",
      "Epoch [3/10], Step [114/625], Loss: 1.4142\n",
      "Epoch [3/10], Step [115/625], Loss: 1.1589\n",
      "Epoch [3/10], Step [116/625], Loss: 1.4328\n",
      "Epoch [3/10], Step [117/625], Loss: 1.3227\n",
      "Epoch [3/10], Step [118/625], Loss: 1.4054\n",
      "Epoch [3/10], Step [119/625], Loss: 1.2291\n",
      "Epoch [3/10], Step [120/625], Loss: 1.6546\n",
      "Epoch [3/10], Step [121/625], Loss: 1.6544\n",
      "Epoch [3/10], Step [122/625], Loss: 1.8888\n",
      "Epoch [3/10], Step [123/625], Loss: 1.2160\n",
      "Epoch [3/10], Step [124/625], Loss: 1.4018\n",
      "Epoch [3/10], Step [125/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [126/625], Loss: 1.9043\n",
      "Epoch [3/10], Step [127/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [128/625], Loss: 1.4875\n",
      "Epoch [3/10], Step [129/625], Loss: 1.2620\n",
      "Epoch [3/10], Step [130/625], Loss: 1.1656\n",
      "Epoch [3/10], Step [131/625], Loss: 1.8815\n",
      "Epoch [3/10], Step [132/625], Loss: 1.3973\n",
      "Epoch [3/10], Step [133/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [134/625], Loss: 1.2376\n",
      "Epoch [3/10], Step [135/625], Loss: 1.3933\n",
      "Epoch [3/10], Step [136/625], Loss: 1.4044\n",
      "Epoch [3/10], Step [137/625], Loss: 1.4034\n",
      "Epoch [3/10], Step [138/625], Loss: 1.3981\n",
      "Epoch [3/10], Step [139/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [140/625], Loss: 1.2345\n",
      "Epoch [3/10], Step [141/625], Loss: 1.8912\n",
      "Epoch [3/10], Step [142/625], Loss: 1.8334\n",
      "Epoch [3/10], Step [143/625], Loss: 1.1550\n",
      "Epoch [3/10], Step [144/625], Loss: 1.6509\n",
      "Epoch [3/10], Step [145/625], Loss: 0.9163\n",
      "Epoch [3/10], Step [146/625], Loss: 1.6535\n",
      "Epoch [3/10], Step [147/625], Loss: 1.4054\n",
      "Epoch [3/10], Step [148/625], Loss: 1.6538\n",
      "Epoch [3/10], Step [149/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [150/625], Loss: 1.6576\n",
      "Epoch [3/10], Step [151/625], Loss: 1.6539\n",
      "Epoch [3/10], Step [152/625], Loss: 1.7665\n",
      "Epoch [3/10], Step [153/625], Loss: 0.9375\n",
      "Epoch [3/10], Step [154/625], Loss: 1.4210\n",
      "Epoch [3/10], Step [155/625], Loss: 1.1518\n",
      "Epoch [3/10], Step [156/625], Loss: 1.1886\n",
      "Epoch [3/10], Step [157/625], Loss: 1.1441\n",
      "Epoch [3/10], Step [158/625], Loss: 1.4004\n",
      "Epoch [3/10], Step [159/625], Loss: 1.1430\n",
      "Epoch [3/10], Step [160/625], Loss: 1.5330\n",
      "Epoch [3/10], Step [161/625], Loss: 1.1549\n",
      "Epoch [3/10], Step [162/625], Loss: 1.6296\n",
      "Epoch [3/10], Step [163/625], Loss: 1.1586\n",
      "Epoch [3/10], Step [164/625], Loss: 1.3390\n",
      "Epoch [3/10], Step [165/625], Loss: 1.1586\n",
      "Epoch [3/10], Step [166/625], Loss: 0.9056\n",
      "Epoch [3/10], Step [167/625], Loss: 1.1553\n",
      "Epoch [3/10], Step [168/625], Loss: 1.5208\n",
      "Epoch [3/10], Step [169/625], Loss: 1.4417\n",
      "Epoch [3/10], Step [170/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [171/625], Loss: 1.4639\n",
      "Epoch [3/10], Step [172/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [173/625], Loss: 1.1646\n",
      "Epoch [3/10], Step [174/625], Loss: 1.1553\n",
      "Epoch [3/10], Step [175/625], Loss: 1.3246\n",
      "Epoch [3/10], Step [176/625], Loss: 1.1468\n",
      "Epoch [3/10], Step [177/625], Loss: 0.9686\n",
      "Epoch [3/10], Step [178/625], Loss: 1.4035\n",
      "Epoch [3/10], Step [179/625], Loss: 1.1691\n",
      "Epoch [3/10], Step [180/625], Loss: 1.6524\n",
      "Epoch [3/10], Step [181/625], Loss: 1.2573\n",
      "Epoch [3/10], Step [182/625], Loss: 1.0680\n",
      "Epoch [3/10], Step [183/625], Loss: 1.5676\n",
      "Epoch [3/10], Step [184/625], Loss: 1.4004\n",
      "Epoch [3/10], Step [185/625], Loss: 1.7587\n",
      "Epoch [3/10], Step [186/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [187/625], Loss: 1.1325\n",
      "Epoch [3/10], Step [188/625], Loss: 1.4053\n",
      "Epoch [3/10], Step [189/625], Loss: 1.6518\n",
      "Epoch [3/10], Step [190/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [191/625], Loss: 1.3699\n",
      "Epoch [3/10], Step [192/625], Loss: 1.4082\n",
      "Epoch [3/10], Step [193/625], Loss: 1.1560\n",
      "Epoch [3/10], Step [194/625], Loss: 1.3950\n",
      "Epoch [3/10], Step [195/625], Loss: 1.6549\n",
      "Epoch [3/10], Step [196/625], Loss: 1.4094\n",
      "Epoch [3/10], Step [197/625], Loss: 0.9083\n",
      "Epoch [3/10], Step [198/625], Loss: 1.3932\n",
      "Epoch [3/10], Step [199/625], Loss: 0.9731\n",
      "Epoch [3/10], Step [200/625], Loss: 1.4056\n",
      "Epoch [3/10], Step [201/625], Loss: 1.2328\n",
      "Epoch [3/10], Step [202/625], Loss: 1.5998\n",
      "Epoch [3/10], Step [203/625], Loss: 1.1714\n",
      "Epoch [3/10], Step [204/625], Loss: 1.6420\n",
      "Epoch [3/10], Step [205/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [206/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [207/625], Loss: 0.9319\n",
      "Epoch [3/10], Step [208/625], Loss: 1.3866\n",
      "Epoch [3/10], Step [209/625], Loss: 1.1780\n",
      "Epoch [3/10], Step [210/625], Loss: 0.9314\n",
      "Epoch [3/10], Step [211/625], Loss: 1.6526\n",
      "Epoch [3/10], Step [212/625], Loss: 1.6347\n",
      "Epoch [3/10], Step [213/625], Loss: 1.1845\n",
      "Epoch [3/10], Step [214/625], Loss: 1.6363\n",
      "Epoch [3/10], Step [215/625], Loss: 1.3677\n",
      "Epoch [3/10], Step [216/625], Loss: 1.6535\n",
      "Epoch [3/10], Step [217/625], Loss: 1.6077\n",
      "Epoch [3/10], Step [218/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [219/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [220/625], Loss: 1.5650\n",
      "Epoch [3/10], Step [221/625], Loss: 1.4084\n",
      "Epoch [3/10], Step [222/625], Loss: 1.5550\n",
      "Epoch [3/10], Step [223/625], Loss: 1.1545\n",
      "Epoch [3/10], Step [224/625], Loss: 1.4052\n",
      "Epoch [3/10], Step [225/625], Loss: 1.6379\n",
      "Epoch [3/10], Step [226/625], Loss: 1.3916\n",
      "Epoch [3/10], Step [227/625], Loss: 1.4112\n",
      "Epoch [3/10], Step [228/625], Loss: 1.1536\n",
      "Epoch [3/10], Step [229/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [230/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [231/625], Loss: 1.6542\n",
      "Epoch [3/10], Step [232/625], Loss: 1.3908\n",
      "Epoch [3/10], Step [233/625], Loss: 0.9507\n",
      "Epoch [3/10], Step [234/625], Loss: 1.9045\n",
      "Epoch [3/10], Step [235/625], Loss: 1.6409\n",
      "Epoch [3/10], Step [236/625], Loss: 1.1543\n",
      "Epoch [3/10], Step [237/625], Loss: 1.6480\n",
      "Epoch [3/10], Step [238/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [239/625], Loss: 1.1699\n",
      "Epoch [3/10], Step [240/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [241/625], Loss: 1.6528\n",
      "Epoch [3/10], Step [242/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [243/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [244/625], Loss: 1.2986\n",
      "Epoch [3/10], Step [245/625], Loss: 1.8809\n",
      "Epoch [3/10], Step [246/625], Loss: 1.3754\n",
      "Epoch [3/10], Step [247/625], Loss: 1.2058\n",
      "Epoch [3/10], Step [248/625], Loss: 1.1680\n",
      "Epoch [3/10], Step [249/625], Loss: 1.3928\n",
      "Epoch [3/10], Step [250/625], Loss: 1.7136\n",
      "Epoch [3/10], Step [251/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [252/625], Loss: 1.8834\n",
      "Epoch [3/10], Step [253/625], Loss: 1.3948\n",
      "Epoch [3/10], Step [254/625], Loss: 1.1551\n",
      "Epoch [3/10], Step [255/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [256/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [257/625], Loss: 1.4002\n",
      "Epoch [3/10], Step [258/625], Loss: 1.6529\n",
      "Epoch [3/10], Step [259/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [260/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [261/625], Loss: 1.6495\n",
      "Epoch [3/10], Step [262/625], Loss: 1.1554\n",
      "Epoch [3/10], Step [263/625], Loss: 1.3168\n",
      "Epoch [3/10], Step [264/625], Loss: 1.3599\n",
      "Epoch [3/10], Step [265/625], Loss: 1.3981\n",
      "Epoch [3/10], Step [266/625], Loss: 1.6362\n",
      "Epoch [3/10], Step [267/625], Loss: 1.6363\n",
      "Epoch [3/10], Step [268/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [269/625], Loss: 0.9197\n",
      "Epoch [3/10], Step [270/625], Loss: 1.4045\n",
      "Epoch [3/10], Step [271/625], Loss: 1.5787\n",
      "Epoch [3/10], Step [272/625], Loss: 1.1562\n",
      "Epoch [3/10], Step [273/625], Loss: 1.6452\n",
      "Epoch [3/10], Step [274/625], Loss: 1.3918\n",
      "Epoch [3/10], Step [275/625], Loss: 1.6082\n",
      "Epoch [3/10], Step [276/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [277/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [278/625], Loss: 1.4491\n",
      "Epoch [3/10], Step [279/625], Loss: 1.4066\n",
      "Epoch [3/10], Step [280/625], Loss: 1.6391\n",
      "Epoch [3/10], Step [281/625], Loss: 1.8453\n",
      "Epoch [3/10], Step [282/625], Loss: 1.6392\n",
      "Epoch [3/10], Step [283/625], Loss: 1.1928\n",
      "Epoch [3/10], Step [284/625], Loss: 1.1559\n",
      "Epoch [3/10], Step [285/625], Loss: 1.6504\n",
      "Epoch [3/10], Step [286/625], Loss: 1.1055\n",
      "Epoch [3/10], Step [287/625], Loss: 1.6533\n",
      "Epoch [3/10], Step [288/625], Loss: 1.1502\n",
      "Epoch [3/10], Step [289/625], Loss: 0.9473\n",
      "Epoch [3/10], Step [290/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [291/625], Loss: 1.4038\n",
      "Epoch [3/10], Step [292/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [293/625], Loss: 1.5294\n",
      "Epoch [3/10], Step [294/625], Loss: 1.6546\n",
      "Epoch [3/10], Step [295/625], Loss: 0.9323\n",
      "Epoch [3/10], Step [296/625], Loss: 1.4105\n",
      "Epoch [3/10], Step [297/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [298/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [299/625], Loss: 0.9063\n",
      "Epoch [3/10], Step [300/625], Loss: 1.6520\n",
      "Epoch [3/10], Step [301/625], Loss: 1.9045\n",
      "Epoch [3/10], Step [302/625], Loss: 1.6485\n",
      "Epoch [3/10], Step [303/625], Loss: 1.4066\n",
      "Epoch [3/10], Step [304/625], Loss: 1.5769\n",
      "Epoch [3/10], Step [305/625], Loss: 1.3622\n",
      "Epoch [3/10], Step [306/625], Loss: 0.9827\n",
      "Epoch [3/10], Step [307/625], Loss: 1.6703\n",
      "Epoch [3/10], Step [308/625], Loss: 1.4628\n",
      "Epoch [3/10], Step [309/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [310/625], Loss: 1.6308\n",
      "Epoch [3/10], Step [311/625], Loss: 1.1550\n",
      "Epoch [3/10], Step [312/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [313/625], Loss: 1.1551\n",
      "Epoch [3/10], Step [314/625], Loss: 1.6541\n",
      "Epoch [3/10], Step [315/625], Loss: 1.6544\n",
      "Epoch [3/10], Step [316/625], Loss: 1.2278\n",
      "Epoch [3/10], Step [317/625], Loss: 1.3966\n",
      "Epoch [3/10], Step [318/625], Loss: 1.3860\n",
      "Epoch [3/10], Step [319/625], Loss: 1.3771\n",
      "Epoch [3/10], Step [320/625], Loss: 1.6532\n",
      "Epoch [3/10], Step [321/625], Loss: 1.2677\n",
      "Epoch [3/10], Step [322/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [323/625], Loss: 1.6281\n",
      "Epoch [3/10], Step [324/625], Loss: 1.1247\n",
      "Epoch [3/10], Step [325/625], Loss: 0.9055\n",
      "Epoch [3/10], Step [326/625], Loss: 1.6424\n",
      "Epoch [3/10], Step [327/625], Loss: 1.6493\n",
      "Epoch [3/10], Step [328/625], Loss: 1.6554\n",
      "Epoch [3/10], Step [329/625], Loss: 1.1585\n",
      "Epoch [3/10], Step [330/625], Loss: 0.9421\n",
      "Epoch [3/10], Step [331/625], Loss: 1.1941\n",
      "Epoch [3/10], Step [332/625], Loss: 1.1566\n",
      "Epoch [3/10], Step [333/625], Loss: 1.6376\n",
      "Epoch [3/10], Step [334/625], Loss: 1.4030\n",
      "Epoch [3/10], Step [335/625], Loss: 1.3902\n",
      "Epoch [3/10], Step [336/625], Loss: 1.6103\n",
      "Epoch [3/10], Step [337/625], Loss: 1.6223\n",
      "Epoch [3/10], Step [338/625], Loss: 0.9112\n",
      "Epoch [3/10], Step [339/625], Loss: 1.4729\n",
      "Epoch [3/10], Step [340/625], Loss: 1.4022\n",
      "Epoch [3/10], Step [341/625], Loss: 1.2138\n",
      "Epoch [3/10], Step [342/625], Loss: 1.4020\n",
      "Epoch [3/10], Step [343/625], Loss: 1.2572\n",
      "Epoch [3/10], Step [344/625], Loss: 0.9233\n",
      "Epoch [3/10], Step [345/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [346/625], Loss: 1.3977\n",
      "Epoch [3/10], Step [347/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [348/625], Loss: 1.4014\n",
      "Epoch [3/10], Step [349/625], Loss: 1.4823\n",
      "Epoch [3/10], Step [350/625], Loss: 1.4024\n",
      "Epoch [3/10], Step [351/625], Loss: 1.8760\n",
      "Epoch [3/10], Step [352/625], Loss: 1.1545\n",
      "Epoch [3/10], Step [353/625], Loss: 1.6209\n",
      "Epoch [3/10], Step [354/625], Loss: 1.3890\n",
      "Epoch [3/10], Step [355/625], Loss: 1.3980\n",
      "Epoch [3/10], Step [356/625], Loss: 1.8133\n",
      "Epoch [3/10], Step [357/625], Loss: 1.1473\n",
      "Epoch [3/10], Step [358/625], Loss: 1.4126\n",
      "Epoch [3/10], Step [359/625], Loss: 1.7647\n",
      "Epoch [3/10], Step [360/625], Loss: 1.7584\n",
      "Epoch [3/10], Step [361/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [362/625], Loss: 1.3184\n",
      "Epoch [3/10], Step [363/625], Loss: 1.3896\n",
      "Epoch [3/10], Step [364/625], Loss: 1.8753\n",
      "Epoch [3/10], Step [365/625], Loss: 1.8962\n",
      "Epoch [3/10], Step [366/625], Loss: 1.2608\n",
      "Epoch [3/10], Step [367/625], Loss: 1.2484\n",
      "Epoch [3/10], Step [368/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [369/625], Loss: 0.9605\n",
      "Epoch [3/10], Step [370/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [371/625], Loss: 1.4084\n",
      "Epoch [3/10], Step [372/625], Loss: 1.0490\n",
      "Epoch [3/10], Step [373/625], Loss: 1.6544\n",
      "Epoch [3/10], Step [374/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [375/625], Loss: 1.4034\n",
      "Epoch [3/10], Step [376/625], Loss: 1.1482\n",
      "Epoch [3/10], Step [377/625], Loss: 1.4257\n",
      "Epoch [3/10], Step [378/625], Loss: 1.0053\n",
      "Epoch [3/10], Step [379/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [380/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [381/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [382/625], Loss: 1.1928\n",
      "Epoch [3/10], Step [383/625], Loss: 1.4043\n",
      "Epoch [3/10], Step [384/625], Loss: 1.3647\n",
      "Epoch [3/10], Step [385/625], Loss: 1.7427\n",
      "Epoch [3/10], Step [386/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [387/625], Loss: 0.9945\n",
      "Epoch [3/10], Step [388/625], Loss: 1.4043\n",
      "Epoch [3/10], Step [389/625], Loss: 1.3247\n",
      "Epoch [3/10], Step [390/625], Loss: 0.9735\n",
      "Epoch [3/10], Step [391/625], Loss: 1.4040\n",
      "Epoch [3/10], Step [392/625], Loss: 1.1546\n",
      "Epoch [3/10], Step [393/625], Loss: 1.3899\n",
      "Epoch [3/10], Step [394/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [395/625], Loss: 1.4031\n",
      "Epoch [3/10], Step [396/625], Loss: 1.4029\n",
      "Epoch [3/10], Step [397/625], Loss: 1.1478\n",
      "Epoch [3/10], Step [398/625], Loss: 1.7156\n",
      "Epoch [3/10], Step [399/625], Loss: 1.4035\n",
      "Epoch [3/10], Step [400/625], Loss: 1.4452\n",
      "Epoch [3/10], Step [401/625], Loss: 1.1547\n",
      "Epoch [3/10], Step [402/625], Loss: 1.3156\n",
      "Epoch [3/10], Step [403/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [404/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [405/625], Loss: 1.1695\n",
      "Epoch [3/10], Step [406/625], Loss: 0.9452\n",
      "Epoch [3/10], Step [407/625], Loss: 1.8194\n",
      "Epoch [3/10], Step [408/625], Loss: 1.1545\n",
      "Epoch [3/10], Step [409/625], Loss: 1.3564\n",
      "Epoch [3/10], Step [410/625], Loss: 1.6421\n",
      "Epoch [3/10], Step [411/625], Loss: 1.4078\n",
      "Epoch [3/10], Step [412/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [413/625], Loss: 1.4021\n",
      "Epoch [3/10], Step [414/625], Loss: 1.1771\n",
      "Epoch [3/10], Step [415/625], Loss: 1.6535\n",
      "Epoch [3/10], Step [416/625], Loss: 1.1543\n",
      "Epoch [3/10], Step [417/625], Loss: 1.2358\n",
      "Epoch [3/10], Step [418/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [419/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [420/625], Loss: 1.7583\n",
      "Epoch [3/10], Step [421/625], Loss: 1.1388\n",
      "Epoch [3/10], Step [422/625], Loss: 1.4196\n",
      "Epoch [3/10], Step [423/625], Loss: 1.4284\n",
      "Epoch [3/10], Step [424/625], Loss: 1.4033\n",
      "Epoch [3/10], Step [425/625], Loss: 1.1588\n",
      "Epoch [3/10], Step [426/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [427/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [428/625], Loss: 1.6409\n",
      "Epoch [3/10], Step [429/625], Loss: 1.4008\n",
      "Epoch [3/10], Step [430/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [431/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [432/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [433/625], Loss: 1.1549\n",
      "Epoch [3/10], Step [434/625], Loss: 1.3282\n",
      "Epoch [3/10], Step [435/625], Loss: 1.5077\n",
      "Epoch [3/10], Step [436/625], Loss: 1.1660\n",
      "Epoch [3/10], Step [437/625], Loss: 1.9013\n",
      "Epoch [3/10], Step [438/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [439/625], Loss: 1.1536\n",
      "Epoch [3/10], Step [440/625], Loss: 1.5224\n",
      "Epoch [3/10], Step [441/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [442/625], Loss: 1.1940\n",
      "Epoch [3/10], Step [443/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [444/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [445/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [446/625], Loss: 1.3531\n",
      "Epoch [3/10], Step [447/625], Loss: 0.9080\n",
      "Epoch [3/10], Step [448/625], Loss: 1.4111\n",
      "Epoch [3/10], Step [449/625], Loss: 1.4050\n",
      "Epoch [3/10], Step [450/625], Loss: 1.5415\n",
      "Epoch [3/10], Step [451/625], Loss: 1.3887\n",
      "Epoch [3/10], Step [452/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [453/625], Loss: 1.4271\n",
      "Epoch [3/10], Step [454/625], Loss: 1.1299\n",
      "Epoch [3/10], Step [455/625], Loss: 1.3900\n",
      "Epoch [3/10], Step [456/625], Loss: 1.3350\n",
      "Epoch [3/10], Step [457/625], Loss: 1.4858\n",
      "Epoch [3/10], Step [458/625], Loss: 1.8956\n",
      "Epoch [3/10], Step [459/625], Loss: 1.1547\n",
      "Epoch [3/10], Step [460/625], Loss: 1.4035\n",
      "Epoch [3/10], Step [461/625], Loss: 1.6309\n",
      "Epoch [3/10], Step [462/625], Loss: 1.5000\n",
      "Epoch [3/10], Step [463/625], Loss: 1.4072\n",
      "Epoch [3/10], Step [464/625], Loss: 1.6388\n",
      "Epoch [3/10], Step [465/625], Loss: 0.9748\n",
      "Epoch [3/10], Step [466/625], Loss: 1.4066\n",
      "Epoch [3/10], Step [467/625], Loss: 1.6412\n",
      "Epoch [3/10], Step [468/625], Loss: 1.4058\n",
      "Epoch [3/10], Step [469/625], Loss: 1.1527\n",
      "Epoch [3/10], Step [470/625], Loss: 1.2748\n",
      "Epoch [3/10], Step [471/625], Loss: 1.5295\n",
      "Epoch [3/10], Step [472/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [473/625], Loss: 1.4044\n",
      "Epoch [3/10], Step [474/625], Loss: 1.4189\n",
      "Epoch [3/10], Step [475/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [476/625], Loss: 1.1296\n",
      "Epoch [3/10], Step [477/625], Loss: 1.6000\n",
      "Epoch [3/10], Step [478/625], Loss: 1.1324\n",
      "Epoch [3/10], Step [479/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [480/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [481/625], Loss: 1.5941\n",
      "Epoch [3/10], Step [482/625], Loss: 1.3960\n",
      "Epoch [3/10], Step [483/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [484/625], Loss: 1.6503\n",
      "Epoch [3/10], Step [485/625], Loss: 1.6330\n",
      "Epoch [3/10], Step [486/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [487/625], Loss: 1.1543\n",
      "Epoch [3/10], Step [488/625], Loss: 1.1416\n",
      "Epoch [3/10], Step [489/625], Loss: 1.1699\n",
      "Epoch [3/10], Step [490/625], Loss: 1.3712\n",
      "Epoch [3/10], Step [491/625], Loss: 1.9036\n",
      "Epoch [3/10], Step [492/625], Loss: 1.6546\n",
      "Epoch [3/10], Step [493/625], Loss: 1.4036\n",
      "Epoch [3/10], Step [494/625], Loss: 0.9049\n",
      "Epoch [3/10], Step [495/625], Loss: 1.6541\n",
      "Epoch [3/10], Step [496/625], Loss: 1.1498\n",
      "Epoch [3/10], Step [497/625], Loss: 1.7207\n",
      "Epoch [3/10], Step [498/625], Loss: 1.9028\n",
      "Epoch [3/10], Step [499/625], Loss: 1.1501\n",
      "Epoch [3/10], Step [500/625], Loss: 1.4042\n",
      "Epoch [3/10], Step [501/625], Loss: 1.6005\n",
      "Epoch [3/10], Step [502/625], Loss: 1.1614\n",
      "Epoch [3/10], Step [503/625], Loss: 1.3895\n",
      "Epoch [3/10], Step [504/625], Loss: 1.4481\n",
      "Epoch [3/10], Step [505/625], Loss: 1.2514\n",
      "Epoch [3/10], Step [506/625], Loss: 1.6018\n",
      "Epoch [3/10], Step [507/625], Loss: 1.4072\n",
      "Epoch [3/10], Step [508/625], Loss: 0.9048\n",
      "Epoch [3/10], Step [509/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [510/625], Loss: 1.7567\n",
      "Epoch [3/10], Step [511/625], Loss: 1.1570\n",
      "Epoch [3/10], Step [512/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [513/625], Loss: 0.9077\n",
      "Epoch [3/10], Step [514/625], Loss: 1.1543\n",
      "Epoch [3/10], Step [515/625], Loss: 1.4058\n",
      "Epoch [3/10], Step [516/625], Loss: 1.1985\n",
      "Epoch [3/10], Step [517/625], Loss: 1.3997\n",
      "Epoch [3/10], Step [518/625], Loss: 1.4364\n",
      "Epoch [3/10], Step [519/625], Loss: 0.9399\n",
      "Epoch [3/10], Step [520/625], Loss: 1.1686\n",
      "Epoch [3/10], Step [521/625], Loss: 1.1543\n",
      "Epoch [3/10], Step [522/625], Loss: 1.8973\n",
      "Epoch [3/10], Step [523/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [524/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [525/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [526/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [527/625], Loss: 1.0916\n",
      "Epoch [3/10], Step [528/625], Loss: 1.6464\n",
      "Epoch [3/10], Step [529/625], Loss: 1.1951\n",
      "Epoch [3/10], Step [530/625], Loss: 1.1549\n",
      "Epoch [3/10], Step [531/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [532/625], Loss: 1.6189\n",
      "Epoch [3/10], Step [533/625], Loss: 1.6529\n",
      "Epoch [3/10], Step [534/625], Loss: 1.3779\n",
      "Epoch [3/10], Step [535/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [536/625], Loss: 0.9060\n",
      "Epoch [3/10], Step [537/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [538/625], Loss: 1.1518\n",
      "Epoch [3/10], Step [539/625], Loss: 1.0195\n",
      "Epoch [3/10], Step [540/625], Loss: 0.9075\n",
      "Epoch [3/10], Step [541/625], Loss: 1.2701\n",
      "Epoch [3/10], Step [542/625], Loss: 1.9031\n",
      "Epoch [3/10], Step [543/625], Loss: 1.4032\n",
      "Epoch [3/10], Step [544/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [545/625], Loss: 1.1552\n",
      "Epoch [3/10], Step [546/625], Loss: 1.3252\n",
      "Epoch [3/10], Step [547/625], Loss: 1.3903\n",
      "Epoch [3/10], Step [548/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [549/625], Loss: 1.4002\n",
      "Epoch [3/10], Step [550/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [551/625], Loss: 1.3570\n",
      "Epoch [3/10], Step [552/625], Loss: 1.1589\n",
      "Epoch [3/10], Step [553/625], Loss: 1.3923\n",
      "Epoch [3/10], Step [554/625], Loss: 1.1493\n",
      "Epoch [3/10], Step [555/625], Loss: 1.3747\n",
      "Epoch [3/10], Step [556/625], Loss: 1.6539\n",
      "Epoch [3/10], Step [557/625], Loss: 1.3198\n",
      "Epoch [3/10], Step [558/625], Loss: 1.3858\n",
      "Epoch [3/10], Step [559/625], Loss: 1.1549\n",
      "Epoch [3/10], Step [560/625], Loss: 1.3543\n",
      "Epoch [3/10], Step [561/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [562/625], Loss: 1.1401\n",
      "Epoch [3/10], Step [563/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [564/625], Loss: 1.3886\n",
      "Epoch [3/10], Step [565/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [566/625], Loss: 1.0569\n",
      "Epoch [3/10], Step [567/625], Loss: 1.4511\n",
      "Epoch [3/10], Step [568/625], Loss: 1.2907\n",
      "Epoch [3/10], Step [569/625], Loss: 1.3882\n",
      "Epoch [3/10], Step [570/625], Loss: 1.3507\n",
      "Epoch [3/10], Step [571/625], Loss: 1.1657\n",
      "Epoch [3/10], Step [572/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [573/625], Loss: 1.5208\n",
      "Epoch [3/10], Step [574/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [575/625], Loss: 1.4009\n",
      "Epoch [3/10], Step [576/625], Loss: 1.3458\n",
      "Epoch [3/10], Step [577/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [578/625], Loss: 1.4128\n",
      "Epoch [3/10], Step [579/625], Loss: 1.1681\n",
      "Epoch [3/10], Step [580/625], Loss: 1.4041\n",
      "Epoch [3/10], Step [581/625], Loss: 1.1553\n",
      "Epoch [3/10], Step [582/625], Loss: 0.9114\n",
      "Epoch [3/10], Step [583/625], Loss: 1.3889\n",
      "Epoch [3/10], Step [584/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [585/625], Loss: 1.9033\n",
      "Epoch [3/10], Step [586/625], Loss: 1.4282\n",
      "Epoch [3/10], Step [587/625], Loss: 1.1710\n",
      "Epoch [3/10], Step [588/625], Loss: 1.4032\n",
      "Epoch [3/10], Step [589/625], Loss: 1.2555\n",
      "Epoch [3/10], Step [590/625], Loss: 1.4492\n",
      "Epoch [3/10], Step [591/625], Loss: 1.4215\n",
      "Epoch [3/10], Step [592/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [593/625], Loss: 1.3937\n",
      "Epoch [3/10], Step [594/625], Loss: 1.1580\n",
      "Epoch [3/10], Step [595/625], Loss: 1.4569\n",
      "Epoch [3/10], Step [596/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [597/625], Loss: 1.5933\n",
      "Epoch [3/10], Step [598/625], Loss: 1.8185\n",
      "Epoch [3/10], Step [599/625], Loss: 1.1551\n",
      "Epoch [3/10], Step [600/625], Loss: 1.4044\n",
      "Epoch [3/10], Step [601/625], Loss: 1.7523\n",
      "Epoch [3/10], Step [602/625], Loss: 1.4100\n",
      "Epoch [3/10], Step [603/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [604/625], Loss: 0.9175\n",
      "Epoch [3/10], Step [605/625], Loss: 1.4047\n",
      "Epoch [3/10], Step [606/625], Loss: 1.8655\n",
      "Epoch [3/10], Step [607/625], Loss: 1.4492\n",
      "Epoch [3/10], Step [608/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [609/625], Loss: 1.6547\n",
      "Epoch [3/10], Step [610/625], Loss: 1.9048\n",
      "Epoch [3/10], Step [611/625], Loss: 1.4049\n",
      "Epoch [3/10], Step [612/625], Loss: 1.8977\n",
      "Epoch [3/10], Step [613/625], Loss: 1.4062\n",
      "Epoch [3/10], Step [614/625], Loss: 1.6545\n",
      "Epoch [3/10], Step [615/625], Loss: 1.6555\n",
      "Epoch [3/10], Step [616/625], Loss: 1.1547\n",
      "Epoch [3/10], Step [617/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [618/625], Loss: 1.3269\n",
      "Epoch [3/10], Step [619/625], Loss: 1.1433\n",
      "Epoch [3/10], Step [620/625], Loss: 1.1548\n",
      "Epoch [3/10], Step [621/625], Loss: 1.4046\n",
      "Epoch [3/10], Step [622/625], Loss: 1.4048\n",
      "Epoch [3/10], Step [623/625], Loss: 1.6548\n",
      "Epoch [3/10], Step [624/625], Loss: 1.4006\n",
      "Epoch [3/10], Step [625/625], Loss: 1.2697\n",
      "Epoch [4/10], Step [1/625], Loss: 1.6507\n",
      "Epoch [4/10], Step [2/625], Loss: 1.8952\n",
      "Epoch [4/10], Step [3/625], Loss: 1.3840\n",
      "Epoch [4/10], Step [4/625], Loss: 1.6542\n",
      "Epoch [4/10], Step [5/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [6/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [7/625], Loss: 1.2474\n",
      "Epoch [4/10], Step [8/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [9/625], Loss: 0.9603\n",
      "Epoch [4/10], Step [10/625], Loss: 1.4063\n",
      "Epoch [4/10], Step [11/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [12/625], Loss: 1.4161\n",
      "Epoch [4/10], Step [13/625], Loss: 0.9070\n",
      "Epoch [4/10], Step [14/625], Loss: 1.1795\n",
      "Epoch [4/10], Step [15/625], Loss: 1.3934\n",
      "Epoch [4/10], Step [16/625], Loss: 1.4053\n",
      "Epoch [4/10], Step [17/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [18/625], Loss: 1.6532\n",
      "Epoch [4/10], Step [19/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [20/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [21/625], Loss: 1.3914\n",
      "Epoch [4/10], Step [22/625], Loss: 1.6477\n",
      "Epoch [4/10], Step [23/625], Loss: 1.6519\n",
      "Epoch [4/10], Step [24/625], Loss: 1.6029\n",
      "Epoch [4/10], Step [25/625], Loss: 1.9037\n",
      "Epoch [4/10], Step [26/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [27/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [28/625], Loss: 1.8337\n",
      "Epoch [4/10], Step [29/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [30/625], Loss: 1.3260\n",
      "Epoch [4/10], Step [31/625], Loss: 1.4041\n",
      "Epoch [4/10], Step [32/625], Loss: 1.2854\n",
      "Epoch [4/10], Step [33/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [34/625], Loss: 1.8635\n",
      "Epoch [4/10], Step [35/625], Loss: 1.3966\n",
      "Epoch [4/10], Step [36/625], Loss: 1.3996\n",
      "Epoch [4/10], Step [37/625], Loss: 1.0873\n",
      "Epoch [4/10], Step [38/625], Loss: 1.6164\n",
      "Epoch [4/10], Step [39/625], Loss: 1.4046\n",
      "Epoch [4/10], Step [40/625], Loss: 1.6322\n",
      "Epoch [4/10], Step [41/625], Loss: 1.1438\n",
      "Epoch [4/10], Step [42/625], Loss: 1.4057\n",
      "Epoch [4/10], Step [43/625], Loss: 1.1411\n",
      "Epoch [4/10], Step [44/625], Loss: 1.6515\n",
      "Epoch [4/10], Step [45/625], Loss: 1.4175\n",
      "Epoch [4/10], Step [46/625], Loss: 1.6545\n",
      "Epoch [4/10], Step [47/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [48/625], Loss: 1.3948\n",
      "Epoch [4/10], Step [49/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [50/625], Loss: 1.6531\n",
      "Epoch [4/10], Step [51/625], Loss: 1.7569\n",
      "Epoch [4/10], Step [52/625], Loss: 1.1547\n",
      "Epoch [4/10], Step [53/625], Loss: 1.1681\n",
      "Epoch [4/10], Step [54/625], Loss: 1.4051\n",
      "Epoch [4/10], Step [55/625], Loss: 1.6491\n",
      "Epoch [4/10], Step [56/625], Loss: 1.8256\n",
      "Epoch [4/10], Step [57/625], Loss: 1.6654\n",
      "Epoch [4/10], Step [58/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [59/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [60/625], Loss: 1.6533\n",
      "Epoch [4/10], Step [61/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [62/625], Loss: 1.3927\n",
      "Epoch [4/10], Step [63/625], Loss: 1.6466\n",
      "Epoch [4/10], Step [64/625], Loss: 1.4590\n",
      "Epoch [4/10], Step [65/625], Loss: 1.3079\n",
      "Epoch [4/10], Step [66/625], Loss: 1.3969\n",
      "Epoch [4/10], Step [67/625], Loss: 1.3917\n",
      "Epoch [4/10], Step [68/625], Loss: 1.2486\n",
      "Epoch [4/10], Step [69/625], Loss: 1.4049\n",
      "Epoch [4/10], Step [70/625], Loss: 1.8168\n",
      "Epoch [4/10], Step [71/625], Loss: 1.4128\n",
      "Epoch [4/10], Step [72/625], Loss: 1.1849\n",
      "Epoch [4/10], Step [73/625], Loss: 1.6378\n",
      "Epoch [4/10], Step [74/625], Loss: 0.9049\n",
      "Epoch [4/10], Step [75/625], Loss: 1.4771\n",
      "Epoch [4/10], Step [76/625], Loss: 1.6553\n",
      "Epoch [4/10], Step [77/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [78/625], Loss: 1.6483\n",
      "Epoch [4/10], Step [79/625], Loss: 1.4545\n",
      "Epoch [4/10], Step [80/625], Loss: 1.3754\n",
      "Epoch [4/10], Step [81/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [82/625], Loss: 1.4042\n",
      "Epoch [4/10], Step [83/625], Loss: 1.6195\n",
      "Epoch [4/10], Step [84/625], Loss: 1.3766\n",
      "Epoch [4/10], Step [85/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [86/625], Loss: 1.6305\n",
      "Epoch [4/10], Step [87/625], Loss: 1.6492\n",
      "Epoch [4/10], Step [88/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [89/625], Loss: 1.6521\n",
      "Epoch [4/10], Step [90/625], Loss: 1.6546\n",
      "Epoch [4/10], Step [91/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [92/625], Loss: 1.4003\n",
      "Epoch [4/10], Step [93/625], Loss: 1.6110\n",
      "Epoch [4/10], Step [94/625], Loss: 1.6392\n",
      "Epoch [4/10], Step [95/625], Loss: 1.6101\n",
      "Epoch [4/10], Step [96/625], Loss: 1.4037\n",
      "Epoch [4/10], Step [97/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [98/625], Loss: 1.1552\n",
      "Epoch [4/10], Step [99/625], Loss: 1.6505\n",
      "Epoch [4/10], Step [100/625], Loss: 1.9048\n",
      "Epoch [4/10], Step [101/625], Loss: 1.6038\n",
      "Epoch [4/10], Step [102/625], Loss: 0.9604\n",
      "Epoch [4/10], Step [103/625], Loss: 1.1410\n",
      "Epoch [4/10], Step [104/625], Loss: 1.9038\n",
      "Epoch [4/10], Step [105/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [106/625], Loss: 1.6446\n",
      "Epoch [4/10], Step [107/625], Loss: 1.1724\n",
      "Epoch [4/10], Step [108/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [109/625], Loss: 1.1504\n",
      "Epoch [4/10], Step [110/625], Loss: 1.4025\n",
      "Epoch [4/10], Step [111/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [112/625], Loss: 1.4779\n",
      "Epoch [4/10], Step [113/625], Loss: 1.5790\n",
      "Epoch [4/10], Step [114/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [115/625], Loss: 1.1579\n",
      "Epoch [4/10], Step [116/625], Loss: 1.2691\n",
      "Epoch [4/10], Step [117/625], Loss: 1.5370\n",
      "Epoch [4/10], Step [118/625], Loss: 1.1942\n",
      "Epoch [4/10], Step [119/625], Loss: 1.4370\n",
      "Epoch [4/10], Step [120/625], Loss: 1.4049\n",
      "Epoch [4/10], Step [121/625], Loss: 1.6451\n",
      "Epoch [4/10], Step [122/625], Loss: 1.6530\n",
      "Epoch [4/10], Step [123/625], Loss: 1.4041\n",
      "Epoch [4/10], Step [124/625], Loss: 1.4029\n",
      "Epoch [4/10], Step [125/625], Loss: 1.7460\n",
      "Epoch [4/10], Step [126/625], Loss: 1.6402\n",
      "Epoch [4/10], Step [127/625], Loss: 1.4760\n",
      "Epoch [4/10], Step [128/625], Loss: 1.0149\n",
      "Epoch [4/10], Step [129/625], Loss: 1.1556\n",
      "Epoch [4/10], Step [130/625], Loss: 1.6454\n",
      "Epoch [4/10], Step [131/625], Loss: 1.2715\n",
      "Epoch [4/10], Step [132/625], Loss: 1.6531\n",
      "Epoch [4/10], Step [133/625], Loss: 1.3966\n",
      "Epoch [4/10], Step [134/625], Loss: 1.4112\n",
      "Epoch [4/10], Step [135/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [136/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [137/625], Loss: 1.3342\n",
      "Epoch [4/10], Step [138/625], Loss: 1.6483\n",
      "Epoch [4/10], Step [139/625], Loss: 1.3827\n",
      "Epoch [4/10], Step [140/625], Loss: 1.4044\n",
      "Epoch [4/10], Step [141/625], Loss: 1.1557\n",
      "Epoch [4/10], Step [142/625], Loss: 0.9076\n",
      "Epoch [4/10], Step [143/625], Loss: 1.1553\n",
      "Epoch [4/10], Step [144/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [145/625], Loss: 1.1571\n",
      "Epoch [4/10], Step [146/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [147/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [148/625], Loss: 1.3877\n",
      "Epoch [4/10], Step [149/625], Loss: 1.4034\n",
      "Epoch [4/10], Step [150/625], Loss: 1.4029\n",
      "Epoch [4/10], Step [151/625], Loss: 1.5715\n",
      "Epoch [4/10], Step [152/625], Loss: 1.6536\n",
      "Epoch [4/10], Step [153/625], Loss: 1.4087\n",
      "Epoch [4/10], Step [154/625], Loss: 1.1965\n",
      "Epoch [4/10], Step [155/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [156/625], Loss: 1.8840\n",
      "Epoch [4/10], Step [157/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [158/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [159/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [160/625], Loss: 1.4075\n",
      "Epoch [4/10], Step [161/625], Loss: 1.1553\n",
      "Epoch [4/10], Step [162/625], Loss: 1.4175\n",
      "Epoch [4/10], Step [163/625], Loss: 1.4035\n",
      "Epoch [4/10], Step [164/625], Loss: 1.4519\n",
      "Epoch [4/10], Step [165/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [166/625], Loss: 1.2634\n",
      "Epoch [4/10], Step [167/625], Loss: 1.3438\n",
      "Epoch [4/10], Step [168/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [169/625], Loss: 1.1515\n",
      "Epoch [4/10], Step [170/625], Loss: 1.1432\n",
      "Epoch [4/10], Step [171/625], Loss: 1.3939\n",
      "Epoch [4/10], Step [172/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [173/625], Loss: 1.1558\n",
      "Epoch [4/10], Step [174/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [175/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [176/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [177/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [178/625], Loss: 1.6242\n",
      "Epoch [4/10], Step [179/625], Loss: 1.6677\n",
      "Epoch [4/10], Step [180/625], Loss: 1.5623\n",
      "Epoch [4/10], Step [181/625], Loss: 1.4021\n",
      "Epoch [4/10], Step [182/625], Loss: 1.8530\n",
      "Epoch [4/10], Step [183/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [184/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [185/625], Loss: 1.4036\n",
      "Epoch [4/10], Step [186/625], Loss: 0.9067\n",
      "Epoch [4/10], Step [187/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [188/625], Loss: 1.1544\n",
      "Epoch [4/10], Step [189/625], Loss: 1.1672\n",
      "Epoch [4/10], Step [190/625], Loss: 1.4019\n",
      "Epoch [4/10], Step [191/625], Loss: 1.1621\n",
      "Epoch [4/10], Step [192/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [193/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [194/625], Loss: 0.9053\n",
      "Epoch [4/10], Step [195/625], Loss: 1.3492\n",
      "Epoch [4/10], Step [196/625], Loss: 1.4046\n",
      "Epoch [4/10], Step [197/625], Loss: 1.2800\n",
      "Epoch [4/10], Step [198/625], Loss: 0.9049\n",
      "Epoch [4/10], Step [199/625], Loss: 1.1447\n",
      "Epoch [4/10], Step [200/625], Loss: 1.9048\n",
      "Epoch [4/10], Step [201/625], Loss: 1.1539\n",
      "Epoch [4/10], Step [202/625], Loss: 1.6437\n",
      "Epoch [4/10], Step [203/625], Loss: 1.4031\n",
      "Epoch [4/10], Step [204/625], Loss: 0.9172\n",
      "Epoch [4/10], Step [205/625], Loss: 1.1682\n",
      "Epoch [4/10], Step [206/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [207/625], Loss: 1.7090\n",
      "Epoch [4/10], Step [208/625], Loss: 1.6161\n",
      "Epoch [4/10], Step [209/625], Loss: 1.4104\n",
      "Epoch [4/10], Step [210/625], Loss: 1.6437\n",
      "Epoch [4/10], Step [211/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [212/625], Loss: 1.1550\n",
      "Epoch [4/10], Step [213/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [214/625], Loss: 1.1585\n",
      "Epoch [4/10], Step [215/625], Loss: 1.4045\n",
      "Epoch [4/10], Step [216/625], Loss: 1.3937\n",
      "Epoch [4/10], Step [217/625], Loss: 1.1594\n",
      "Epoch [4/10], Step [218/625], Loss: 1.4047\n",
      "Epoch [4/10], Step [219/625], Loss: 1.4094\n",
      "Epoch [4/10], Step [220/625], Loss: 1.1487\n",
      "Epoch [4/10], Step [221/625], Loss: 1.1550\n",
      "Epoch [4/10], Step [222/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [223/625], Loss: 1.1536\n",
      "Epoch [4/10], Step [224/625], Loss: 1.2649\n",
      "Epoch [4/10], Step [225/625], Loss: 1.3945\n",
      "Epoch [4/10], Step [226/625], Loss: 1.4005\n",
      "Epoch [4/10], Step [227/625], Loss: 1.1551\n",
      "Epoch [4/10], Step [228/625], Loss: 1.4056\n",
      "Epoch [4/10], Step [229/625], Loss: 1.6376\n",
      "Epoch [4/10], Step [230/625], Loss: 1.0667\n",
      "Epoch [4/10], Step [231/625], Loss: 1.4064\n",
      "Epoch [4/10], Step [232/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [233/625], Loss: 1.4016\n",
      "Epoch [4/10], Step [234/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [235/625], Loss: 1.1571\n",
      "Epoch [4/10], Step [236/625], Loss: 1.8548\n",
      "Epoch [4/10], Step [237/625], Loss: 1.6362\n",
      "Epoch [4/10], Step [238/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [239/625], Loss: 1.4050\n",
      "Epoch [4/10], Step [240/625], Loss: 1.6462\n",
      "Epoch [4/10], Step [241/625], Loss: 1.6302\n",
      "Epoch [4/10], Step [242/625], Loss: 1.4313\n",
      "Epoch [4/10], Step [243/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [244/625], Loss: 1.2984\n",
      "Epoch [4/10], Step [245/625], Loss: 1.4026\n",
      "Epoch [4/10], Step [246/625], Loss: 1.6082\n",
      "Epoch [4/10], Step [247/625], Loss: 1.1524\n",
      "Epoch [4/10], Step [248/625], Loss: 1.6421\n",
      "Epoch [4/10], Step [249/625], Loss: 1.6314\n",
      "Epoch [4/10], Step [250/625], Loss: 1.6536\n",
      "Epoch [4/10], Step [251/625], Loss: 1.4204\n",
      "Epoch [4/10], Step [252/625], Loss: 1.8977\n",
      "Epoch [4/10], Step [253/625], Loss: 1.1510\n",
      "Epoch [4/10], Step [254/625], Loss: 1.4051\n",
      "Epoch [4/10], Step [255/625], Loss: 1.3276\n",
      "Epoch [4/10], Step [256/625], Loss: 1.6461\n",
      "Epoch [4/10], Step [257/625], Loss: 1.4304\n",
      "Epoch [4/10], Step [258/625], Loss: 0.9359\n",
      "Epoch [4/10], Step [259/625], Loss: 0.9312\n",
      "Epoch [4/10], Step [260/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [261/625], Loss: 1.2833\n",
      "Epoch [4/10], Step [262/625], Loss: 1.6513\n",
      "Epoch [4/10], Step [263/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [264/625], Loss: 1.6462\n",
      "Epoch [4/10], Step [265/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [266/625], Loss: 1.1510\n",
      "Epoch [4/10], Step [267/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [268/625], Loss: 1.8882\n",
      "Epoch [4/10], Step [269/625], Loss: 1.6546\n",
      "Epoch [4/10], Step [270/625], Loss: 1.6387\n",
      "Epoch [4/10], Step [271/625], Loss: 1.2335\n",
      "Epoch [4/10], Step [272/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [273/625], Loss: 1.6016\n",
      "Epoch [4/10], Step [274/625], Loss: 0.9683\n",
      "Epoch [4/10], Step [275/625], Loss: 1.6545\n",
      "Epoch [4/10], Step [276/625], Loss: 1.1559\n",
      "Epoch [4/10], Step [277/625], Loss: 1.6542\n",
      "Epoch [4/10], Step [278/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [279/625], Loss: 1.8989\n",
      "Epoch [4/10], Step [280/625], Loss: 0.9157\n",
      "Epoch [4/10], Step [281/625], Loss: 1.1557\n",
      "Epoch [4/10], Step [282/625], Loss: 1.6133\n",
      "Epoch [4/10], Step [283/625], Loss: 1.1554\n",
      "Epoch [4/10], Step [284/625], Loss: 1.4021\n",
      "Epoch [4/10], Step [285/625], Loss: 1.4054\n",
      "Epoch [4/10], Step [286/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [287/625], Loss: 1.4007\n",
      "Epoch [4/10], Step [288/625], Loss: 0.9056\n",
      "Epoch [4/10], Step [289/625], Loss: 1.3637\n",
      "Epoch [4/10], Step [290/625], Loss: 1.4040\n",
      "Epoch [4/10], Step [291/625], Loss: 1.4047\n",
      "Epoch [4/10], Step [292/625], Loss: 1.3873\n",
      "Epoch [4/10], Step [293/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [294/625], Loss: 1.1575\n",
      "Epoch [4/10], Step [295/625], Loss: 1.9044\n",
      "Epoch [4/10], Step [296/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [297/625], Loss: 1.1395\n",
      "Epoch [4/10], Step [298/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [299/625], Loss: 1.6231\n",
      "Epoch [4/10], Step [300/625], Loss: 1.7608\n",
      "Epoch [4/10], Step [301/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [302/625], Loss: 1.6546\n",
      "Epoch [4/10], Step [303/625], Loss: 1.3937\n",
      "Epoch [4/10], Step [304/625], Loss: 1.4014\n",
      "Epoch [4/10], Step [305/625], Loss: 0.9120\n",
      "Epoch [4/10], Step [306/625], Loss: 1.1821\n",
      "Epoch [4/10], Step [307/625], Loss: 1.1710\n",
      "Epoch [4/10], Step [308/625], Loss: 1.1543\n",
      "Epoch [4/10], Step [309/625], Loss: 1.1650\n",
      "Epoch [4/10], Step [310/625], Loss: 1.3943\n",
      "Epoch [4/10], Step [311/625], Loss: 1.4007\n",
      "Epoch [4/10], Step [312/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [313/625], Loss: 0.9052\n",
      "Epoch [4/10], Step [314/625], Loss: 0.9709\n",
      "Epoch [4/10], Step [315/625], Loss: 1.0811\n",
      "Epoch [4/10], Step [316/625], Loss: 1.0596\n",
      "Epoch [4/10], Step [317/625], Loss: 1.3906\n",
      "Epoch [4/10], Step [318/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [319/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [320/625], Loss: 0.9542\n",
      "Epoch [4/10], Step [321/625], Loss: 1.1526\n",
      "Epoch [4/10], Step [322/625], Loss: 1.3754\n",
      "Epoch [4/10], Step [323/625], Loss: 1.2032\n",
      "Epoch [4/10], Step [324/625], Loss: 1.6401\n",
      "Epoch [4/10], Step [325/625], Loss: 1.1426\n",
      "Epoch [4/10], Step [326/625], Loss: 0.9050\n",
      "Epoch [4/10], Step [327/625], Loss: 1.5741\n",
      "Epoch [4/10], Step [328/625], Loss: 1.6401\n",
      "Epoch [4/10], Step [329/625], Loss: 1.4049\n",
      "Epoch [4/10], Step [330/625], Loss: 1.0794\n",
      "Epoch [4/10], Step [331/625], Loss: 1.3954\n",
      "Epoch [4/10], Step [332/625], Loss: 1.4156\n",
      "Epoch [4/10], Step [333/625], Loss: 1.3908\n",
      "Epoch [4/10], Step [334/625], Loss: 1.3910\n",
      "Epoch [4/10], Step [335/625], Loss: 1.3907\n",
      "Epoch [4/10], Step [336/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [337/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [338/625], Loss: 1.4107\n",
      "Epoch [4/10], Step [339/625], Loss: 1.4063\n",
      "Epoch [4/10], Step [340/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [341/625], Loss: 1.9046\n",
      "Epoch [4/10], Step [342/625], Loss: 1.2925\n",
      "Epoch [4/10], Step [343/625], Loss: 1.3903\n",
      "Epoch [4/10], Step [344/625], Loss: 1.1511\n",
      "Epoch [4/10], Step [345/625], Loss: 1.1658\n",
      "Epoch [4/10], Step [346/625], Loss: 1.3809\n",
      "Epoch [4/10], Step [347/625], Loss: 1.3999\n",
      "Epoch [4/10], Step [348/625], Loss: 1.4694\n",
      "Epoch [4/10], Step [349/625], Loss: 1.4138\n",
      "Epoch [4/10], Step [350/625], Loss: 1.4152\n",
      "Epoch [4/10], Step [351/625], Loss: 1.4010\n",
      "Epoch [4/10], Step [352/625], Loss: 1.3067\n",
      "Epoch [4/10], Step [353/625], Loss: 1.4825\n",
      "Epoch [4/10], Step [354/625], Loss: 1.4018\n",
      "Epoch [4/10], Step [355/625], Loss: 1.4339\n",
      "Epoch [4/10], Step [356/625], Loss: 1.1870\n",
      "Epoch [4/10], Step [357/625], Loss: 1.6257\n",
      "Epoch [4/10], Step [358/625], Loss: 1.7881\n",
      "Epoch [4/10], Step [359/625], Loss: 1.2676\n",
      "Epoch [4/10], Step [360/625], Loss: 0.9256\n",
      "Epoch [4/10], Step [361/625], Loss: 0.9050\n",
      "Epoch [4/10], Step [362/625], Loss: 1.4010\n",
      "Epoch [4/10], Step [363/625], Loss: 1.4049\n",
      "Epoch [4/10], Step [364/625], Loss: 1.1629\n",
      "Epoch [4/10], Step [365/625], Loss: 1.4121\n",
      "Epoch [4/10], Step [366/625], Loss: 1.1543\n",
      "Epoch [4/10], Step [367/625], Loss: 1.1547\n",
      "Epoch [4/10], Step [368/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [369/625], Loss: 0.9203\n",
      "Epoch [4/10], Step [370/625], Loss: 1.1893\n",
      "Epoch [4/10], Step [371/625], Loss: 1.1552\n",
      "Epoch [4/10], Step [372/625], Loss: 1.3968\n",
      "Epoch [4/10], Step [373/625], Loss: 1.6598\n",
      "Epoch [4/10], Step [374/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [375/625], Loss: 1.4197\n",
      "Epoch [4/10], Step [376/625], Loss: 1.3965\n",
      "Epoch [4/10], Step [377/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [378/625], Loss: 1.1512\n",
      "Epoch [4/10], Step [379/625], Loss: 1.2304\n",
      "Epoch [4/10], Step [380/625], Loss: 1.0648\n",
      "Epoch [4/10], Step [381/625], Loss: 1.3897\n",
      "Epoch [4/10], Step [382/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [383/625], Loss: 1.7995\n",
      "Epoch [4/10], Step [384/625], Loss: 1.4041\n",
      "Epoch [4/10], Step [385/625], Loss: 1.4792\n",
      "Epoch [4/10], Step [386/625], Loss: 1.6535\n",
      "Epoch [4/10], Step [387/625], Loss: 1.1883\n",
      "Epoch [4/10], Step [388/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [389/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [390/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [391/625], Loss: 1.5299\n",
      "Epoch [4/10], Step [392/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [393/625], Loss: 1.4047\n",
      "Epoch [4/10], Step [394/625], Loss: 1.1557\n",
      "Epoch [4/10], Step [395/625], Loss: 1.2694\n",
      "Epoch [4/10], Step [396/625], Loss: 1.4043\n",
      "Epoch [4/10], Step [397/625], Loss: 1.1518\n",
      "Epoch [4/10], Step [398/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [399/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [400/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [401/625], Loss: 1.6410\n",
      "Epoch [4/10], Step [402/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [403/625], Loss: 1.1550\n",
      "Epoch [4/10], Step [404/625], Loss: 1.6289\n",
      "Epoch [4/10], Step [405/625], Loss: 1.1633\n",
      "Epoch [4/10], Step [406/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [407/625], Loss: 0.9553\n",
      "Epoch [4/10], Step [408/625], Loss: 1.1376\n",
      "Epoch [4/10], Step [409/625], Loss: 1.3888\n",
      "Epoch [4/10], Step [410/625], Loss: 1.6399\n",
      "Epoch [4/10], Step [411/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [412/625], Loss: 1.4009\n",
      "Epoch [4/10], Step [413/625], Loss: 1.6318\n",
      "Epoch [4/10], Step [414/625], Loss: 1.4188\n",
      "Epoch [4/10], Step [415/625], Loss: 1.1552\n",
      "Epoch [4/10], Step [416/625], Loss: 1.1531\n",
      "Epoch [4/10], Step [417/625], Loss: 1.6502\n",
      "Epoch [4/10], Step [418/625], Loss: 1.3850\n",
      "Epoch [4/10], Step [419/625], Loss: 1.1561\n",
      "Epoch [4/10], Step [420/625], Loss: 1.6546\n",
      "Epoch [4/10], Step [421/625], Loss: 1.4062\n",
      "Epoch [4/10], Step [422/625], Loss: 1.9048\n",
      "Epoch [4/10], Step [423/625], Loss: 0.9051\n",
      "Epoch [4/10], Step [424/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [425/625], Loss: 1.6415\n",
      "Epoch [4/10], Step [426/625], Loss: 1.3963\n",
      "Epoch [4/10], Step [427/625], Loss: 1.1448\n",
      "Epoch [4/10], Step [428/625], Loss: 1.4060\n",
      "Epoch [4/10], Step [429/625], Loss: 1.3861\n",
      "Epoch [4/10], Step [430/625], Loss: 0.9830\n",
      "Epoch [4/10], Step [431/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [432/625], Loss: 1.3014\n",
      "Epoch [4/10], Step [433/625], Loss: 1.4088\n",
      "Epoch [4/10], Step [434/625], Loss: 1.6367\n",
      "Epoch [4/10], Step [435/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [436/625], Loss: 1.4049\n",
      "Epoch [4/10], Step [437/625], Loss: 1.3911\n",
      "Epoch [4/10], Step [438/625], Loss: 1.3865\n",
      "Epoch [4/10], Step [439/625], Loss: 1.4291\n",
      "Epoch [4/10], Step [440/625], Loss: 1.1539\n",
      "Epoch [4/10], Step [441/625], Loss: 1.4052\n",
      "Epoch [4/10], Step [442/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [443/625], Loss: 1.6535\n",
      "Epoch [4/10], Step [444/625], Loss: 1.6742\n",
      "Epoch [4/10], Step [445/625], Loss: 1.4067\n",
      "Epoch [4/10], Step [446/625], Loss: 0.9272\n",
      "Epoch [4/10], Step [447/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [448/625], Loss: 1.1531\n",
      "Epoch [4/10], Step [449/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [450/625], Loss: 0.9714\n",
      "Epoch [4/10], Step [451/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [452/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [453/625], Loss: 1.5343\n",
      "Epoch [4/10], Step [454/625], Loss: 1.6505\n",
      "Epoch [4/10], Step [455/625], Loss: 1.3667\n",
      "Epoch [4/10], Step [456/625], Loss: 1.6211\n",
      "Epoch [4/10], Step [457/625], Loss: 1.4034\n",
      "Epoch [4/10], Step [458/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [459/625], Loss: 1.4035\n",
      "Epoch [4/10], Step [460/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [461/625], Loss: 1.4035\n",
      "Epoch [4/10], Step [462/625], Loss: 1.4051\n",
      "Epoch [4/10], Step [463/625], Loss: 1.2310\n",
      "Epoch [4/10], Step [464/625], Loss: 1.6541\n",
      "Epoch [4/10], Step [465/625], Loss: 1.4075\n",
      "Epoch [4/10], Step [466/625], Loss: 1.6465\n",
      "Epoch [4/10], Step [467/625], Loss: 1.3106\n",
      "Epoch [4/10], Step [468/625], Loss: 1.6547\n",
      "Epoch [4/10], Step [469/625], Loss: 1.1560\n",
      "Epoch [4/10], Step [470/625], Loss: 1.4043\n",
      "Epoch [4/10], Step [471/625], Loss: 1.4041\n",
      "Epoch [4/10], Step [472/625], Loss: 1.5643\n",
      "Epoch [4/10], Step [473/625], Loss: 1.6339\n",
      "Epoch [4/10], Step [474/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [475/625], Loss: 1.0905\n",
      "Epoch [4/10], Step [476/625], Loss: 1.6580\n",
      "Epoch [4/10], Step [477/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [478/625], Loss: 0.9483\n",
      "Epoch [4/10], Step [479/625], Loss: 1.1450\n",
      "Epoch [4/10], Step [480/625], Loss: 1.2590\n",
      "Epoch [4/10], Step [481/625], Loss: 1.1582\n",
      "Epoch [4/10], Step [482/625], Loss: 1.3732\n",
      "Epoch [4/10], Step [483/625], Loss: 1.8992\n",
      "Epoch [4/10], Step [484/625], Loss: 1.6543\n",
      "Epoch [4/10], Step [485/625], Loss: 1.3917\n",
      "Epoch [4/10], Step [486/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [487/625], Loss: 1.1551\n",
      "Epoch [4/10], Step [488/625], Loss: 1.0635\n",
      "Epoch [4/10], Step [489/625], Loss: 1.6173\n",
      "Epoch [4/10], Step [490/625], Loss: 1.9048\n",
      "Epoch [4/10], Step [491/625], Loss: 1.4161\n",
      "Epoch [4/10], Step [492/625], Loss: 1.1547\n",
      "Epoch [4/10], Step [493/625], Loss: 1.3555\n",
      "Epoch [4/10], Step [494/625], Loss: 1.6989\n",
      "Epoch [4/10], Step [495/625], Loss: 0.9049\n",
      "Epoch [4/10], Step [496/625], Loss: 1.1421\n",
      "Epoch [4/10], Step [497/625], Loss: 1.1546\n",
      "Epoch [4/10], Step [498/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [499/625], Loss: 0.9049\n",
      "Epoch [4/10], Step [500/625], Loss: 1.4042\n",
      "Epoch [4/10], Step [501/625], Loss: 1.4000\n",
      "Epoch [4/10], Step [502/625], Loss: 1.1703\n",
      "Epoch [4/10], Step [503/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [504/625], Loss: 1.0269\n",
      "Epoch [4/10], Step [505/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [506/625], Loss: 1.7313\n",
      "Epoch [4/10], Step [507/625], Loss: 1.6433\n",
      "Epoch [4/10], Step [508/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [509/625], Loss: 1.6535\n",
      "Epoch [4/10], Step [510/625], Loss: 1.3898\n",
      "Epoch [4/10], Step [511/625], Loss: 1.6524\n",
      "Epoch [4/10], Step [512/625], Loss: 1.1565\n",
      "Epoch [4/10], Step [513/625], Loss: 1.6511\n",
      "Epoch [4/10], Step [514/625], Loss: 0.9049\n",
      "Epoch [4/10], Step [515/625], Loss: 1.5117\n",
      "Epoch [4/10], Step [516/625], Loss: 1.5936\n",
      "Epoch [4/10], Step [517/625], Loss: 0.9909\n",
      "Epoch [4/10], Step [518/625], Loss: 1.1710\n",
      "Epoch [4/10], Step [519/625], Loss: 1.4037\n",
      "Epoch [4/10], Step [520/625], Loss: 1.1387\n",
      "Epoch [4/10], Step [521/625], Loss: 1.6478\n",
      "Epoch [4/10], Step [522/625], Loss: 1.3962\n",
      "Epoch [4/10], Step [523/625], Loss: 1.3182\n",
      "Epoch [4/10], Step [524/625], Loss: 1.3955\n",
      "Epoch [4/10], Step [525/625], Loss: 1.6613\n",
      "Epoch [4/10], Step [526/625], Loss: 1.4173\n",
      "Epoch [4/10], Step [527/625], Loss: 1.1575\n",
      "Epoch [4/10], Step [528/625], Loss: 0.9079\n",
      "Epoch [4/10], Step [529/625], Loss: 1.1542\n",
      "Epoch [4/10], Step [530/625], Loss: 1.4047\n",
      "Epoch [4/10], Step [531/625], Loss: 1.4043\n",
      "Epoch [4/10], Step [532/625], Loss: 1.6542\n",
      "Epoch [4/10], Step [533/625], Loss: 1.5769\n",
      "Epoch [4/10], Step [534/625], Loss: 1.6525\n",
      "Epoch [4/10], Step [535/625], Loss: 1.6581\n",
      "Epoch [4/10], Step [536/625], Loss: 1.2191\n",
      "Epoch [4/10], Step [537/625], Loss: 0.9263\n",
      "Epoch [4/10], Step [538/625], Loss: 0.9061\n",
      "Epoch [4/10], Step [539/625], Loss: 1.6423\n",
      "Epoch [4/10], Step [540/625], Loss: 1.2590\n",
      "Epoch [4/10], Step [541/625], Loss: 1.4066\n",
      "Epoch [4/10], Step [542/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [543/625], Loss: 1.1551\n",
      "Epoch [4/10], Step [544/625], Loss: 1.3993\n",
      "Epoch [4/10], Step [545/625], Loss: 1.5818\n",
      "Epoch [4/10], Step [546/625], Loss: 1.1658\n",
      "Epoch [4/10], Step [547/625], Loss: 1.4055\n",
      "Epoch [4/10], Step [548/625], Loss: 1.3632\n",
      "Epoch [4/10], Step [549/625], Loss: 1.4887\n",
      "Epoch [4/10], Step [550/625], Loss: 1.4174\n",
      "Epoch [4/10], Step [551/625], Loss: 1.2925\n",
      "Epoch [4/10], Step [552/625], Loss: 0.9112\n",
      "Epoch [4/10], Step [553/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [554/625], Loss: 1.3892\n",
      "Epoch [4/10], Step [555/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [556/625], Loss: 1.9048\n",
      "Epoch [4/10], Step [557/625], Loss: 1.6542\n",
      "Epoch [4/10], Step [558/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [559/625], Loss: 0.9125\n",
      "Epoch [4/10], Step [560/625], Loss: 1.4028\n",
      "Epoch [4/10], Step [561/625], Loss: 1.0495\n",
      "Epoch [4/10], Step [562/625], Loss: 1.2722\n",
      "Epoch [4/10], Step [563/625], Loss: 1.1421\n",
      "Epoch [4/10], Step [564/625], Loss: 1.6907\n",
      "Epoch [4/10], Step [565/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [566/625], Loss: 1.4012\n",
      "Epoch [4/10], Step [567/625], Loss: 1.5706\n",
      "Epoch [4/10], Step [568/625], Loss: 1.6100\n",
      "Epoch [4/10], Step [569/625], Loss: 1.1560\n",
      "Epoch [4/10], Step [570/625], Loss: 1.5278\n",
      "Epoch [4/10], Step [571/625], Loss: 1.6531\n",
      "Epoch [4/10], Step [572/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [573/625], Loss: 1.5532\n",
      "Epoch [4/10], Step [574/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [575/625], Loss: 1.4056\n",
      "Epoch [4/10], Step [576/625], Loss: 1.1462\n",
      "Epoch [4/10], Step [577/625], Loss: 1.6566\n",
      "Epoch [4/10], Step [578/625], Loss: 1.4002\n",
      "Epoch [4/10], Step [579/625], Loss: 1.9027\n",
      "Epoch [4/10], Step [580/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [581/625], Loss: 1.6408\n",
      "Epoch [4/10], Step [582/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [583/625], Loss: 1.6471\n",
      "Epoch [4/10], Step [584/625], Loss: 1.6538\n",
      "Epoch [4/10], Step [585/625], Loss: 1.5028\n",
      "Epoch [4/10], Step [586/625], Loss: 1.1624\n",
      "Epoch [4/10], Step [587/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [588/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [589/625], Loss: 1.4097\n",
      "Epoch [4/10], Step [590/625], Loss: 1.9046\n",
      "Epoch [4/10], Step [591/625], Loss: 1.1624\n",
      "Epoch [4/10], Step [592/625], Loss: 1.1551\n",
      "Epoch [4/10], Step [593/625], Loss: 1.2567\n",
      "Epoch [4/10], Step [594/625], Loss: 1.4227\n",
      "Epoch [4/10], Step [595/625], Loss: 1.9041\n",
      "Epoch [4/10], Step [596/625], Loss: 1.4075\n",
      "Epoch [4/10], Step [597/625], Loss: 1.6554\n",
      "Epoch [4/10], Step [598/625], Loss: 0.9592\n",
      "Epoch [4/10], Step [599/625], Loss: 1.7216\n",
      "Epoch [4/10], Step [600/625], Loss: 0.9197\n",
      "Epoch [4/10], Step [601/625], Loss: 0.9085\n",
      "Epoch [4/10], Step [602/625], Loss: 1.1415\n",
      "Epoch [4/10], Step [603/625], Loss: 1.6405\n",
      "Epoch [4/10], Step [604/625], Loss: 1.6548\n",
      "Epoch [4/10], Step [605/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [606/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [607/625], Loss: 1.3886\n",
      "Epoch [4/10], Step [608/625], Loss: 0.9048\n",
      "Epoch [4/10], Step [609/625], Loss: 1.4020\n",
      "Epoch [4/10], Step [610/625], Loss: 1.4048\n",
      "Epoch [4/10], Step [611/625], Loss: 1.4107\n",
      "Epoch [4/10], Step [612/625], Loss: 1.1548\n",
      "Epoch [4/10], Step [613/625], Loss: 1.1549\n",
      "Epoch [4/10], Step [614/625], Loss: 0.9050\n",
      "Epoch [4/10], Step [615/625], Loss: 1.4008\n",
      "Epoch [4/10], Step [616/625], Loss: 1.1556\n",
      "Epoch [4/10], Step [617/625], Loss: 1.6546\n",
      "Epoch [4/10], Step [618/625], Loss: 1.2085\n",
      "Epoch [4/10], Step [619/625], Loss: 1.8975\n",
      "Epoch [4/10], Step [620/625], Loss: 1.1539\n",
      "Epoch [4/10], Step [621/625], Loss: 1.5721\n",
      "Epoch [4/10], Step [622/625], Loss: 1.4032\n",
      "Epoch [4/10], Step [623/625], Loss: 1.1515\n",
      "Epoch [4/10], Step [624/625], Loss: 1.3367\n",
      "Epoch [4/10], Step [625/625], Loss: 1.3988\n",
      "Epoch [5/10], Step [1/625], Loss: 1.6120\n",
      "Epoch [5/10], Step [2/625], Loss: 1.1677\n",
      "Epoch [5/10], Step [3/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [4/625], Loss: 1.4177\n",
      "Epoch [5/10], Step [5/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [6/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [7/625], Loss: 1.1299\n",
      "Epoch [5/10], Step [8/625], Loss: 1.1547\n",
      "Epoch [5/10], Step [9/625], Loss: 0.9069\n",
      "Epoch [5/10], Step [10/625], Loss: 1.1397\n",
      "Epoch [5/10], Step [11/625], Loss: 1.4981\n",
      "Epoch [5/10], Step [12/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [13/625], Loss: 1.2906\n",
      "Epoch [5/10], Step [14/625], Loss: 1.1513\n",
      "Epoch [5/10], Step [15/625], Loss: 1.3619\n",
      "Epoch [5/10], Step [16/625], Loss: 1.1571\n",
      "Epoch [5/10], Step [17/625], Loss: 1.1500\n",
      "Epoch [5/10], Step [18/625], Loss: 1.1515\n",
      "Epoch [5/10], Step [19/625], Loss: 1.3997\n",
      "Epoch [5/10], Step [20/625], Loss: 1.3924\n",
      "Epoch [5/10], Step [21/625], Loss: 1.0323\n",
      "Epoch [5/10], Step [22/625], Loss: 1.3911\n",
      "Epoch [5/10], Step [23/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [24/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [25/625], Loss: 1.6546\n",
      "Epoch [5/10], Step [26/625], Loss: 1.3947\n",
      "Epoch [5/10], Step [27/625], Loss: 1.5668\n",
      "Epoch [5/10], Step [28/625], Loss: 1.1916\n",
      "Epoch [5/10], Step [29/625], Loss: 1.3416\n",
      "Epoch [5/10], Step [30/625], Loss: 1.1559\n",
      "Epoch [5/10], Step [31/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [32/625], Loss: 1.2298\n",
      "Epoch [5/10], Step [33/625], Loss: 1.3569\n",
      "Epoch [5/10], Step [34/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [35/625], Loss: 0.9996\n",
      "Epoch [5/10], Step [36/625], Loss: 1.3938\n",
      "Epoch [5/10], Step [37/625], Loss: 1.4317\n",
      "Epoch [5/10], Step [38/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [39/625], Loss: 1.6479\n",
      "Epoch [5/10], Step [40/625], Loss: 1.1391\n",
      "Epoch [5/10], Step [41/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [42/625], Loss: 1.3979\n",
      "Epoch [5/10], Step [43/625], Loss: 1.3871\n",
      "Epoch [5/10], Step [44/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [45/625], Loss: 0.9079\n",
      "Epoch [5/10], Step [46/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [47/625], Loss: 1.1588\n",
      "Epoch [5/10], Step [48/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [49/625], Loss: 1.6546\n",
      "Epoch [5/10], Step [50/625], Loss: 0.9099\n",
      "Epoch [5/10], Step [51/625], Loss: 1.4046\n",
      "Epoch [5/10], Step [52/625], Loss: 1.1508\n",
      "Epoch [5/10], Step [53/625], Loss: 1.5624\n",
      "Epoch [5/10], Step [54/625], Loss: 1.0818\n",
      "Epoch [5/10], Step [55/625], Loss: 1.6286\n",
      "Epoch [5/10], Step [56/625], Loss: 1.4023\n",
      "Epoch [5/10], Step [57/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [58/625], Loss: 1.5758\n",
      "Epoch [5/10], Step [59/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [60/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [61/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [62/625], Loss: 1.2901\n",
      "Epoch [5/10], Step [63/625], Loss: 1.8568\n",
      "Epoch [5/10], Step [64/625], Loss: 1.1662\n",
      "Epoch [5/10], Step [65/625], Loss: 1.3963\n",
      "Epoch [5/10], Step [66/625], Loss: 1.3090\n",
      "Epoch [5/10], Step [67/625], Loss: 1.1775\n",
      "Epoch [5/10], Step [68/625], Loss: 0.9346\n",
      "Epoch [5/10], Step [69/625], Loss: 1.6378\n",
      "Epoch [5/10], Step [70/625], Loss: 1.6409\n",
      "Epoch [5/10], Step [71/625], Loss: 1.4046\n",
      "Epoch [5/10], Step [72/625], Loss: 1.1634\n",
      "Epoch [5/10], Step [73/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [74/625], Loss: 1.2174\n",
      "Epoch [5/10], Step [75/625], Loss: 1.6059\n",
      "Epoch [5/10], Step [76/625], Loss: 0.9101\n",
      "Epoch [5/10], Step [77/625], Loss: 1.4034\n",
      "Epoch [5/10], Step [78/625], Loss: 1.8865\n",
      "Epoch [5/10], Step [79/625], Loss: 1.3930\n",
      "Epoch [5/10], Step [80/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [81/625], Loss: 1.1733\n",
      "Epoch [5/10], Step [82/625], Loss: 1.4238\n",
      "Epoch [5/10], Step [83/625], Loss: 1.1589\n",
      "Epoch [5/10], Step [84/625], Loss: 1.1446\n",
      "Epoch [5/10], Step [85/625], Loss: 1.4005\n",
      "Epoch [5/10], Step [86/625], Loss: 1.4036\n",
      "Epoch [5/10], Step [87/625], Loss: 1.1547\n",
      "Epoch [5/10], Step [88/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [89/625], Loss: 1.8802\n",
      "Epoch [5/10], Step [90/625], Loss: 1.4038\n",
      "Epoch [5/10], Step [91/625], Loss: 1.6547\n",
      "Epoch [5/10], Step [92/625], Loss: 1.6490\n",
      "Epoch [5/10], Step [93/625], Loss: 1.1412\n",
      "Epoch [5/10], Step [94/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [95/625], Loss: 1.6547\n",
      "Epoch [5/10], Step [96/625], Loss: 1.4030\n",
      "Epoch [5/10], Step [97/625], Loss: 1.1585\n",
      "Epoch [5/10], Step [98/625], Loss: 0.9552\n",
      "Epoch [5/10], Step [99/625], Loss: 1.4767\n",
      "Epoch [5/10], Step [100/625], Loss: 1.4204\n",
      "Epoch [5/10], Step [101/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [102/625], Loss: 1.1153\n",
      "Epoch [5/10], Step [103/625], Loss: 1.6248\n",
      "Epoch [5/10], Step [104/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [105/625], Loss: 1.4605\n",
      "Epoch [5/10], Step [106/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [107/625], Loss: 1.2107\n",
      "Epoch [5/10], Step [108/625], Loss: 1.6278\n",
      "Epoch [5/10], Step [109/625], Loss: 1.1362\n",
      "Epoch [5/10], Step [110/625], Loss: 1.7336\n",
      "Epoch [5/10], Step [111/625], Loss: 1.6483\n",
      "Epoch [5/10], Step [112/625], Loss: 1.3804\n",
      "Epoch [5/10], Step [113/625], Loss: 1.3102\n",
      "Epoch [5/10], Step [114/625], Loss: 1.8380\n",
      "Epoch [5/10], Step [115/625], Loss: 1.6526\n",
      "Epoch [5/10], Step [116/625], Loss: 1.4103\n",
      "Epoch [5/10], Step [117/625], Loss: 1.1717\n",
      "Epoch [5/10], Step [118/625], Loss: 1.3289\n",
      "Epoch [5/10], Step [119/625], Loss: 1.2781\n",
      "Epoch [5/10], Step [120/625], Loss: 1.4065\n",
      "Epoch [5/10], Step [121/625], Loss: 1.2068\n",
      "Epoch [5/10], Step [122/625], Loss: 0.9792\n",
      "Epoch [5/10], Step [123/625], Loss: 0.9397\n",
      "Epoch [5/10], Step [124/625], Loss: 1.4008\n",
      "Epoch [5/10], Step [125/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [126/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [127/625], Loss: 1.6405\n",
      "Epoch [5/10], Step [128/625], Loss: 1.2583\n",
      "Epoch [5/10], Step [129/625], Loss: 1.6545\n",
      "Epoch [5/10], Step [130/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [131/625], Loss: 0.9447\n",
      "Epoch [5/10], Step [132/625], Loss: 1.1540\n",
      "Epoch [5/10], Step [133/625], Loss: 1.6499\n",
      "Epoch [5/10], Step [134/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [135/625], Loss: 1.3592\n",
      "Epoch [5/10], Step [136/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [137/625], Loss: 0.9245\n",
      "Epoch [5/10], Step [138/625], Loss: 1.2066\n",
      "Epoch [5/10], Step [139/625], Loss: 1.3945\n",
      "Epoch [5/10], Step [140/625], Loss: 1.4181\n",
      "Epoch [5/10], Step [141/625], Loss: 1.6057\n",
      "Epoch [5/10], Step [142/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [143/625], Loss: 1.1389\n",
      "Epoch [5/10], Step [144/625], Loss: 1.1389\n",
      "Epoch [5/10], Step [145/625], Loss: 0.9261\n",
      "Epoch [5/10], Step [146/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [147/625], Loss: 1.2434\n",
      "Epoch [5/10], Step [148/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [149/625], Loss: 1.6549\n",
      "Epoch [5/10], Step [150/625], Loss: 1.1174\n",
      "Epoch [5/10], Step [151/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [152/625], Loss: 1.7807\n",
      "Epoch [5/10], Step [153/625], Loss: 1.1932\n",
      "Epoch [5/10], Step [154/625], Loss: 1.1551\n",
      "Epoch [5/10], Step [155/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [156/625], Loss: 1.2844\n",
      "Epoch [5/10], Step [157/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [158/625], Loss: 1.4286\n",
      "Epoch [5/10], Step [159/625], Loss: 1.6477\n",
      "Epoch [5/10], Step [160/625], Loss: 1.1387\n",
      "Epoch [5/10], Step [161/625], Loss: 1.8910\n",
      "Epoch [5/10], Step [162/625], Loss: 1.3549\n",
      "Epoch [5/10], Step [163/625], Loss: 1.2551\n",
      "Epoch [5/10], Step [164/625], Loss: 1.4660\n",
      "Epoch [5/10], Step [165/625], Loss: 1.3892\n",
      "Epoch [5/10], Step [166/625], Loss: 1.6521\n",
      "Epoch [5/10], Step [167/625], Loss: 1.6422\n",
      "Epoch [5/10], Step [168/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [169/625], Loss: 1.4066\n",
      "Epoch [5/10], Step [170/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [171/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [172/625], Loss: 1.0270\n",
      "Epoch [5/10], Step [173/625], Loss: 1.4047\n",
      "Epoch [5/10], Step [174/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [175/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [176/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [177/625], Loss: 0.9120\n",
      "Epoch [5/10], Step [178/625], Loss: 1.6558\n",
      "Epoch [5/10], Step [179/625], Loss: 1.6526\n",
      "Epoch [5/10], Step [180/625], Loss: 1.4047\n",
      "Epoch [5/10], Step [181/625], Loss: 1.4019\n",
      "Epoch [5/10], Step [182/625], Loss: 1.6387\n",
      "Epoch [5/10], Step [183/625], Loss: 1.3891\n",
      "Epoch [5/10], Step [184/625], Loss: 1.5264\n",
      "Epoch [5/10], Step [185/625], Loss: 1.3888\n",
      "Epoch [5/10], Step [186/625], Loss: 1.1479\n",
      "Epoch [5/10], Step [187/625], Loss: 1.4809\n",
      "Epoch [5/10], Step [188/625], Loss: 1.2780\n",
      "Epoch [5/10], Step [189/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [190/625], Loss: 1.2670\n",
      "Epoch [5/10], Step [191/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [192/625], Loss: 1.3551\n",
      "Epoch [5/10], Step [193/625], Loss: 1.6585\n",
      "Epoch [5/10], Step [194/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [195/625], Loss: 1.2680\n",
      "Epoch [5/10], Step [196/625], Loss: 1.1849\n",
      "Epoch [5/10], Step [197/625], Loss: 1.6223\n",
      "Epoch [5/10], Step [198/625], Loss: 1.3871\n",
      "Epoch [5/10], Step [199/625], Loss: 1.4295\n",
      "Epoch [5/10], Step [200/625], Loss: 1.4600\n",
      "Epoch [5/10], Step [201/625], Loss: 1.4068\n",
      "Epoch [5/10], Step [202/625], Loss: 1.1547\n",
      "Epoch [5/10], Step [203/625], Loss: 1.3755\n",
      "Epoch [5/10], Step [204/625], Loss: 1.7522\n",
      "Epoch [5/10], Step [205/625], Loss: 1.4033\n",
      "Epoch [5/10], Step [206/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [207/625], Loss: 1.3943\n",
      "Epoch [5/10], Step [208/625], Loss: 1.6537\n",
      "Epoch [5/10], Step [209/625], Loss: 1.1559\n",
      "Epoch [5/10], Step [210/625], Loss: 1.6506\n",
      "Epoch [5/10], Step [211/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [212/625], Loss: 1.4032\n",
      "Epoch [5/10], Step [213/625], Loss: 1.4132\n",
      "Epoch [5/10], Step [214/625], Loss: 1.1564\n",
      "Epoch [5/10], Step [215/625], Loss: 1.4014\n",
      "Epoch [5/10], Step [216/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [217/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [218/625], Loss: 1.6458\n",
      "Epoch [5/10], Step [219/625], Loss: 1.1555\n",
      "Epoch [5/10], Step [220/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [221/625], Loss: 1.3997\n",
      "Epoch [5/10], Step [222/625], Loss: 1.6386\n",
      "Epoch [5/10], Step [223/625], Loss: 1.1710\n",
      "Epoch [5/10], Step [224/625], Loss: 1.4211\n",
      "Epoch [5/10], Step [225/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [226/625], Loss: 1.3955\n",
      "Epoch [5/10], Step [227/625], Loss: 1.2353\n",
      "Epoch [5/10], Step [228/625], Loss: 1.3857\n",
      "Epoch [5/10], Step [229/625], Loss: 1.6530\n",
      "Epoch [5/10], Step [230/625], Loss: 1.1555\n",
      "Epoch [5/10], Step [231/625], Loss: 1.1841\n",
      "Epoch [5/10], Step [232/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [233/625], Loss: 1.1826\n",
      "Epoch [5/10], Step [234/625], Loss: 1.8951\n",
      "Epoch [5/10], Step [235/625], Loss: 1.7541\n",
      "Epoch [5/10], Step [236/625], Loss: 1.6225\n",
      "Epoch [5/10], Step [237/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [238/625], Loss: 1.4051\n",
      "Epoch [5/10], Step [239/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [240/625], Loss: 1.2610\n",
      "Epoch [5/10], Step [241/625], Loss: 1.6547\n",
      "Epoch [5/10], Step [242/625], Loss: 1.6473\n",
      "Epoch [5/10], Step [243/625], Loss: 1.7404\n",
      "Epoch [5/10], Step [244/625], Loss: 1.4537\n",
      "Epoch [5/10], Step [245/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [246/625], Loss: 1.4041\n",
      "Epoch [5/10], Step [247/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [248/625], Loss: 1.6541\n",
      "Epoch [5/10], Step [249/625], Loss: 1.3731\n",
      "Epoch [5/10], Step [250/625], Loss: 1.2163\n",
      "Epoch [5/10], Step [251/625], Loss: 1.4047\n",
      "Epoch [5/10], Step [252/625], Loss: 1.1529\n",
      "Epoch [5/10], Step [253/625], Loss: 1.1545\n",
      "Epoch [5/10], Step [254/625], Loss: 1.7765\n",
      "Epoch [5/10], Step [255/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [256/625], Loss: 1.6219\n",
      "Epoch [5/10], Step [257/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [258/625], Loss: 1.4046\n",
      "Epoch [5/10], Step [259/625], Loss: 1.3908\n",
      "Epoch [5/10], Step [260/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [261/625], Loss: 1.6546\n",
      "Epoch [5/10], Step [262/625], Loss: 1.1372\n",
      "Epoch [5/10], Step [263/625], Loss: 1.1532\n",
      "Epoch [5/10], Step [264/625], Loss: 1.1656\n",
      "Epoch [5/10], Step [265/625], Loss: 1.4035\n",
      "Epoch [5/10], Step [266/625], Loss: 1.6348\n",
      "Epoch [5/10], Step [267/625], Loss: 1.4044\n",
      "Epoch [5/10], Step [268/625], Loss: 1.9012\n",
      "Epoch [5/10], Step [269/625], Loss: 1.4105\n",
      "Epoch [5/10], Step [270/625], Loss: 0.9057\n",
      "Epoch [5/10], Step [271/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [272/625], Loss: 1.4047\n",
      "Epoch [5/10], Step [273/625], Loss: 1.1585\n",
      "Epoch [5/10], Step [274/625], Loss: 1.3977\n",
      "Epoch [5/10], Step [275/625], Loss: 1.9048\n",
      "Epoch [5/10], Step [276/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [277/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [278/625], Loss: 1.4162\n",
      "Epoch [5/10], Step [279/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [280/625], Loss: 1.4040\n",
      "Epoch [5/10], Step [281/625], Loss: 1.8920\n",
      "Epoch [5/10], Step [282/625], Loss: 1.4050\n",
      "Epoch [5/10], Step [283/625], Loss: 1.6284\n",
      "Epoch [5/10], Step [284/625], Loss: 0.9268\n",
      "Epoch [5/10], Step [285/625], Loss: 1.3944\n",
      "Epoch [5/10], Step [286/625], Loss: 1.4019\n",
      "Epoch [5/10], Step [287/625], Loss: 1.4012\n",
      "Epoch [5/10], Step [288/625], Loss: 0.9097\n",
      "Epoch [5/10], Step [289/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [290/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [291/625], Loss: 0.9095\n",
      "Epoch [5/10], Step [292/625], Loss: 1.6419\n",
      "Epoch [5/10], Step [293/625], Loss: 1.4426\n",
      "Epoch [5/10], Step [294/625], Loss: 1.1543\n",
      "Epoch [5/10], Step [295/625], Loss: 1.6568\n",
      "Epoch [5/10], Step [296/625], Loss: 0.9059\n",
      "Epoch [5/10], Step [297/625], Loss: 1.3781\n",
      "Epoch [5/10], Step [298/625], Loss: 1.1219\n",
      "Epoch [5/10], Step [299/625], Loss: 1.3758\n",
      "Epoch [5/10], Step [300/625], Loss: 1.2415\n",
      "Epoch [5/10], Step [301/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [302/625], Loss: 1.1535\n",
      "Epoch [5/10], Step [303/625], Loss: 1.9048\n",
      "Epoch [5/10], Step [304/625], Loss: 1.3866\n",
      "Epoch [5/10], Step [305/625], Loss: 1.1563\n",
      "Epoch [5/10], Step [306/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [307/625], Loss: 1.3993\n",
      "Epoch [5/10], Step [308/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [309/625], Loss: 1.6515\n",
      "Epoch [5/10], Step [310/625], Loss: 1.1695\n",
      "Epoch [5/10], Step [311/625], Loss: 1.1505\n",
      "Epoch [5/10], Step [312/625], Loss: 1.1447\n",
      "Epoch [5/10], Step [313/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [314/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [315/625], Loss: 1.1390\n",
      "Epoch [5/10], Step [316/625], Loss: 1.3808\n",
      "Epoch [5/10], Step [317/625], Loss: 1.6372\n",
      "Epoch [5/10], Step [318/625], Loss: 1.3899\n",
      "Epoch [5/10], Step [319/625], Loss: 1.1541\n",
      "Epoch [5/10], Step [320/625], Loss: 0.9112\n",
      "Epoch [5/10], Step [321/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [322/625], Loss: 1.1543\n",
      "Epoch [5/10], Step [323/625], Loss: 1.4122\n",
      "Epoch [5/10], Step [324/625], Loss: 1.1515\n",
      "Epoch [5/10], Step [325/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [326/625], Loss: 0.9086\n",
      "Epoch [5/10], Step [327/625], Loss: 1.6543\n",
      "Epoch [5/10], Step [328/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [329/625], Loss: 1.5141\n",
      "Epoch [5/10], Step [330/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [331/625], Loss: 1.1477\n",
      "Epoch [5/10], Step [332/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [333/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [334/625], Loss: 1.3776\n",
      "Epoch [5/10], Step [335/625], Loss: 1.4283\n",
      "Epoch [5/10], Step [336/625], Loss: 0.9061\n",
      "Epoch [5/10], Step [337/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [338/625], Loss: 1.1299\n",
      "Epoch [5/10], Step [339/625], Loss: 1.3162\n",
      "Epoch [5/10], Step [340/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [341/625], Loss: 1.4051\n",
      "Epoch [5/10], Step [342/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [343/625], Loss: 1.1307\n",
      "Epoch [5/10], Step [344/625], Loss: 1.4085\n",
      "Epoch [5/10], Step [345/625], Loss: 1.1611\n",
      "Epoch [5/10], Step [346/625], Loss: 1.5417\n",
      "Epoch [5/10], Step [347/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [348/625], Loss: 1.4042\n",
      "Epoch [5/10], Step [349/625], Loss: 1.6461\n",
      "Epoch [5/10], Step [350/625], Loss: 1.6514\n",
      "Epoch [5/10], Step [351/625], Loss: 1.4051\n",
      "Epoch [5/10], Step [352/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [353/625], Loss: 1.4066\n",
      "Epoch [5/10], Step [354/625], Loss: 1.6418\n",
      "Epoch [5/10], Step [355/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [356/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [357/625], Loss: 1.3260\n",
      "Epoch [5/10], Step [358/625], Loss: 1.3924\n",
      "Epoch [5/10], Step [359/625], Loss: 1.4172\n",
      "Epoch [5/10], Step [360/625], Loss: 1.3994\n",
      "Epoch [5/10], Step [361/625], Loss: 1.5426\n",
      "Epoch [5/10], Step [362/625], Loss: 1.4172\n",
      "Epoch [5/10], Step [363/625], Loss: 1.3617\n",
      "Epoch [5/10], Step [364/625], Loss: 1.3766\n",
      "Epoch [5/10], Step [365/625], Loss: 1.1722\n",
      "Epoch [5/10], Step [366/625], Loss: 0.9052\n",
      "Epoch [5/10], Step [367/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [368/625], Loss: 1.1380\n",
      "Epoch [5/10], Step [369/625], Loss: 1.1539\n",
      "Epoch [5/10], Step [370/625], Loss: 1.2268\n",
      "Epoch [5/10], Step [371/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [372/625], Loss: 1.4047\n",
      "Epoch [5/10], Step [373/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [374/625], Loss: 1.9003\n",
      "Epoch [5/10], Step [375/625], Loss: 1.8520\n",
      "Epoch [5/10], Step [376/625], Loss: 1.6398\n",
      "Epoch [5/10], Step [377/625], Loss: 1.6580\n",
      "Epoch [5/10], Step [378/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [379/625], Loss: 1.6315\n",
      "Epoch [5/10], Step [380/625], Loss: 1.4027\n",
      "Epoch [5/10], Step [381/625], Loss: 1.4033\n",
      "Epoch [5/10], Step [382/625], Loss: 1.1571\n",
      "Epoch [5/10], Step [383/625], Loss: 1.1558\n",
      "Epoch [5/10], Step [384/625], Loss: 1.6545\n",
      "Epoch [5/10], Step [385/625], Loss: 0.9057\n",
      "Epoch [5/10], Step [386/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [387/625], Loss: 1.1769\n",
      "Epoch [5/10], Step [388/625], Loss: 1.1613\n",
      "Epoch [5/10], Step [389/625], Loss: 1.5835\n",
      "Epoch [5/10], Step [390/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [391/625], Loss: 1.1669\n",
      "Epoch [5/10], Step [392/625], Loss: 1.4072\n",
      "Epoch [5/10], Step [393/625], Loss: 1.6528\n",
      "Epoch [5/10], Step [394/625], Loss: 1.3875\n",
      "Epoch [5/10], Step [395/625], Loss: 1.6512\n",
      "Epoch [5/10], Step [396/625], Loss: 1.5226\n",
      "Epoch [5/10], Step [397/625], Loss: 1.3135\n",
      "Epoch [5/10], Step [398/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [399/625], Loss: 1.2610\n",
      "Epoch [5/10], Step [400/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [401/625], Loss: 1.3893\n",
      "Epoch [5/10], Step [402/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [403/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [404/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [405/625], Loss: 1.3968\n",
      "Epoch [5/10], Step [406/625], Loss: 1.6442\n",
      "Epoch [5/10], Step [407/625], Loss: 1.6374\n",
      "Epoch [5/10], Step [408/625], Loss: 1.6521\n",
      "Epoch [5/10], Step [409/625], Loss: 1.0103\n",
      "Epoch [5/10], Step [410/625], Loss: 1.1722\n",
      "Epoch [5/10], Step [411/625], Loss: 1.2044\n",
      "Epoch [5/10], Step [412/625], Loss: 1.3118\n",
      "Epoch [5/10], Step [413/625], Loss: 1.6521\n",
      "Epoch [5/10], Step [414/625], Loss: 1.6275\n",
      "Epoch [5/10], Step [415/625], Loss: 1.3429\n",
      "Epoch [5/10], Step [416/625], Loss: 1.1685\n",
      "Epoch [5/10], Step [417/625], Loss: 1.0942\n",
      "Epoch [5/10], Step [418/625], Loss: 1.5556\n",
      "Epoch [5/10], Step [419/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [420/625], Loss: 1.6326\n",
      "Epoch [5/10], Step [421/625], Loss: 1.4190\n",
      "Epoch [5/10], Step [422/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [423/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [424/625], Loss: 1.2922\n",
      "Epoch [5/10], Step [425/625], Loss: 1.1959\n",
      "Epoch [5/10], Step [426/625], Loss: 1.4372\n",
      "Epoch [5/10], Step [427/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [428/625], Loss: 1.1761\n",
      "Epoch [5/10], Step [429/625], Loss: 1.3397\n",
      "Epoch [5/10], Step [430/625], Loss: 1.1152\n",
      "Epoch [5/10], Step [431/625], Loss: 1.5684\n",
      "Epoch [5/10], Step [432/625], Loss: 0.9233\n",
      "Epoch [5/10], Step [433/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [434/625], Loss: 1.3621\n",
      "Epoch [5/10], Step [435/625], Loss: 1.4036\n",
      "Epoch [5/10], Step [436/625], Loss: 1.6495\n",
      "Epoch [5/10], Step [437/625], Loss: 1.6515\n",
      "Epoch [5/10], Step [438/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [439/625], Loss: 1.6547\n",
      "Epoch [5/10], Step [440/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [441/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [442/625], Loss: 1.0856\n",
      "Epoch [5/10], Step [443/625], Loss: 1.6340\n",
      "Epoch [5/10], Step [444/625], Loss: 1.4987\n",
      "Epoch [5/10], Step [445/625], Loss: 1.3941\n",
      "Epoch [5/10], Step [446/625], Loss: 1.3977\n",
      "Epoch [5/10], Step [447/625], Loss: 0.9443\n",
      "Epoch [5/10], Step [448/625], Loss: 1.1228\n",
      "Epoch [5/10], Step [449/625], Loss: 1.4398\n",
      "Epoch [5/10], Step [450/625], Loss: 1.6567\n",
      "Epoch [5/10], Step [451/625], Loss: 1.3101\n",
      "Epoch [5/10], Step [452/625], Loss: 1.1545\n",
      "Epoch [5/10], Step [453/625], Loss: 1.0938\n",
      "Epoch [5/10], Step [454/625], Loss: 1.9010\n",
      "Epoch [5/10], Step [455/625], Loss: 1.9037\n",
      "Epoch [5/10], Step [456/625], Loss: 1.4107\n",
      "Epoch [5/10], Step [457/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [458/625], Loss: 1.6549\n",
      "Epoch [5/10], Step [459/625], Loss: 1.0797\n",
      "Epoch [5/10], Step [460/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [461/625], Loss: 1.6307\n",
      "Epoch [5/10], Step [462/625], Loss: 1.8613\n",
      "Epoch [5/10], Step [463/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [464/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [465/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [466/625], Loss: 1.8809\n",
      "Epoch [5/10], Step [467/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [468/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [469/625], Loss: 1.1390\n",
      "Epoch [5/10], Step [470/625], Loss: 1.1576\n",
      "Epoch [5/10], Step [471/625], Loss: 1.1583\n",
      "Epoch [5/10], Step [472/625], Loss: 1.1502\n",
      "Epoch [5/10], Step [473/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [474/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [475/625], Loss: 1.6464\n",
      "Epoch [5/10], Step [476/625], Loss: 1.6504\n",
      "Epoch [5/10], Step [477/625], Loss: 1.3873\n",
      "Epoch [5/10], Step [478/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [479/625], Loss: 1.6230\n",
      "Epoch [5/10], Step [480/625], Loss: 1.3585\n",
      "Epoch [5/10], Step [481/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [482/625], Loss: 0.9422\n",
      "Epoch [5/10], Step [483/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [484/625], Loss: 1.4578\n",
      "Epoch [5/10], Step [485/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [486/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [487/625], Loss: 1.7771\n",
      "Epoch [5/10], Step [488/625], Loss: 1.6298\n",
      "Epoch [5/10], Step [489/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [490/625], Loss: 1.4029\n",
      "Epoch [5/10], Step [491/625], Loss: 1.9031\n",
      "Epoch [5/10], Step [492/625], Loss: 1.3840\n",
      "Epoch [5/10], Step [493/625], Loss: 1.4041\n",
      "Epoch [5/10], Step [494/625], Loss: 1.6429\n",
      "Epoch [5/10], Step [495/625], Loss: 1.6546\n",
      "Epoch [5/10], Step [496/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [497/625], Loss: 1.5422\n",
      "Epoch [5/10], Step [498/625], Loss: 1.1547\n",
      "Epoch [5/10], Step [499/625], Loss: 1.5076\n",
      "Epoch [5/10], Step [500/625], Loss: 1.9037\n",
      "Epoch [5/10], Step [501/625], Loss: 1.6364\n",
      "Epoch [5/10], Step [502/625], Loss: 1.6402\n",
      "Epoch [5/10], Step [503/625], Loss: 1.9033\n",
      "Epoch [5/10], Step [504/625], Loss: 1.1383\n",
      "Epoch [5/10], Step [505/625], Loss: 0.9050\n",
      "Epoch [5/10], Step [506/625], Loss: 1.4044\n",
      "Epoch [5/10], Step [507/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [508/625], Loss: 1.6527\n",
      "Epoch [5/10], Step [509/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [510/625], Loss: 1.8890\n",
      "Epoch [5/10], Step [511/625], Loss: 1.6289\n",
      "Epoch [5/10], Step [512/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [513/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [514/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [515/625], Loss: 1.6506\n",
      "Epoch [5/10], Step [516/625], Loss: 1.4045\n",
      "Epoch [5/10], Step [517/625], Loss: 1.1107\n",
      "Epoch [5/10], Step [518/625], Loss: 1.9048\n",
      "Epoch [5/10], Step [519/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [520/625], Loss: 1.6472\n",
      "Epoch [5/10], Step [521/625], Loss: 1.1547\n",
      "Epoch [5/10], Step [522/625], Loss: 1.0221\n",
      "Epoch [5/10], Step [523/625], Loss: 1.8965\n",
      "Epoch [5/10], Step [524/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [525/625], Loss: 1.6507\n",
      "Epoch [5/10], Step [526/625], Loss: 0.9053\n",
      "Epoch [5/10], Step [527/625], Loss: 1.3812\n",
      "Epoch [5/10], Step [528/625], Loss: 1.3949\n",
      "Epoch [5/10], Step [529/625], Loss: 1.3232\n",
      "Epoch [5/10], Step [530/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [531/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [532/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [533/625], Loss: 1.2236\n",
      "Epoch [5/10], Step [534/625], Loss: 1.1577\n",
      "Epoch [5/10], Step [535/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [536/625], Loss: 1.1562\n",
      "Epoch [5/10], Step [537/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [538/625], Loss: 1.3933\n",
      "Epoch [5/10], Step [539/625], Loss: 1.1619\n",
      "Epoch [5/10], Step [540/625], Loss: 1.1614\n",
      "Epoch [5/10], Step [541/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [542/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [543/625], Loss: 0.9296\n",
      "Epoch [5/10], Step [544/625], Loss: 1.6518\n",
      "Epoch [5/10], Step [545/625], Loss: 1.3645\n",
      "Epoch [5/10], Step [546/625], Loss: 1.4044\n",
      "Epoch [5/10], Step [547/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [548/625], Loss: 1.3913\n",
      "Epoch [5/10], Step [549/625], Loss: 1.3887\n",
      "Epoch [5/10], Step [550/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [551/625], Loss: 1.3835\n",
      "Epoch [5/10], Step [552/625], Loss: 1.1770\n",
      "Epoch [5/10], Step [553/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [554/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [555/625], Loss: 1.6213\n",
      "Epoch [5/10], Step [556/625], Loss: 1.1589\n",
      "Epoch [5/10], Step [557/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [558/625], Loss: 1.3790\n",
      "Epoch [5/10], Step [559/625], Loss: 1.4043\n",
      "Epoch [5/10], Step [560/625], Loss: 1.4034\n",
      "Epoch [5/10], Step [561/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [562/625], Loss: 0.9312\n",
      "Epoch [5/10], Step [563/625], Loss: 1.1367\n",
      "Epoch [5/10], Step [564/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [565/625], Loss: 1.0333\n",
      "Epoch [5/10], Step [566/625], Loss: 1.1553\n",
      "Epoch [5/10], Step [567/625], Loss: 1.6571\n",
      "Epoch [5/10], Step [568/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [569/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [570/625], Loss: 1.6541\n",
      "Epoch [5/10], Step [571/625], Loss: 1.4970\n",
      "Epoch [5/10], Step [572/625], Loss: 1.1516\n",
      "Epoch [5/10], Step [573/625], Loss: 1.6507\n",
      "Epoch [5/10], Step [574/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [575/625], Loss: 1.1549\n",
      "Epoch [5/10], Step [576/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [577/625], Loss: 1.8968\n",
      "Epoch [5/10], Step [578/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [579/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [580/625], Loss: 0.9131\n",
      "Epoch [5/10], Step [581/625], Loss: 1.3994\n",
      "Epoch [5/10], Step [582/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [583/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [584/625], Loss: 1.4074\n",
      "Epoch [5/10], Step [585/625], Loss: 1.3988\n",
      "Epoch [5/10], Step [586/625], Loss: 1.4046\n",
      "Epoch [5/10], Step [587/625], Loss: 1.4049\n",
      "Epoch [5/10], Step [588/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [589/625], Loss: 1.3986\n",
      "Epoch [5/10], Step [590/625], Loss: 0.9055\n",
      "Epoch [5/10], Step [591/625], Loss: 1.5099\n",
      "Epoch [5/10], Step [592/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [593/625], Loss: 1.4064\n",
      "Epoch [5/10], Step [594/625], Loss: 1.3438\n",
      "Epoch [5/10], Step [595/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [596/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [597/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [598/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [599/625], Loss: 1.6121\n",
      "Epoch [5/10], Step [600/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [601/625], Loss: 0.9049\n",
      "Epoch [5/10], Step [602/625], Loss: 1.4048\n",
      "Epoch [5/10], Step [603/625], Loss: 1.6465\n",
      "Epoch [5/10], Step [604/625], Loss: 0.9048\n",
      "Epoch [5/10], Step [605/625], Loss: 1.4035\n",
      "Epoch [5/10], Step [606/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [607/625], Loss: 1.6533\n",
      "Epoch [5/10], Step [608/625], Loss: 1.1541\n",
      "Epoch [5/10], Step [609/625], Loss: 1.4045\n",
      "Epoch [5/10], Step [610/625], Loss: 0.9060\n",
      "Epoch [5/10], Step [611/625], Loss: 1.1550\n",
      "Epoch [5/10], Step [612/625], Loss: 1.2667\n",
      "Epoch [5/10], Step [613/625], Loss: 1.3147\n",
      "Epoch [5/10], Step [614/625], Loss: 1.6544\n",
      "Epoch [5/10], Step [615/625], Loss: 1.9021\n",
      "Epoch [5/10], Step [616/625], Loss: 1.1548\n",
      "Epoch [5/10], Step [617/625], Loss: 1.1397\n",
      "Epoch [5/10], Step [618/625], Loss: 1.1551\n",
      "Epoch [5/10], Step [619/625], Loss: 1.6427\n",
      "Epoch [5/10], Step [620/625], Loss: 1.6507\n",
      "Epoch [5/10], Step [621/625], Loss: 1.3824\n",
      "Epoch [5/10], Step [622/625], Loss: 1.6548\n",
      "Epoch [5/10], Step [623/625], Loss: 1.2584\n",
      "Epoch [5/10], Step [624/625], Loss: 1.6789\n",
      "Epoch [5/10], Step [625/625], Loss: 1.4082\n",
      "Epoch [6/10], Step [1/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [2/625], Loss: 1.1507\n",
      "Epoch [6/10], Step [3/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [4/625], Loss: 1.9048\n",
      "Epoch [6/10], Step [5/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [6/625], Loss: 1.4769\n",
      "Epoch [6/10], Step [7/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [8/625], Loss: 0.9107\n",
      "Epoch [6/10], Step [9/625], Loss: 1.0012\n",
      "Epoch [6/10], Step [10/625], Loss: 1.1552\n",
      "Epoch [6/10], Step [11/625], Loss: 0.9480\n",
      "Epoch [6/10], Step [12/625], Loss: 0.9354\n",
      "Epoch [6/10], Step [13/625], Loss: 1.1418\n",
      "Epoch [6/10], Step [14/625], Loss: 1.4035\n",
      "Epoch [6/10], Step [15/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [16/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [17/625], Loss: 1.4138\n",
      "Epoch [6/10], Step [18/625], Loss: 1.1590\n",
      "Epoch [6/10], Step [19/625], Loss: 1.2527\n",
      "Epoch [6/10], Step [20/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [21/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [22/625], Loss: 1.1454\n",
      "Epoch [6/10], Step [23/625], Loss: 1.4345\n",
      "Epoch [6/10], Step [24/625], Loss: 1.1905\n",
      "Epoch [6/10], Step [25/625], Loss: 1.5242\n",
      "Epoch [6/10], Step [26/625], Loss: 1.1075\n",
      "Epoch [6/10], Step [27/625], Loss: 1.1555\n",
      "Epoch [6/10], Step [28/625], Loss: 0.9402\n",
      "Epoch [6/10], Step [29/625], Loss: 1.1551\n",
      "Epoch [6/10], Step [30/625], Loss: 1.4034\n",
      "Epoch [6/10], Step [31/625], Loss: 1.1294\n",
      "Epoch [6/10], Step [32/625], Loss: 1.1478\n",
      "Epoch [6/10], Step [33/625], Loss: 1.6466\n",
      "Epoch [6/10], Step [34/625], Loss: 1.4063\n",
      "Epoch [6/10], Step [35/625], Loss: 1.1593\n",
      "Epoch [6/10], Step [36/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [37/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [38/625], Loss: 1.5822\n",
      "Epoch [6/10], Step [39/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [40/625], Loss: 1.8771\n",
      "Epoch [6/10], Step [41/625], Loss: 1.1694\n",
      "Epoch [6/10], Step [42/625], Loss: 1.4096\n",
      "Epoch [6/10], Step [43/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [44/625], Loss: 1.6424\n",
      "Epoch [6/10], Step [45/625], Loss: 1.1431\n",
      "Epoch [6/10], Step [46/625], Loss: 1.3357\n",
      "Epoch [6/10], Step [47/625], Loss: 1.6369\n",
      "Epoch [6/10], Step [48/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [49/625], Loss: 1.1575\n",
      "Epoch [6/10], Step [50/625], Loss: 0.9333\n",
      "Epoch [6/10], Step [51/625], Loss: 1.4068\n",
      "Epoch [6/10], Step [52/625], Loss: 1.1456\n",
      "Epoch [6/10], Step [53/625], Loss: 1.5362\n",
      "Epoch [6/10], Step [54/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [55/625], Loss: 1.1552\n",
      "Epoch [6/10], Step [56/625], Loss: 1.6376\n",
      "Epoch [6/10], Step [57/625], Loss: 1.3254\n",
      "Epoch [6/10], Step [58/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [59/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [60/625], Loss: 1.0622\n",
      "Epoch [6/10], Step [61/625], Loss: 1.1775\n",
      "Epoch [6/10], Step [62/625], Loss: 1.6401\n",
      "Epoch [6/10], Step [63/625], Loss: 1.3894\n",
      "Epoch [6/10], Step [64/625], Loss: 1.1550\n",
      "Epoch [6/10], Step [65/625], Loss: 1.3969\n",
      "Epoch [6/10], Step [66/625], Loss: 1.6554\n",
      "Epoch [6/10], Step [67/625], Loss: 1.3980\n",
      "Epoch [6/10], Step [68/625], Loss: 1.4044\n",
      "Epoch [6/10], Step [69/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [70/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [71/625], Loss: 1.4117\n",
      "Epoch [6/10], Step [72/625], Loss: 1.4915\n",
      "Epoch [6/10], Step [73/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [74/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [75/625], Loss: 1.3921\n",
      "Epoch [6/10], Step [76/625], Loss: 1.4019\n",
      "Epoch [6/10], Step [77/625], Loss: 1.4051\n",
      "Epoch [6/10], Step [78/625], Loss: 1.1722\n",
      "Epoch [6/10], Step [79/625], Loss: 1.1550\n",
      "Epoch [6/10], Step [80/625], Loss: 1.4500\n",
      "Epoch [6/10], Step [81/625], Loss: 1.4031\n",
      "Epoch [6/10], Step [82/625], Loss: 1.1730\n",
      "Epoch [6/10], Step [83/625], Loss: 0.9053\n",
      "Epoch [6/10], Step [84/625], Loss: 1.4010\n",
      "Epoch [6/10], Step [85/625], Loss: 1.3920\n",
      "Epoch [6/10], Step [86/625], Loss: 1.3911\n",
      "Epoch [6/10], Step [87/625], Loss: 1.6523\n",
      "Epoch [6/10], Step [88/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [89/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [90/625], Loss: 1.3172\n",
      "Epoch [6/10], Step [91/625], Loss: 1.6529\n",
      "Epoch [6/10], Step [92/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [93/625], Loss: 1.1543\n",
      "Epoch [6/10], Step [94/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [95/625], Loss: 1.1944\n",
      "Epoch [6/10], Step [96/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [97/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [98/625], Loss: 1.4039\n",
      "Epoch [6/10], Step [99/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [100/625], Loss: 1.1547\n",
      "Epoch [6/10], Step [101/625], Loss: 1.1740\n",
      "Epoch [6/10], Step [102/625], Loss: 1.3471\n",
      "Epoch [6/10], Step [103/625], Loss: 1.4568\n",
      "Epoch [6/10], Step [104/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [105/625], Loss: 1.6533\n",
      "Epoch [6/10], Step [106/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [107/625], Loss: 1.1522\n",
      "Epoch [6/10], Step [108/625], Loss: 1.6535\n",
      "Epoch [6/10], Step [109/625], Loss: 1.1630\n",
      "Epoch [6/10], Step [110/625], Loss: 1.1554\n",
      "Epoch [6/10], Step [111/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [112/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [113/625], Loss: 1.2916\n",
      "Epoch [6/10], Step [114/625], Loss: 1.1637\n",
      "Epoch [6/10], Step [115/625], Loss: 1.6396\n",
      "Epoch [6/10], Step [116/625], Loss: 1.6194\n",
      "Epoch [6/10], Step [117/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [118/625], Loss: 1.3398\n",
      "Epoch [6/10], Step [119/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [120/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [121/625], Loss: 1.0148\n",
      "Epoch [6/10], Step [122/625], Loss: 1.4076\n",
      "Epoch [6/10], Step [123/625], Loss: 1.1614\n",
      "Epoch [6/10], Step [124/625], Loss: 1.1172\n",
      "Epoch [6/10], Step [125/625], Loss: 1.1541\n",
      "Epoch [6/10], Step [126/625], Loss: 1.2155\n",
      "Epoch [6/10], Step [127/625], Loss: 1.6423\n",
      "Epoch [6/10], Step [128/625], Loss: 1.1491\n",
      "Epoch [6/10], Step [129/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [130/625], Loss: 1.1558\n",
      "Epoch [6/10], Step [131/625], Loss: 1.1553\n",
      "Epoch [6/10], Step [132/625], Loss: 1.1870\n",
      "Epoch [6/10], Step [133/625], Loss: 1.3436\n",
      "Epoch [6/10], Step [134/625], Loss: 1.0678\n",
      "Epoch [6/10], Step [135/625], Loss: 1.3994\n",
      "Epoch [6/10], Step [136/625], Loss: 1.1628\n",
      "Epoch [6/10], Step [137/625], Loss: 1.3817\n",
      "Epoch [6/10], Step [138/625], Loss: 1.1418\n",
      "Epoch [6/10], Step [139/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [140/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [141/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [142/625], Loss: 1.3428\n",
      "Epoch [6/10], Step [143/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [144/625], Loss: 1.1545\n",
      "Epoch [6/10], Step [145/625], Loss: 1.4053\n",
      "Epoch [6/10], Step [146/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [147/625], Loss: 1.6543\n",
      "Epoch [6/10], Step [148/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [149/625], Loss: 1.4045\n",
      "Epoch [6/10], Step [150/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [151/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [152/625], Loss: 1.2876\n",
      "Epoch [6/10], Step [153/625], Loss: 1.1519\n",
      "Epoch [6/10], Step [154/625], Loss: 1.6547\n",
      "Epoch [6/10], Step [155/625], Loss: 1.8599\n",
      "Epoch [6/10], Step [156/625], Loss: 1.3379\n",
      "Epoch [6/10], Step [157/625], Loss: 1.6020\n",
      "Epoch [6/10], Step [158/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [159/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [160/625], Loss: 1.3940\n",
      "Epoch [6/10], Step [161/625], Loss: 1.2434\n",
      "Epoch [6/10], Step [162/625], Loss: 1.6536\n",
      "Epoch [6/10], Step [163/625], Loss: 1.1547\n",
      "Epoch [6/10], Step [164/625], Loss: 1.6457\n",
      "Epoch [6/10], Step [165/625], Loss: 1.4836\n",
      "Epoch [6/10], Step [166/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [167/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [168/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [169/625], Loss: 1.4097\n",
      "Epoch [6/10], Step [170/625], Loss: 1.8232\n",
      "Epoch [6/10], Step [171/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [172/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [173/625], Loss: 0.9330\n",
      "Epoch [6/10], Step [174/625], Loss: 1.2793\n",
      "Epoch [6/10], Step [175/625], Loss: 1.3976\n",
      "Epoch [6/10], Step [176/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [177/625], Loss: 1.4787\n",
      "Epoch [6/10], Step [178/625], Loss: 1.0089\n",
      "Epoch [6/10], Step [179/625], Loss: 1.4063\n",
      "Epoch [6/10], Step [180/625], Loss: 1.8837\n",
      "Epoch [6/10], Step [181/625], Loss: 1.3974\n",
      "Epoch [6/10], Step [182/625], Loss: 1.4042\n",
      "Epoch [6/10], Step [183/625], Loss: 1.4052\n",
      "Epoch [6/10], Step [184/625], Loss: 1.1849\n",
      "Epoch [6/10], Step [185/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [186/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [187/625], Loss: 1.1377\n",
      "Epoch [6/10], Step [188/625], Loss: 1.3019\n",
      "Epoch [6/10], Step [189/625], Loss: 1.4084\n",
      "Epoch [6/10], Step [190/625], Loss: 1.5368\n",
      "Epoch [6/10], Step [191/625], Loss: 1.5526\n",
      "Epoch [6/10], Step [192/625], Loss: 1.3947\n",
      "Epoch [6/10], Step [193/625], Loss: 0.9669\n",
      "Epoch [6/10], Step [194/625], Loss: 1.3633\n",
      "Epoch [6/10], Step [195/625], Loss: 1.4618\n",
      "Epoch [6/10], Step [196/625], Loss: 1.1483\n",
      "Epoch [6/10], Step [197/625], Loss: 1.1551\n",
      "Epoch [6/10], Step [198/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [199/625], Loss: 1.1388\n",
      "Epoch [6/10], Step [200/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [201/625], Loss: 1.6367\n",
      "Epoch [6/10], Step [202/625], Loss: 1.1530\n",
      "Epoch [6/10], Step [203/625], Loss: 1.1544\n",
      "Epoch [6/10], Step [204/625], Loss: 1.3893\n",
      "Epoch [6/10], Step [205/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [206/625], Loss: 1.1484\n",
      "Epoch [6/10], Step [207/625], Loss: 1.4041\n",
      "Epoch [6/10], Step [208/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [209/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [210/625], Loss: 1.3951\n",
      "Epoch [6/10], Step [211/625], Loss: 1.6420\n",
      "Epoch [6/10], Step [212/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [213/625], Loss: 0.9065\n",
      "Epoch [6/10], Step [214/625], Loss: 1.4079\n",
      "Epoch [6/10], Step [215/625], Loss: 1.6385\n",
      "Epoch [6/10], Step [216/625], Loss: 1.3432\n",
      "Epoch [6/10], Step [217/625], Loss: 1.3751\n",
      "Epoch [6/10], Step [218/625], Loss: 1.2430\n",
      "Epoch [6/10], Step [219/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [220/625], Loss: 1.6532\n",
      "Epoch [6/10], Step [221/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [222/625], Loss: 1.6369\n",
      "Epoch [6/10], Step [223/625], Loss: 1.4056\n",
      "Epoch [6/10], Step [224/625], Loss: 1.5186\n",
      "Epoch [6/10], Step [225/625], Loss: 1.6380\n",
      "Epoch [6/10], Step [226/625], Loss: 1.1505\n",
      "Epoch [6/10], Step [227/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [228/625], Loss: 1.1455\n",
      "Epoch [6/10], Step [229/625], Loss: 1.4043\n",
      "Epoch [6/10], Step [230/625], Loss: 1.3627\n",
      "Epoch [6/10], Step [231/625], Loss: 1.3960\n",
      "Epoch [6/10], Step [232/625], Loss: 1.6256\n",
      "Epoch [6/10], Step [233/625], Loss: 1.8280\n",
      "Epoch [6/10], Step [234/625], Loss: 1.3229\n",
      "Epoch [6/10], Step [235/625], Loss: 1.1465\n",
      "Epoch [6/10], Step [236/625], Loss: 1.9045\n",
      "Epoch [6/10], Step [237/625], Loss: 1.6376\n",
      "Epoch [6/10], Step [238/625], Loss: 1.1578\n",
      "Epoch [6/10], Step [239/625], Loss: 1.5583\n",
      "Epoch [6/10], Step [240/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [241/625], Loss: 0.9630\n",
      "Epoch [6/10], Step [242/625], Loss: 1.6005\n",
      "Epoch [6/10], Step [243/625], Loss: 1.7868\n",
      "Epoch [6/10], Step [244/625], Loss: 1.3931\n",
      "Epoch [6/10], Step [245/625], Loss: 1.4019\n",
      "Epoch [6/10], Step [246/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [247/625], Loss: 1.4052\n",
      "Epoch [6/10], Step [248/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [249/625], Loss: 1.3358\n",
      "Epoch [6/10], Step [250/625], Loss: 1.6514\n",
      "Epoch [6/10], Step [251/625], Loss: 1.3975\n",
      "Epoch [6/10], Step [252/625], Loss: 1.8945\n",
      "Epoch [6/10], Step [253/625], Loss: 1.4778\n",
      "Epoch [6/10], Step [254/625], Loss: 1.6577\n",
      "Epoch [6/10], Step [255/625], Loss: 1.4050\n",
      "Epoch [6/10], Step [256/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [257/625], Loss: 1.1538\n",
      "Epoch [6/10], Step [258/625], Loss: 1.9013\n",
      "Epoch [6/10], Step [259/625], Loss: 1.6448\n",
      "Epoch [6/10], Step [260/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [261/625], Loss: 1.1570\n",
      "Epoch [6/10], Step [262/625], Loss: 1.6079\n",
      "Epoch [6/10], Step [263/625], Loss: 1.1535\n",
      "Epoch [6/10], Step [264/625], Loss: 1.2606\n",
      "Epoch [6/10], Step [265/625], Loss: 0.9178\n",
      "Epoch [6/10], Step [266/625], Loss: 1.1410\n",
      "Epoch [6/10], Step [267/625], Loss: 1.4049\n",
      "Epoch [6/10], Step [268/625], Loss: 1.4050\n",
      "Epoch [6/10], Step [269/625], Loss: 1.3908\n",
      "Epoch [6/10], Step [270/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [271/625], Loss: 1.4042\n",
      "Epoch [6/10], Step [272/625], Loss: 1.6547\n",
      "Epoch [6/10], Step [273/625], Loss: 1.6411\n",
      "Epoch [6/10], Step [274/625], Loss: 1.4479\n",
      "Epoch [6/10], Step [275/625], Loss: 1.5611\n",
      "Epoch [6/10], Step [276/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [277/625], Loss: 1.8966\n",
      "Epoch [6/10], Step [278/625], Loss: 1.6437\n",
      "Epoch [6/10], Step [279/625], Loss: 1.4661\n",
      "Epoch [6/10], Step [280/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [281/625], Loss: 0.9467\n",
      "Epoch [6/10], Step [282/625], Loss: 1.0628\n",
      "Epoch [6/10], Step [283/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [284/625], Loss: 1.4044\n",
      "Epoch [6/10], Step [285/625], Loss: 1.3959\n",
      "Epoch [6/10], Step [286/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [287/625], Loss: 1.6426\n",
      "Epoch [6/10], Step [288/625], Loss: 1.0920\n",
      "Epoch [6/10], Step [289/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [290/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [291/625], Loss: 1.1618\n",
      "Epoch [6/10], Step [292/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [293/625], Loss: 1.4931\n",
      "Epoch [6/10], Step [294/625], Loss: 1.4057\n",
      "Epoch [6/10], Step [295/625], Loss: 1.1834\n",
      "Epoch [6/10], Step [296/625], Loss: 1.1568\n",
      "Epoch [6/10], Step [297/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [298/625], Loss: 1.6546\n",
      "Epoch [6/10], Step [299/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [300/625], Loss: 1.4132\n",
      "Epoch [6/10], Step [301/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [302/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [303/625], Loss: 1.2009\n",
      "Epoch [6/10], Step [304/625], Loss: 1.1900\n",
      "Epoch [6/10], Step [305/625], Loss: 0.9192\n",
      "Epoch [6/10], Step [306/625], Loss: 1.3701\n",
      "Epoch [6/10], Step [307/625], Loss: 1.1476\n",
      "Epoch [6/10], Step [308/625], Loss: 1.3428\n",
      "Epoch [6/10], Step [309/625], Loss: 1.4353\n",
      "Epoch [6/10], Step [310/625], Loss: 1.1648\n",
      "Epoch [6/10], Step [311/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [312/625], Loss: 1.4016\n",
      "Epoch [6/10], Step [313/625], Loss: 1.4057\n",
      "Epoch [6/10], Step [314/625], Loss: 1.6338\n",
      "Epoch [6/10], Step [315/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [316/625], Loss: 0.9256\n",
      "Epoch [6/10], Step [317/625], Loss: 1.3893\n",
      "Epoch [6/10], Step [318/625], Loss: 1.1546\n",
      "Epoch [6/10], Step [319/625], Loss: 1.3985\n",
      "Epoch [6/10], Step [320/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [321/625], Loss: 1.4042\n",
      "Epoch [6/10], Step [322/625], Loss: 1.4042\n",
      "Epoch [6/10], Step [323/625], Loss: 1.6600\n",
      "Epoch [6/10], Step [324/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [325/625], Loss: 1.1550\n",
      "Epoch [6/10], Step [326/625], Loss: 1.3897\n",
      "Epoch [6/10], Step [327/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [328/625], Loss: 1.4176\n",
      "Epoch [6/10], Step [329/625], Loss: 0.9051\n",
      "Epoch [6/10], Step [330/625], Loss: 1.4138\n",
      "Epoch [6/10], Step [331/625], Loss: 1.1494\n",
      "Epoch [6/10], Step [332/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [333/625], Loss: 1.4490\n",
      "Epoch [6/10], Step [334/625], Loss: 1.2145\n",
      "Epoch [6/10], Step [335/625], Loss: 1.4167\n",
      "Epoch [6/10], Step [336/625], Loss: 1.2299\n",
      "Epoch [6/10], Step [337/625], Loss: 1.1387\n",
      "Epoch [6/10], Step [338/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [339/625], Loss: 1.3796\n",
      "Epoch [6/10], Step [340/625], Loss: 1.0863\n",
      "Epoch [6/10], Step [341/625], Loss: 1.6410\n",
      "Epoch [6/10], Step [342/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [343/625], Loss: 0.9061\n",
      "Epoch [6/10], Step [344/625], Loss: 0.9073\n",
      "Epoch [6/10], Step [345/625], Loss: 0.9126\n",
      "Epoch [6/10], Step [346/625], Loss: 1.2741\n",
      "Epoch [6/10], Step [347/625], Loss: 1.6174\n",
      "Epoch [6/10], Step [348/625], Loss: 1.4049\n",
      "Epoch [6/10], Step [349/625], Loss: 1.6455\n",
      "Epoch [6/10], Step [350/625], Loss: 1.1555\n",
      "Epoch [6/10], Step [351/625], Loss: 1.2539\n",
      "Epoch [6/10], Step [352/625], Loss: 1.3977\n",
      "Epoch [6/10], Step [353/625], Loss: 1.3987\n",
      "Epoch [6/10], Step [354/625], Loss: 1.1545\n",
      "Epoch [6/10], Step [355/625], Loss: 1.3985\n",
      "Epoch [6/10], Step [356/625], Loss: 1.3849\n",
      "Epoch [6/10], Step [357/625], Loss: 1.3277\n",
      "Epoch [6/10], Step [358/625], Loss: 1.2091\n",
      "Epoch [6/10], Step [359/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [360/625], Loss: 1.5639\n",
      "Epoch [6/10], Step [361/625], Loss: 1.1667\n",
      "Epoch [6/10], Step [362/625], Loss: 0.9206\n",
      "Epoch [6/10], Step [363/625], Loss: 1.2112\n",
      "Epoch [6/10], Step [364/625], Loss: 1.6209\n",
      "Epoch [6/10], Step [365/625], Loss: 1.6867\n",
      "Epoch [6/10], Step [366/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [367/625], Loss: 1.1507\n",
      "Epoch [6/10], Step [368/625], Loss: 0.9447\n",
      "Epoch [6/10], Step [369/625], Loss: 1.3865\n",
      "Epoch [6/10], Step [370/625], Loss: 1.2756\n",
      "Epoch [6/10], Step [371/625], Loss: 1.1545\n",
      "Epoch [6/10], Step [372/625], Loss: 1.1819\n",
      "Epoch [6/10], Step [373/625], Loss: 1.1552\n",
      "Epoch [6/10], Step [374/625], Loss: 1.6498\n",
      "Epoch [6/10], Step [375/625], Loss: 0.9236\n",
      "Epoch [6/10], Step [376/625], Loss: 1.3886\n",
      "Epoch [6/10], Step [377/625], Loss: 1.4041\n",
      "Epoch [6/10], Step [378/625], Loss: 1.4033\n",
      "Epoch [6/10], Step [379/625], Loss: 1.4277\n",
      "Epoch [6/10], Step [380/625], Loss: 1.1557\n",
      "Epoch [6/10], Step [381/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [382/625], Loss: 0.9833\n",
      "Epoch [6/10], Step [383/625], Loss: 0.9079\n",
      "Epoch [6/10], Step [384/625], Loss: 1.4373\n",
      "Epoch [6/10], Step [385/625], Loss: 1.4023\n",
      "Epoch [6/10], Step [386/625], Loss: 1.0136\n",
      "Epoch [6/10], Step [387/625], Loss: 0.9052\n",
      "Epoch [6/10], Step [388/625], Loss: 0.9076\n",
      "Epoch [6/10], Step [389/625], Loss: 1.1331\n",
      "Epoch [6/10], Step [390/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [391/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [392/625], Loss: 1.1718\n",
      "Epoch [6/10], Step [393/625], Loss: 1.5506\n",
      "Epoch [6/10], Step [394/625], Loss: 1.6369\n",
      "Epoch [6/10], Step [395/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [396/625], Loss: 1.3835\n",
      "Epoch [6/10], Step [397/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [398/625], Loss: 1.6513\n",
      "Epoch [6/10], Step [399/625], Loss: 1.1541\n",
      "Epoch [6/10], Step [400/625], Loss: 1.3571\n",
      "Epoch [6/10], Step [401/625], Loss: 0.9052\n",
      "Epoch [6/10], Step [402/625], Loss: 1.1261\n",
      "Epoch [6/10], Step [403/625], Loss: 1.6382\n",
      "Epoch [6/10], Step [404/625], Loss: 1.4025\n",
      "Epoch [6/10], Step [405/625], Loss: 1.3977\n",
      "Epoch [6/10], Step [406/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [407/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [408/625], Loss: 1.3551\n",
      "Epoch [6/10], Step [409/625], Loss: 1.6474\n",
      "Epoch [6/10], Step [410/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [411/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [412/625], Loss: 1.4049\n",
      "Epoch [6/10], Step [413/625], Loss: 1.2151\n",
      "Epoch [6/10], Step [414/625], Loss: 1.1929\n",
      "Epoch [6/10], Step [415/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [416/625], Loss: 1.2077\n",
      "Epoch [6/10], Step [417/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [418/625], Loss: 1.1415\n",
      "Epoch [6/10], Step [419/625], Loss: 1.1468\n",
      "Epoch [6/10], Step [420/625], Loss: 0.9123\n",
      "Epoch [6/10], Step [421/625], Loss: 1.3113\n",
      "Epoch [6/10], Step [422/625], Loss: 1.6412\n",
      "Epoch [6/10], Step [423/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [424/625], Loss: 1.1547\n",
      "Epoch [6/10], Step [425/625], Loss: 1.6358\n",
      "Epoch [6/10], Step [426/625], Loss: 1.1863\n",
      "Epoch [6/10], Step [427/625], Loss: 1.6272\n",
      "Epoch [6/10], Step [428/625], Loss: 1.4029\n",
      "Epoch [6/10], Step [429/625], Loss: 1.3007\n",
      "Epoch [6/10], Step [430/625], Loss: 1.1904\n",
      "Epoch [6/10], Step [431/625], Loss: 1.4805\n",
      "Epoch [6/10], Step [432/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [433/625], Loss: 1.1567\n",
      "Epoch [6/10], Step [434/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [435/625], Loss: 1.1544\n",
      "Epoch [6/10], Step [436/625], Loss: 1.8646\n",
      "Epoch [6/10], Step [437/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [438/625], Loss: 1.6397\n",
      "Epoch [6/10], Step [439/625], Loss: 1.2396\n",
      "Epoch [6/10], Step [440/625], Loss: 1.0198\n",
      "Epoch [6/10], Step [441/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [442/625], Loss: 1.4041\n",
      "Epoch [6/10], Step [443/625], Loss: 0.9176\n",
      "Epoch [6/10], Step [444/625], Loss: 1.1474\n",
      "Epoch [6/10], Step [445/625], Loss: 0.9135\n",
      "Epoch [6/10], Step [446/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [447/625], Loss: 1.6404\n",
      "Epoch [6/10], Step [448/625], Loss: 1.1610\n",
      "Epoch [6/10], Step [449/625], Loss: 1.1390\n",
      "Epoch [6/10], Step [450/625], Loss: 1.2521\n",
      "Epoch [6/10], Step [451/625], Loss: 1.1418\n",
      "Epoch [6/10], Step [452/625], Loss: 1.1452\n",
      "Epoch [6/10], Step [453/625], Loss: 1.6547\n",
      "Epoch [6/10], Step [454/625], Loss: 0.9050\n",
      "Epoch [6/10], Step [455/625], Loss: 1.4055\n",
      "Epoch [6/10], Step [456/625], Loss: 1.6831\n",
      "Epoch [6/10], Step [457/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [458/625], Loss: 1.1587\n",
      "Epoch [6/10], Step [459/625], Loss: 1.1570\n",
      "Epoch [6/10], Step [460/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [461/625], Loss: 1.3887\n",
      "Epoch [6/10], Step [462/625], Loss: 1.6120\n",
      "Epoch [6/10], Step [463/625], Loss: 1.3554\n",
      "Epoch [6/10], Step [464/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [465/625], Loss: 1.0837\n",
      "Epoch [6/10], Step [466/625], Loss: 1.1563\n",
      "Epoch [6/10], Step [467/625], Loss: 1.2324\n",
      "Epoch [6/10], Step [468/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [469/625], Loss: 1.4148\n",
      "Epoch [6/10], Step [470/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [471/625], Loss: 1.6534\n",
      "Epoch [6/10], Step [472/625], Loss: 1.1623\n",
      "Epoch [6/10], Step [473/625], Loss: 0.9093\n",
      "Epoch [6/10], Step [474/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [475/625], Loss: 1.4052\n",
      "Epoch [6/10], Step [476/625], Loss: 1.1555\n",
      "Epoch [6/10], Step [477/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [478/625], Loss: 1.5678\n",
      "Epoch [6/10], Step [479/625], Loss: 1.0232\n",
      "Epoch [6/10], Step [480/625], Loss: 1.4071\n",
      "Epoch [6/10], Step [481/625], Loss: 1.6372\n",
      "Epoch [6/10], Step [482/625], Loss: 1.2312\n",
      "Epoch [6/10], Step [483/625], Loss: 0.9252\n",
      "Epoch [6/10], Step [484/625], Loss: 1.2116\n",
      "Epoch [6/10], Step [485/625], Loss: 1.3948\n",
      "Epoch [6/10], Step [486/625], Loss: 1.1962\n",
      "Epoch [6/10], Step [487/625], Loss: 1.8021\n",
      "Epoch [6/10], Step [488/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [489/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [490/625], Loss: 1.1550\n",
      "Epoch [6/10], Step [491/625], Loss: 1.6569\n",
      "Epoch [6/10], Step [492/625], Loss: 1.4021\n",
      "Epoch [6/10], Step [493/625], Loss: 1.6578\n",
      "Epoch [6/10], Step [494/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [495/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [496/625], Loss: 1.1544\n",
      "Epoch [6/10], Step [497/625], Loss: 1.4908\n",
      "Epoch [6/10], Step [498/625], Loss: 1.4054\n",
      "Epoch [6/10], Step [499/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [500/625], Loss: 1.1472\n",
      "Epoch [6/10], Step [501/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [502/625], Loss: 1.1562\n",
      "Epoch [6/10], Step [503/625], Loss: 1.2971\n",
      "Epoch [6/10], Step [504/625], Loss: 1.4053\n",
      "Epoch [6/10], Step [505/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [506/625], Loss: 1.1550\n",
      "Epoch [6/10], Step [507/625], Loss: 1.4132\n",
      "Epoch [6/10], Step [508/625], Loss: 1.2252\n",
      "Epoch [6/10], Step [509/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [510/625], Loss: 1.1683\n",
      "Epoch [6/10], Step [511/625], Loss: 1.6543\n",
      "Epoch [6/10], Step [512/625], Loss: 1.8654\n",
      "Epoch [6/10], Step [513/625], Loss: 1.4067\n",
      "Epoch [6/10], Step [514/625], Loss: 0.9051\n",
      "Epoch [6/10], Step [515/625], Loss: 1.4468\n",
      "Epoch [6/10], Step [516/625], Loss: 1.2252\n",
      "Epoch [6/10], Step [517/625], Loss: 1.6427\n",
      "Epoch [6/10], Step [518/625], Loss: 1.1544\n",
      "Epoch [6/10], Step [519/625], Loss: 1.8928\n",
      "Epoch [6/10], Step [520/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [521/625], Loss: 1.1554\n",
      "Epoch [6/10], Step [522/625], Loss: 1.6360\n",
      "Epoch [6/10], Step [523/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [524/625], Loss: 1.4282\n",
      "Epoch [6/10], Step [525/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [526/625], Loss: 1.6258\n",
      "Epoch [6/10], Step [527/625], Loss: 1.5909\n",
      "Epoch [6/10], Step [528/625], Loss: 1.6393\n",
      "Epoch [6/10], Step [529/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [530/625], Loss: 1.1419\n",
      "Epoch [6/10], Step [531/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [532/625], Loss: 1.4101\n",
      "Epoch [6/10], Step [533/625], Loss: 1.6419\n",
      "Epoch [6/10], Step [534/625], Loss: 1.1780\n",
      "Epoch [6/10], Step [535/625], Loss: 0.9049\n",
      "Epoch [6/10], Step [536/625], Loss: 1.6222\n",
      "Epoch [6/10], Step [537/625], Loss: 0.9273\n",
      "Epoch [6/10], Step [538/625], Loss: 1.1649\n",
      "Epoch [6/10], Step [539/625], Loss: 1.3904\n",
      "Epoch [6/10], Step [540/625], Loss: 1.2130\n",
      "Epoch [6/10], Step [541/625], Loss: 1.8678\n",
      "Epoch [6/10], Step [542/625], Loss: 1.6517\n",
      "Epoch [6/10], Step [543/625], Loss: 1.4064\n",
      "Epoch [6/10], Step [544/625], Loss: 1.1561\n",
      "Epoch [6/10], Step [545/625], Loss: 1.4037\n",
      "Epoch [6/10], Step [546/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [547/625], Loss: 0.9150\n",
      "Epoch [6/10], Step [548/625], Loss: 1.4045\n",
      "Epoch [6/10], Step [549/625], Loss: 1.6337\n",
      "Epoch [6/10], Step [550/625], Loss: 1.9022\n",
      "Epoch [6/10], Step [551/625], Loss: 1.5701\n",
      "Epoch [6/10], Step [552/625], Loss: 0.9154\n",
      "Epoch [6/10], Step [553/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [554/625], Loss: 1.4310\n",
      "Epoch [6/10], Step [555/625], Loss: 1.6320\n",
      "Epoch [6/10], Step [556/625], Loss: 1.4055\n",
      "Epoch [6/10], Step [557/625], Loss: 1.1507\n",
      "Epoch [6/10], Step [558/625], Loss: 1.0756\n",
      "Epoch [6/10], Step [559/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [560/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [561/625], Loss: 1.4047\n",
      "Epoch [6/10], Step [562/625], Loss: 1.2535\n",
      "Epoch [6/10], Step [563/625], Loss: 1.3397\n",
      "Epoch [6/10], Step [564/625], Loss: 1.1979\n",
      "Epoch [6/10], Step [565/625], Loss: 1.1553\n",
      "Epoch [6/10], Step [566/625], Loss: 1.1438\n",
      "Epoch [6/10], Step [567/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [568/625], Loss: 1.4086\n",
      "Epoch [6/10], Step [569/625], Loss: 1.4114\n",
      "Epoch [6/10], Step [570/625], Loss: 1.4007\n",
      "Epoch [6/10], Step [571/625], Loss: 1.4051\n",
      "Epoch [6/10], Step [572/625], Loss: 1.7042\n",
      "Epoch [6/10], Step [573/625], Loss: 1.7626\n",
      "Epoch [6/10], Step [574/625], Loss: 1.4015\n",
      "Epoch [6/10], Step [575/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [576/625], Loss: 0.9048\n",
      "Epoch [6/10], Step [577/625], Loss: 1.6336\n",
      "Epoch [6/10], Step [578/625], Loss: 1.3994\n",
      "Epoch [6/10], Step [579/625], Loss: 1.4033\n",
      "Epoch [6/10], Step [580/625], Loss: 1.4783\n",
      "Epoch [6/10], Step [581/625], Loss: 1.2393\n",
      "Epoch [6/10], Step [582/625], Loss: 1.4326\n",
      "Epoch [6/10], Step [583/625], Loss: 1.4301\n",
      "Epoch [6/10], Step [584/625], Loss: 1.1423\n",
      "Epoch [6/10], Step [585/625], Loss: 1.1549\n",
      "Epoch [6/10], Step [586/625], Loss: 1.4357\n",
      "Epoch [6/10], Step [587/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [588/625], Loss: 1.6326\n",
      "Epoch [6/10], Step [589/625], Loss: 1.0811\n",
      "Epoch [6/10], Step [590/625], Loss: 1.6378\n",
      "Epoch [6/10], Step [591/625], Loss: 1.4022\n",
      "Epoch [6/10], Step [592/625], Loss: 1.6296\n",
      "Epoch [6/10], Step [593/625], Loss: 0.9052\n",
      "Epoch [6/10], Step [594/625], Loss: 1.6548\n",
      "Epoch [6/10], Step [595/625], Loss: 1.1548\n",
      "Epoch [6/10], Step [596/625], Loss: 1.3984\n",
      "Epoch [6/10], Step [597/625], Loss: 1.3502\n",
      "Epoch [6/10], Step [598/625], Loss: 1.4090\n",
      "Epoch [6/10], Step [599/625], Loss: 1.4011\n",
      "Epoch [6/10], Step [600/625], Loss: 1.4043\n",
      "Epoch [6/10], Step [601/625], Loss: 0.9212\n",
      "Epoch [6/10], Step [602/625], Loss: 1.4059\n",
      "Epoch [6/10], Step [603/625], Loss: 1.4002\n",
      "Epoch [6/10], Step [604/625], Loss: 1.6389\n",
      "Epoch [6/10], Step [605/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [606/625], Loss: 0.9093\n",
      "Epoch [6/10], Step [607/625], Loss: 0.9085\n",
      "Epoch [6/10], Step [608/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [609/625], Loss: 1.6278\n",
      "Epoch [6/10], Step [610/625], Loss: 0.9307\n",
      "Epoch [6/10], Step [611/625], Loss: 1.4046\n",
      "Epoch [6/10], Step [612/625], Loss: 1.5868\n",
      "Epoch [6/10], Step [613/625], Loss: 1.6350\n",
      "Epoch [6/10], Step [614/625], Loss: 1.4893\n",
      "Epoch [6/10], Step [615/625], Loss: 1.1733\n",
      "Epoch [6/10], Step [616/625], Loss: 1.1544\n",
      "Epoch [6/10], Step [617/625], Loss: 0.9115\n",
      "Epoch [6/10], Step [618/625], Loss: 1.4048\n",
      "Epoch [6/10], Step [619/625], Loss: 0.9347\n",
      "Epoch [6/10], Step [620/625], Loss: 1.6239\n",
      "Epoch [6/10], Step [621/625], Loss: 1.3956\n",
      "Epoch [6/10], Step [622/625], Loss: 1.1012\n",
      "Epoch [6/10], Step [623/625], Loss: 1.1556\n",
      "Epoch [6/10], Step [624/625], Loss: 1.3725\n",
      "Epoch [6/10], Step [625/625], Loss: 1.0292\n",
      "Epoch [7/10], Step [1/625], Loss: 1.6412\n",
      "Epoch [7/10], Step [2/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [3/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [4/625], Loss: 1.8240\n",
      "Epoch [7/10], Step [5/625], Loss: 1.3939\n",
      "Epoch [7/10], Step [6/625], Loss: 1.1919\n",
      "Epoch [7/10], Step [7/625], Loss: 1.2996\n",
      "Epoch [7/10], Step [8/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [9/625], Loss: 1.4627\n",
      "Epoch [7/10], Step [10/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [11/625], Loss: 1.1544\n",
      "Epoch [7/10], Step [12/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [13/625], Loss: 1.2803\n",
      "Epoch [7/10], Step [14/625], Loss: 1.4052\n",
      "Epoch [7/10], Step [15/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [16/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [17/625], Loss: 1.4106\n",
      "Epoch [7/10], Step [18/625], Loss: 0.9052\n",
      "Epoch [7/10], Step [19/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [20/625], Loss: 1.1970\n",
      "Epoch [7/10], Step [21/625], Loss: 1.3887\n",
      "Epoch [7/10], Step [22/625], Loss: 1.6495\n",
      "Epoch [7/10], Step [23/625], Loss: 1.3916\n",
      "Epoch [7/10], Step [24/625], Loss: 1.4100\n",
      "Epoch [7/10], Step [25/625], Loss: 1.7724\n",
      "Epoch [7/10], Step [26/625], Loss: 1.1596\n",
      "Epoch [7/10], Step [27/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [28/625], Loss: 1.6059\n",
      "Epoch [7/10], Step [29/625], Loss: 1.1666\n",
      "Epoch [7/10], Step [30/625], Loss: 1.3713\n",
      "Epoch [7/10], Step [31/625], Loss: 1.1498\n",
      "Epoch [7/10], Step [32/625], Loss: 1.4063\n",
      "Epoch [7/10], Step [33/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [34/625], Loss: 1.4034\n",
      "Epoch [7/10], Step [35/625], Loss: 1.1448\n",
      "Epoch [7/10], Step [36/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [37/625], Loss: 0.9453\n",
      "Epoch [7/10], Step [38/625], Loss: 1.4034\n",
      "Epoch [7/10], Step [39/625], Loss: 1.6531\n",
      "Epoch [7/10], Step [40/625], Loss: 1.1491\n",
      "Epoch [7/10], Step [41/625], Loss: 1.4020\n",
      "Epoch [7/10], Step [42/625], Loss: 1.6065\n",
      "Epoch [7/10], Step [43/625], Loss: 1.2395\n",
      "Epoch [7/10], Step [44/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [45/625], Loss: 1.1676\n",
      "Epoch [7/10], Step [46/625], Loss: 1.4020\n",
      "Epoch [7/10], Step [47/625], Loss: 1.3914\n",
      "Epoch [7/10], Step [48/625], Loss: 0.9862\n",
      "Epoch [7/10], Step [49/625], Loss: 1.8817\n",
      "Epoch [7/10], Step [50/625], Loss: 1.1211\n",
      "Epoch [7/10], Step [51/625], Loss: 1.6485\n",
      "Epoch [7/10], Step [52/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [53/625], Loss: 1.4126\n",
      "Epoch [7/10], Step [54/625], Loss: 1.2315\n",
      "Epoch [7/10], Step [55/625], Loss: 1.6533\n",
      "Epoch [7/10], Step [56/625], Loss: 1.6492\n",
      "Epoch [7/10], Step [57/625], Loss: 1.4135\n",
      "Epoch [7/10], Step [58/625], Loss: 0.9852\n",
      "Epoch [7/10], Step [59/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [60/625], Loss: 1.1730\n",
      "Epoch [7/10], Step [61/625], Loss: 1.1926\n",
      "Epoch [7/10], Step [62/625], Loss: 1.6231\n",
      "Epoch [7/10], Step [63/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [64/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [65/625], Loss: 1.3185\n",
      "Epoch [7/10], Step [66/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [67/625], Loss: 1.1045\n",
      "Epoch [7/10], Step [68/625], Loss: 1.6556\n",
      "Epoch [7/10], Step [69/625], Loss: 1.4039\n",
      "Epoch [7/10], Step [70/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [71/625], Loss: 1.4052\n",
      "Epoch [7/10], Step [72/625], Loss: 1.4585\n",
      "Epoch [7/10], Step [73/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [74/625], Loss: 0.9084\n",
      "Epoch [7/10], Step [75/625], Loss: 1.1533\n",
      "Epoch [7/10], Step [76/625], Loss: 1.1517\n",
      "Epoch [7/10], Step [77/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [78/625], Loss: 1.4111\n",
      "Epoch [7/10], Step [79/625], Loss: 1.6361\n",
      "Epoch [7/10], Step [80/625], Loss: 1.6404\n",
      "Epoch [7/10], Step [81/625], Loss: 1.3857\n",
      "Epoch [7/10], Step [82/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [83/625], Loss: 1.6272\n",
      "Epoch [7/10], Step [84/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [85/625], Loss: 1.6427\n",
      "Epoch [7/10], Step [86/625], Loss: 1.6406\n",
      "Epoch [7/10], Step [87/625], Loss: 1.1553\n",
      "Epoch [7/10], Step [88/625], Loss: 1.3739\n",
      "Epoch [7/10], Step [89/625], Loss: 1.1551\n",
      "Epoch [7/10], Step [90/625], Loss: 0.9763\n",
      "Epoch [7/10], Step [91/625], Loss: 1.2726\n",
      "Epoch [7/10], Step [92/625], Loss: 1.3913\n",
      "Epoch [7/10], Step [93/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [94/625], Loss: 1.1424\n",
      "Epoch [7/10], Step [95/625], Loss: 1.4011\n",
      "Epoch [7/10], Step [96/625], Loss: 1.1351\n",
      "Epoch [7/10], Step [97/625], Loss: 1.1452\n",
      "Epoch [7/10], Step [98/625], Loss: 1.6199\n",
      "Epoch [7/10], Step [99/625], Loss: 1.6387\n",
      "Epoch [7/10], Step [100/625], Loss: 1.1545\n",
      "Epoch [7/10], Step [101/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [102/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [103/625], Loss: 1.5856\n",
      "Epoch [7/10], Step [104/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [105/625], Loss: 0.9205\n",
      "Epoch [7/10], Step [106/625], Loss: 1.2187\n",
      "Epoch [7/10], Step [107/625], Loss: 1.9040\n",
      "Epoch [7/10], Step [108/625], Loss: 1.4053\n",
      "Epoch [7/10], Step [109/625], Loss: 1.3333\n",
      "Epoch [7/10], Step [110/625], Loss: 1.0103\n",
      "Epoch [7/10], Step [111/625], Loss: 1.2058\n",
      "Epoch [7/10], Step [112/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [113/625], Loss: 1.1614\n",
      "Epoch [7/10], Step [114/625], Loss: 1.2701\n",
      "Epoch [7/10], Step [115/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [116/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [117/625], Loss: 1.4027\n",
      "Epoch [7/10], Step [118/625], Loss: 1.3932\n",
      "Epoch [7/10], Step [119/625], Loss: 1.6540\n",
      "Epoch [7/10], Step [120/625], Loss: 1.8989\n",
      "Epoch [7/10], Step [121/625], Loss: 1.4151\n",
      "Epoch [7/10], Step [122/625], Loss: 1.0825\n",
      "Epoch [7/10], Step [123/625], Loss: 1.3962\n",
      "Epoch [7/10], Step [124/625], Loss: 1.6457\n",
      "Epoch [7/10], Step [125/625], Loss: 1.2628\n",
      "Epoch [7/10], Step [126/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [127/625], Loss: 1.2134\n",
      "Epoch [7/10], Step [128/625], Loss: 0.9237\n",
      "Epoch [7/10], Step [129/625], Loss: 1.6455\n",
      "Epoch [7/10], Step [130/625], Loss: 1.3766\n",
      "Epoch [7/10], Step [131/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [132/625], Loss: 1.1510\n",
      "Epoch [7/10], Step [133/625], Loss: 1.1700\n",
      "Epoch [7/10], Step [134/625], Loss: 1.1562\n",
      "Epoch [7/10], Step [135/625], Loss: 1.3890\n",
      "Epoch [7/10], Step [136/625], Loss: 1.1339\n",
      "Epoch [7/10], Step [137/625], Loss: 1.4598\n",
      "Epoch [7/10], Step [138/625], Loss: 1.6396\n",
      "Epoch [7/10], Step [139/625], Loss: 1.9048\n",
      "Epoch [7/10], Step [140/625], Loss: 1.2441\n",
      "Epoch [7/10], Step [141/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [142/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [143/625], Loss: 1.1537\n",
      "Epoch [7/10], Step [144/625], Loss: 1.5889\n",
      "Epoch [7/10], Step [145/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [146/625], Loss: 1.6541\n",
      "Epoch [7/10], Step [147/625], Loss: 1.7020\n",
      "Epoch [7/10], Step [148/625], Loss: 1.3569\n",
      "Epoch [7/10], Step [149/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [150/625], Loss: 1.3976\n",
      "Epoch [7/10], Step [151/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [152/625], Loss: 1.6374\n",
      "Epoch [7/10], Step [153/625], Loss: 1.3967\n",
      "Epoch [7/10], Step [154/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [155/625], Loss: 1.1554\n",
      "Epoch [7/10], Step [156/625], Loss: 1.4057\n",
      "Epoch [7/10], Step [157/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [158/625], Loss: 1.4188\n",
      "Epoch [7/10], Step [159/625], Loss: 1.4043\n",
      "Epoch [7/10], Step [160/625], Loss: 1.3996\n",
      "Epoch [7/10], Step [161/625], Loss: 0.9055\n",
      "Epoch [7/10], Step [162/625], Loss: 1.5195\n",
      "Epoch [7/10], Step [163/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [164/625], Loss: 1.4677\n",
      "Epoch [7/10], Step [165/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [166/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [167/625], Loss: 1.1584\n",
      "Epoch [7/10], Step [168/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [169/625], Loss: 1.1523\n",
      "Epoch [7/10], Step [170/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [171/625], Loss: 1.3767\n",
      "Epoch [7/10], Step [172/625], Loss: 1.5721\n",
      "Epoch [7/10], Step [173/625], Loss: 1.0172\n",
      "Epoch [7/10], Step [174/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [175/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [176/625], Loss: 1.1428\n",
      "Epoch [7/10], Step [177/625], Loss: 1.5796\n",
      "Epoch [7/10], Step [178/625], Loss: 1.6070\n",
      "Epoch [7/10], Step [179/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [180/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [181/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [182/625], Loss: 1.5935\n",
      "Epoch [7/10], Step [183/625], Loss: 1.6092\n",
      "Epoch [7/10], Step [184/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [185/625], Loss: 1.4039\n",
      "Epoch [7/10], Step [186/625], Loss: 1.3922\n",
      "Epoch [7/10], Step [187/625], Loss: 1.1410\n",
      "Epoch [7/10], Step [188/625], Loss: 1.3682\n",
      "Epoch [7/10], Step [189/625], Loss: 1.1540\n",
      "Epoch [7/10], Step [190/625], Loss: 1.4126\n",
      "Epoch [7/10], Step [191/625], Loss: 1.6465\n",
      "Epoch [7/10], Step [192/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [193/625], Loss: 1.4909\n",
      "Epoch [7/10], Step [194/625], Loss: 1.6810\n",
      "Epoch [7/10], Step [195/625], Loss: 0.9056\n",
      "Epoch [7/10], Step [196/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [197/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [198/625], Loss: 1.2183\n",
      "Epoch [7/10], Step [199/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [200/625], Loss: 1.2267\n",
      "Epoch [7/10], Step [201/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [202/625], Loss: 1.3601\n",
      "Epoch [7/10], Step [203/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [204/625], Loss: 1.1731\n",
      "Epoch [7/10], Step [205/625], Loss: 1.1535\n",
      "Epoch [7/10], Step [206/625], Loss: 1.4046\n",
      "Epoch [7/10], Step [207/625], Loss: 1.4081\n",
      "Epoch [7/10], Step [208/625], Loss: 1.3912\n",
      "Epoch [7/10], Step [209/625], Loss: 0.9116\n",
      "Epoch [7/10], Step [210/625], Loss: 1.1537\n",
      "Epoch [7/10], Step [211/625], Loss: 1.4221\n",
      "Epoch [7/10], Step [212/625], Loss: 1.1253\n",
      "Epoch [7/10], Step [213/625], Loss: 1.1579\n",
      "Epoch [7/10], Step [214/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [215/625], Loss: 1.1639\n",
      "Epoch [7/10], Step [216/625], Loss: 1.6049\n",
      "Epoch [7/10], Step [217/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [218/625], Loss: 1.1555\n",
      "Epoch [7/10], Step [219/625], Loss: 1.1552\n",
      "Epoch [7/10], Step [220/625], Loss: 1.5816\n",
      "Epoch [7/10], Step [221/625], Loss: 1.6116\n",
      "Epoch [7/10], Step [222/625], Loss: 1.4220\n",
      "Epoch [7/10], Step [223/625], Loss: 1.1500\n",
      "Epoch [7/10], Step [224/625], Loss: 1.4009\n",
      "Epoch [7/10], Step [225/625], Loss: 1.4044\n",
      "Epoch [7/10], Step [226/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [227/625], Loss: 1.8983\n",
      "Epoch [7/10], Step [228/625], Loss: 1.3236\n",
      "Epoch [7/10], Step [229/625], Loss: 1.4052\n",
      "Epoch [7/10], Step [230/625], Loss: 1.4017\n",
      "Epoch [7/10], Step [231/625], Loss: 1.3915\n",
      "Epoch [7/10], Step [232/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [233/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [234/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [235/625], Loss: 1.3923\n",
      "Epoch [7/10], Step [236/625], Loss: 1.3992\n",
      "Epoch [7/10], Step [237/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [238/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [239/625], Loss: 1.2938\n",
      "Epoch [7/10], Step [240/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [241/625], Loss: 0.9743\n",
      "Epoch [7/10], Step [242/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [243/625], Loss: 1.1537\n",
      "Epoch [7/10], Step [244/625], Loss: 1.1556\n",
      "Epoch [7/10], Step [245/625], Loss: 1.6852\n",
      "Epoch [7/10], Step [246/625], Loss: 0.9056\n",
      "Epoch [7/10], Step [247/625], Loss: 1.1810\n",
      "Epoch [7/10], Step [248/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [249/625], Loss: 1.6277\n",
      "Epoch [7/10], Step [250/625], Loss: 1.1785\n",
      "Epoch [7/10], Step [251/625], Loss: 1.4578\n",
      "Epoch [7/10], Step [252/625], Loss: 1.1512\n",
      "Epoch [7/10], Step [253/625], Loss: 1.4053\n",
      "Epoch [7/10], Step [254/625], Loss: 1.9013\n",
      "Epoch [7/10], Step [255/625], Loss: 1.1516\n",
      "Epoch [7/10], Step [256/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [257/625], Loss: 1.1542\n",
      "Epoch [7/10], Step [258/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [259/625], Loss: 1.1697\n",
      "Epoch [7/10], Step [260/625], Loss: 1.6391\n",
      "Epoch [7/10], Step [261/625], Loss: 1.1555\n",
      "Epoch [7/10], Step [262/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [263/625], Loss: 1.6524\n",
      "Epoch [7/10], Step [264/625], Loss: 1.5741\n",
      "Epoch [7/10], Step [265/625], Loss: 1.1505\n",
      "Epoch [7/10], Step [266/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [267/625], Loss: 1.1266\n",
      "Epoch [7/10], Step [268/625], Loss: 0.9056\n",
      "Epoch [7/10], Step [269/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [270/625], Loss: 1.1473\n",
      "Epoch [7/10], Step [271/625], Loss: 0.9051\n",
      "Epoch [7/10], Step [272/625], Loss: 1.4006\n",
      "Epoch [7/10], Step [273/625], Loss: 0.9452\n",
      "Epoch [7/10], Step [274/625], Loss: 1.6540\n",
      "Epoch [7/10], Step [275/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [276/625], Loss: 1.4851\n",
      "Epoch [7/10], Step [277/625], Loss: 1.3945\n",
      "Epoch [7/10], Step [278/625], Loss: 1.8425\n",
      "Epoch [7/10], Step [279/625], Loss: 1.3614\n",
      "Epoch [7/10], Step [280/625], Loss: 0.9391\n",
      "Epoch [7/10], Step [281/625], Loss: 1.4564\n",
      "Epoch [7/10], Step [282/625], Loss: 1.6500\n",
      "Epoch [7/10], Step [283/625], Loss: 1.6522\n",
      "Epoch [7/10], Step [284/625], Loss: 1.1468\n",
      "Epoch [7/10], Step [285/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [286/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [287/625], Loss: 1.4041\n",
      "Epoch [7/10], Step [288/625], Loss: 1.1118\n",
      "Epoch [7/10], Step [289/625], Loss: 1.4014\n",
      "Epoch [7/10], Step [290/625], Loss: 1.3331\n",
      "Epoch [7/10], Step [291/625], Loss: 1.6452\n",
      "Epoch [7/10], Step [292/625], Loss: 1.5760\n",
      "Epoch [7/10], Step [293/625], Loss: 1.8391\n",
      "Epoch [7/10], Step [294/625], Loss: 1.1386\n",
      "Epoch [7/10], Step [295/625], Loss: 1.3982\n",
      "Epoch [7/10], Step [296/625], Loss: 0.9109\n",
      "Epoch [7/10], Step [297/625], Loss: 1.1453\n",
      "Epoch [7/10], Step [298/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [299/625], Loss: 1.4515\n",
      "Epoch [7/10], Step [300/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [301/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [302/625], Loss: 1.4475\n",
      "Epoch [7/10], Step [303/625], Loss: 1.8909\n",
      "Epoch [7/10], Step [304/625], Loss: 0.9054\n",
      "Epoch [7/10], Step [305/625], Loss: 1.3562\n",
      "Epoch [7/10], Step [306/625], Loss: 1.1420\n",
      "Epoch [7/10], Step [307/625], Loss: 1.6354\n",
      "Epoch [7/10], Step [308/625], Loss: 0.9425\n",
      "Epoch [7/10], Step [309/625], Loss: 1.4036\n",
      "Epoch [7/10], Step [310/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [311/625], Loss: 0.9272\n",
      "Epoch [7/10], Step [312/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [313/625], Loss: 1.1477\n",
      "Epoch [7/10], Step [314/625], Loss: 1.4044\n",
      "Epoch [7/10], Step [315/625], Loss: 1.1292\n",
      "Epoch [7/10], Step [316/625], Loss: 0.9320\n",
      "Epoch [7/10], Step [317/625], Loss: 1.1601\n",
      "Epoch [7/10], Step [318/625], Loss: 1.1555\n",
      "Epoch [7/10], Step [319/625], Loss: 1.3846\n",
      "Epoch [7/10], Step [320/625], Loss: 1.5268\n",
      "Epoch [7/10], Step [321/625], Loss: 1.3287\n",
      "Epoch [7/10], Step [322/625], Loss: 1.4044\n",
      "Epoch [7/10], Step [323/625], Loss: 1.3959\n",
      "Epoch [7/10], Step [324/625], Loss: 1.1508\n",
      "Epoch [7/10], Step [325/625], Loss: 1.1556\n",
      "Epoch [7/10], Step [326/625], Loss: 1.6536\n",
      "Epoch [7/10], Step [327/625], Loss: 1.3855\n",
      "Epoch [7/10], Step [328/625], Loss: 1.4611\n",
      "Epoch [7/10], Step [329/625], Loss: 1.1553\n",
      "Epoch [7/10], Step [330/625], Loss: 1.1650\n",
      "Epoch [7/10], Step [331/625], Loss: 1.3144\n",
      "Epoch [7/10], Step [332/625], Loss: 0.9585\n",
      "Epoch [7/10], Step [333/625], Loss: 1.4043\n",
      "Epoch [7/10], Step [334/625], Loss: 1.3501\n",
      "Epoch [7/10], Step [335/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [336/625], Loss: 0.9176\n",
      "Epoch [7/10], Step [337/625], Loss: 1.4031\n",
      "Epoch [7/10], Step [338/625], Loss: 1.2721\n",
      "Epoch [7/10], Step [339/625], Loss: 1.4853\n",
      "Epoch [7/10], Step [340/625], Loss: 1.3866\n",
      "Epoch [7/10], Step [341/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [342/625], Loss: 1.6160\n",
      "Epoch [7/10], Step [343/625], Loss: 1.1463\n",
      "Epoch [7/10], Step [344/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [345/625], Loss: 1.1562\n",
      "Epoch [7/10], Step [346/625], Loss: 1.4019\n",
      "Epoch [7/10], Step [347/625], Loss: 1.4249\n",
      "Epoch [7/10], Step [348/625], Loss: 1.3943\n",
      "Epoch [7/10], Step [349/625], Loss: 1.1387\n",
      "Epoch [7/10], Step [350/625], Loss: 1.1903\n",
      "Epoch [7/10], Step [351/625], Loss: 1.1530\n",
      "Epoch [7/10], Step [352/625], Loss: 0.9083\n",
      "Epoch [7/10], Step [353/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [354/625], Loss: 1.6076\n",
      "Epoch [7/10], Step [355/625], Loss: 1.2012\n",
      "Epoch [7/10], Step [356/625], Loss: 1.1668\n",
      "Epoch [7/10], Step [357/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [358/625], Loss: 1.5904\n",
      "Epoch [7/10], Step [359/625], Loss: 1.1553\n",
      "Epoch [7/10], Step [360/625], Loss: 1.4044\n",
      "Epoch [7/10], Step [361/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [362/625], Loss: 1.1974\n",
      "Epoch [7/10], Step [363/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [364/625], Loss: 1.4035\n",
      "Epoch [7/10], Step [365/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [366/625], Loss: 1.4285\n",
      "Epoch [7/10], Step [367/625], Loss: 1.4162\n",
      "Epoch [7/10], Step [368/625], Loss: 1.4355\n",
      "Epoch [7/10], Step [369/625], Loss: 1.1666\n",
      "Epoch [7/10], Step [370/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [371/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [372/625], Loss: 1.2495\n",
      "Epoch [7/10], Step [373/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [374/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [375/625], Loss: 1.4916\n",
      "Epoch [7/10], Step [376/625], Loss: 1.1546\n",
      "Epoch [7/10], Step [377/625], Loss: 1.1506\n",
      "Epoch [7/10], Step [378/625], Loss: 1.4059\n",
      "Epoch [7/10], Step [379/625], Loss: 0.9166\n",
      "Epoch [7/10], Step [380/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [381/625], Loss: 1.1565\n",
      "Epoch [7/10], Step [382/625], Loss: 1.1998\n",
      "Epoch [7/10], Step [383/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [384/625], Loss: 1.3906\n",
      "Epoch [7/10], Step [385/625], Loss: 1.1924\n",
      "Epoch [7/10], Step [386/625], Loss: 1.4045\n",
      "Epoch [7/10], Step [387/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [388/625], Loss: 0.9151\n",
      "Epoch [7/10], Step [389/625], Loss: 1.6391\n",
      "Epoch [7/10], Step [390/625], Loss: 1.1421\n",
      "Epoch [7/10], Step [391/625], Loss: 1.6556\n",
      "Epoch [7/10], Step [392/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [393/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [394/625], Loss: 1.4877\n",
      "Epoch [7/10], Step [395/625], Loss: 1.7952\n",
      "Epoch [7/10], Step [396/625], Loss: 1.1546\n",
      "Epoch [7/10], Step [397/625], Loss: 1.0065\n",
      "Epoch [7/10], Step [398/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [399/625], Loss: 1.4210\n",
      "Epoch [7/10], Step [400/625], Loss: 1.4028\n",
      "Epoch [7/10], Step [401/625], Loss: 0.9499\n",
      "Epoch [7/10], Step [402/625], Loss: 1.6517\n",
      "Epoch [7/10], Step [403/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [404/625], Loss: 1.4056\n",
      "Epoch [7/10], Step [405/625], Loss: 1.5361\n",
      "Epoch [7/10], Step [406/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [407/625], Loss: 0.9223\n",
      "Epoch [7/10], Step [408/625], Loss: 1.3197\n",
      "Epoch [7/10], Step [409/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [410/625], Loss: 1.4064\n",
      "Epoch [7/10], Step [411/625], Loss: 1.6242\n",
      "Epoch [7/10], Step [412/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [413/625], Loss: 0.9117\n",
      "Epoch [7/10], Step [414/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [415/625], Loss: 1.4174\n",
      "Epoch [7/10], Step [416/625], Loss: 1.4062\n",
      "Epoch [7/10], Step [417/625], Loss: 1.3842\n",
      "Epoch [7/10], Step [418/625], Loss: 1.6355\n",
      "Epoch [7/10], Step [419/625], Loss: 0.9066\n",
      "Epoch [7/10], Step [420/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [421/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [422/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [423/625], Loss: 1.4796\n",
      "Epoch [7/10], Step [424/625], Loss: 1.4037\n",
      "Epoch [7/10], Step [425/625], Loss: 1.5576\n",
      "Epoch [7/10], Step [426/625], Loss: 1.6316\n",
      "Epoch [7/10], Step [427/625], Loss: 1.2153\n",
      "Epoch [7/10], Step [428/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [429/625], Loss: 1.3953\n",
      "Epoch [7/10], Step [430/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [431/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [432/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [433/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [434/625], Loss: 1.4274\n",
      "Epoch [7/10], Step [435/625], Loss: 1.1643\n",
      "Epoch [7/10], Step [436/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [437/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [438/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [439/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [440/625], Loss: 1.4054\n",
      "Epoch [7/10], Step [441/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [442/625], Loss: 1.4075\n",
      "Epoch [7/10], Step [443/625], Loss: 1.6289\n",
      "Epoch [7/10], Step [444/625], Loss: 1.1488\n",
      "Epoch [7/10], Step [445/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [446/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [447/625], Loss: 1.4027\n",
      "Epoch [7/10], Step [448/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [449/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [450/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [451/625], Loss: 0.9728\n",
      "Epoch [7/10], Step [452/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [453/625], Loss: 1.1553\n",
      "Epoch [7/10], Step [454/625], Loss: 1.5585\n",
      "Epoch [7/10], Step [455/625], Loss: 1.1638\n",
      "Epoch [7/10], Step [456/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [457/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [458/625], Loss: 1.3913\n",
      "Epoch [7/10], Step [459/625], Loss: 0.9052\n",
      "Epoch [7/10], Step [460/625], Loss: 1.1547\n",
      "Epoch [7/10], Step [461/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [462/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [463/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [464/625], Loss: 1.4045\n",
      "Epoch [7/10], Step [465/625], Loss: 0.9891\n",
      "Epoch [7/10], Step [466/625], Loss: 1.1526\n",
      "Epoch [7/10], Step [467/625], Loss: 1.3976\n",
      "Epoch [7/10], Step [468/625], Loss: 1.0185\n",
      "Epoch [7/10], Step [469/625], Loss: 1.4050\n",
      "Epoch [7/10], Step [470/625], Loss: 1.6395\n",
      "Epoch [7/10], Step [471/625], Loss: 1.1554\n",
      "Epoch [7/10], Step [472/625], Loss: 0.9087\n",
      "Epoch [7/10], Step [473/625], Loss: 1.2468\n",
      "Epoch [7/10], Step [474/625], Loss: 1.6494\n",
      "Epoch [7/10], Step [475/625], Loss: 1.3900\n",
      "Epoch [7/10], Step [476/625], Loss: 1.2424\n",
      "Epoch [7/10], Step [477/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [478/625], Loss: 1.3683\n",
      "Epoch [7/10], Step [479/625], Loss: 1.6510\n",
      "Epoch [7/10], Step [480/625], Loss: 1.3922\n",
      "Epoch [7/10], Step [481/625], Loss: 1.3893\n",
      "Epoch [7/10], Step [482/625], Loss: 1.0336\n",
      "Epoch [7/10], Step [483/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [484/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [485/625], Loss: 1.4044\n",
      "Epoch [7/10], Step [486/625], Loss: 1.1839\n",
      "Epoch [7/10], Step [487/625], Loss: 1.3957\n",
      "Epoch [7/10], Step [488/625], Loss: 0.9312\n",
      "Epoch [7/10], Step [489/625], Loss: 1.4045\n",
      "Epoch [7/10], Step [490/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [491/625], Loss: 1.1551\n",
      "Epoch [7/10], Step [492/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [493/625], Loss: 1.1558\n",
      "Epoch [7/10], Step [494/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [495/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [496/625], Loss: 1.6619\n",
      "Epoch [7/10], Step [497/625], Loss: 1.1399\n",
      "Epoch [7/10], Step [498/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [499/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [500/625], Loss: 1.6527\n",
      "Epoch [7/10], Step [501/625], Loss: 1.4023\n",
      "Epoch [7/10], Step [502/625], Loss: 0.9050\n",
      "Epoch [7/10], Step [503/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [504/625], Loss: 1.1544\n",
      "Epoch [7/10], Step [505/625], Loss: 1.3941\n",
      "Epoch [7/10], Step [506/625], Loss: 1.6477\n",
      "Epoch [7/10], Step [507/625], Loss: 1.6542\n",
      "Epoch [7/10], Step [508/625], Loss: 1.6463\n",
      "Epoch [7/10], Step [509/625], Loss: 1.4071\n",
      "Epoch [7/10], Step [510/625], Loss: 0.9089\n",
      "Epoch [7/10], Step [511/625], Loss: 1.6403\n",
      "Epoch [7/10], Step [512/625], Loss: 1.1651\n",
      "Epoch [7/10], Step [513/625], Loss: 1.2768\n",
      "Epoch [7/10], Step [514/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [515/625], Loss: 1.3714\n",
      "Epoch [7/10], Step [516/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [517/625], Loss: 1.0644\n",
      "Epoch [7/10], Step [518/625], Loss: 1.4028\n",
      "Epoch [7/10], Step [519/625], Loss: 1.8887\n",
      "Epoch [7/10], Step [520/625], Loss: 1.5359\n",
      "Epoch [7/10], Step [521/625], Loss: 1.6533\n",
      "Epoch [7/10], Step [522/625], Loss: 1.3916\n",
      "Epoch [7/10], Step [523/625], Loss: 1.4030\n",
      "Epoch [7/10], Step [524/625], Loss: 1.6516\n",
      "Epoch [7/10], Step [525/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [526/625], Loss: 1.4034\n",
      "Epoch [7/10], Step [527/625], Loss: 1.6484\n",
      "Epoch [7/10], Step [528/625], Loss: 1.2567\n",
      "Epoch [7/10], Step [529/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [530/625], Loss: 1.3961\n",
      "Epoch [7/10], Step [531/625], Loss: 1.1552\n",
      "Epoch [7/10], Step [532/625], Loss: 1.1535\n",
      "Epoch [7/10], Step [533/625], Loss: 1.4047\n",
      "Epoch [7/10], Step [534/625], Loss: 1.6406\n",
      "Epoch [7/10], Step [535/625], Loss: 1.3935\n",
      "Epoch [7/10], Step [536/625], Loss: 0.9065\n",
      "Epoch [7/10], Step [537/625], Loss: 1.4067\n",
      "Epoch [7/10], Step [538/625], Loss: 1.5420\n",
      "Epoch [7/10], Step [539/625], Loss: 1.3970\n",
      "Epoch [7/10], Step [540/625], Loss: 1.6107\n",
      "Epoch [7/10], Step [541/625], Loss: 1.6437\n",
      "Epoch [7/10], Step [542/625], Loss: 1.2936\n",
      "Epoch [7/10], Step [543/625], Loss: 1.6543\n",
      "Epoch [7/10], Step [544/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [545/625], Loss: 1.6752\n",
      "Epoch [7/10], Step [546/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [547/625], Loss: 1.3900\n",
      "Epoch [7/10], Step [548/625], Loss: 1.8697\n",
      "Epoch [7/10], Step [549/625], Loss: 1.1540\n",
      "Epoch [7/10], Step [550/625], Loss: 1.1622\n",
      "Epoch [7/10], Step [551/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [552/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [553/625], Loss: 1.4072\n",
      "Epoch [7/10], Step [554/625], Loss: 1.3138\n",
      "Epoch [7/10], Step [555/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [556/625], Loss: 1.4048\n",
      "Epoch [7/10], Step [557/625], Loss: 1.1546\n",
      "Epoch [7/10], Step [558/625], Loss: 1.4049\n",
      "Epoch [7/10], Step [559/625], Loss: 1.1954\n",
      "Epoch [7/10], Step [560/625], Loss: 1.1999\n",
      "Epoch [7/10], Step [561/625], Loss: 1.1387\n",
      "Epoch [7/10], Step [562/625], Loss: 1.1843\n",
      "Epoch [7/10], Step [563/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [564/625], Loss: 1.3997\n",
      "Epoch [7/10], Step [565/625], Loss: 1.6548\n",
      "Epoch [7/10], Step [566/625], Loss: 1.1550\n",
      "Epoch [7/10], Step [567/625], Loss: 1.1628\n",
      "Epoch [7/10], Step [568/625], Loss: 0.9449\n",
      "Epoch [7/10], Step [569/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [570/625], Loss: 1.2930\n",
      "Epoch [7/10], Step [571/625], Loss: 1.6541\n",
      "Epoch [7/10], Step [572/625], Loss: 1.1525\n",
      "Epoch [7/10], Step [573/625], Loss: 1.6533\n",
      "Epoch [7/10], Step [574/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [575/625], Loss: 0.9111\n",
      "Epoch [7/10], Step [576/625], Loss: 1.3588\n",
      "Epoch [7/10], Step [577/625], Loss: 1.1187\n",
      "Epoch [7/10], Step [578/625], Loss: 1.4046\n",
      "Epoch [7/10], Step [579/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [580/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [581/625], Loss: 1.0389\n",
      "Epoch [7/10], Step [582/625], Loss: 1.6547\n",
      "Epoch [7/10], Step [583/625], Loss: 1.1355\n",
      "Epoch [7/10], Step [584/625], Loss: 1.6488\n",
      "Epoch [7/10], Step [585/625], Loss: 1.4318\n",
      "Epoch [7/10], Step [586/625], Loss: 1.1457\n",
      "Epoch [7/10], Step [587/625], Loss: 0.9049\n",
      "Epoch [7/10], Step [588/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [589/625], Loss: 1.6455\n",
      "Epoch [7/10], Step [590/625], Loss: 1.1500\n",
      "Epoch [7/10], Step [591/625], Loss: 0.9532\n",
      "Epoch [7/10], Step [592/625], Loss: 1.1581\n",
      "Epoch [7/10], Step [593/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [594/625], Loss: 1.1507\n",
      "Epoch [7/10], Step [595/625], Loss: 1.6495\n",
      "Epoch [7/10], Step [596/625], Loss: 1.4019\n",
      "Epoch [7/10], Step [597/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [598/625], Loss: 1.4880\n",
      "Epoch [7/10], Step [599/625], Loss: 1.1657\n",
      "Epoch [7/10], Step [600/625], Loss: 1.1564\n",
      "Epoch [7/10], Step [601/625], Loss: 1.1507\n",
      "Epoch [7/10], Step [602/625], Loss: 1.3345\n",
      "Epoch [7/10], Step [603/625], Loss: 1.1515\n",
      "Epoch [7/10], Step [604/625], Loss: 1.3361\n",
      "Epoch [7/10], Step [605/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [606/625], Loss: 1.4035\n",
      "Epoch [7/10], Step [607/625], Loss: 1.4404\n",
      "Epoch [7/10], Step [608/625], Loss: 1.8825\n",
      "Epoch [7/10], Step [609/625], Loss: 1.4067\n",
      "Epoch [7/10], Step [610/625], Loss: 1.1751\n",
      "Epoch [7/10], Step [611/625], Loss: 1.1565\n",
      "Epoch [7/10], Step [612/625], Loss: 1.4045\n",
      "Epoch [7/10], Step [613/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [614/625], Loss: 1.3530\n",
      "Epoch [7/10], Step [615/625], Loss: 1.4294\n",
      "Epoch [7/10], Step [616/625], Loss: 1.1549\n",
      "Epoch [7/10], Step [617/625], Loss: 0.9048\n",
      "Epoch [7/10], Step [618/625], Loss: 0.9080\n",
      "Epoch [7/10], Step [619/625], Loss: 0.9051\n",
      "Epoch [7/10], Step [620/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [621/625], Loss: 1.5305\n",
      "Epoch [7/10], Step [622/625], Loss: 1.1235\n",
      "Epoch [7/10], Step [623/625], Loss: 1.1548\n",
      "Epoch [7/10], Step [624/625], Loss: 1.1457\n",
      "Epoch [7/10], Step [625/625], Loss: 1.4044\n",
      "Epoch [8/10], Step [1/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [2/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [3/625], Loss: 0.9582\n",
      "Epoch [8/10], Step [4/625], Loss: 1.3970\n",
      "Epoch [8/10], Step [5/625], Loss: 1.6526\n",
      "Epoch [8/10], Step [6/625], Loss: 1.2692\n",
      "Epoch [8/10], Step [7/625], Loss: 1.3950\n",
      "Epoch [8/10], Step [8/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [9/625], Loss: 1.3905\n",
      "Epoch [8/10], Step [10/625], Loss: 1.3956\n",
      "Epoch [8/10], Step [11/625], Loss: 1.4533\n",
      "Epoch [8/10], Step [12/625], Loss: 1.7504\n",
      "Epoch [8/10], Step [13/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [14/625], Loss: 1.1547\n",
      "Epoch [8/10], Step [15/625], Loss: 1.1542\n",
      "Epoch [8/10], Step [16/625], Loss: 1.6553\n",
      "Epoch [8/10], Step [17/625], Loss: 1.0642\n",
      "Epoch [8/10], Step [18/625], Loss: 1.3390\n",
      "Epoch [8/10], Step [19/625], Loss: 1.1809\n",
      "Epoch [8/10], Step [20/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [21/625], Loss: 1.3689\n",
      "Epoch [8/10], Step [22/625], Loss: 1.3970\n",
      "Epoch [8/10], Step [23/625], Loss: 1.3818\n",
      "Epoch [8/10], Step [24/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [25/625], Loss: 1.6519\n",
      "Epoch [8/10], Step [26/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [27/625], Loss: 0.9245\n",
      "Epoch [8/10], Step [28/625], Loss: 1.6169\n",
      "Epoch [8/10], Step [29/625], Loss: 1.2568\n",
      "Epoch [8/10], Step [30/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [31/625], Loss: 1.2413\n",
      "Epoch [8/10], Step [32/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [33/625], Loss: 1.6534\n",
      "Epoch [8/10], Step [34/625], Loss: 1.3913\n",
      "Epoch [8/10], Step [35/625], Loss: 1.1676\n",
      "Epoch [8/10], Step [36/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [37/625], Loss: 1.1504\n",
      "Epoch [8/10], Step [38/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [39/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [40/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [41/625], Loss: 1.6547\n",
      "Epoch [8/10], Step [42/625], Loss: 1.1668\n",
      "Epoch [8/10], Step [43/625], Loss: 1.6444\n",
      "Epoch [8/10], Step [44/625], Loss: 1.1566\n",
      "Epoch [8/10], Step [45/625], Loss: 1.6546\n",
      "Epoch [8/10], Step [46/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [47/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [48/625], Loss: 1.2219\n",
      "Epoch [8/10], Step [49/625], Loss: 0.9055\n",
      "Epoch [8/10], Step [50/625], Loss: 1.3985\n",
      "Epoch [8/10], Step [51/625], Loss: 1.4015\n",
      "Epoch [8/10], Step [52/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [53/625], Loss: 1.1577\n",
      "Epoch [8/10], Step [54/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [55/625], Loss: 1.4046\n",
      "Epoch [8/10], Step [56/625], Loss: 1.3995\n",
      "Epoch [8/10], Step [57/625], Loss: 1.1061\n",
      "Epoch [8/10], Step [58/625], Loss: 1.9031\n",
      "Epoch [8/10], Step [59/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [60/625], Loss: 0.9389\n",
      "Epoch [8/10], Step [61/625], Loss: 0.9051\n",
      "Epoch [8/10], Step [62/625], Loss: 1.4036\n",
      "Epoch [8/10], Step [63/625], Loss: 1.1678\n",
      "Epoch [8/10], Step [64/625], Loss: 1.6482\n",
      "Epoch [8/10], Step [65/625], Loss: 1.3391\n",
      "Epoch [8/10], Step [66/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [67/625], Loss: 1.1568\n",
      "Epoch [8/10], Step [68/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [69/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [70/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [71/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [72/625], Loss: 1.4521\n",
      "Epoch [8/10], Step [73/625], Loss: 1.1510\n",
      "Epoch [8/10], Step [74/625], Loss: 1.7449\n",
      "Epoch [8/10], Step [75/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [76/625], Loss: 1.4335\n",
      "Epoch [8/10], Step [77/625], Loss: 1.8987\n",
      "Epoch [8/10], Step [78/625], Loss: 1.1808\n",
      "Epoch [8/10], Step [79/625], Loss: 1.2212\n",
      "Epoch [8/10], Step [80/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [81/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [82/625], Loss: 1.1575\n",
      "Epoch [8/10], Step [83/625], Loss: 0.9199\n",
      "Epoch [8/10], Step [84/625], Loss: 1.1452\n",
      "Epoch [8/10], Step [85/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [86/625], Loss: 1.1571\n",
      "Epoch [8/10], Step [87/625], Loss: 1.1502\n",
      "Epoch [8/10], Step [88/625], Loss: 1.1554\n",
      "Epoch [8/10], Step [89/625], Loss: 1.4025\n",
      "Epoch [8/10], Step [90/625], Loss: 1.4104\n",
      "Epoch [8/10], Step [91/625], Loss: 1.4004\n",
      "Epoch [8/10], Step [92/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [93/625], Loss: 1.0886\n",
      "Epoch [8/10], Step [94/625], Loss: 1.1563\n",
      "Epoch [8/10], Step [95/625], Loss: 1.9042\n",
      "Epoch [8/10], Step [96/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [97/625], Loss: 1.3799\n",
      "Epoch [8/10], Step [98/625], Loss: 1.1809\n",
      "Epoch [8/10], Step [99/625], Loss: 1.4032\n",
      "Epoch [8/10], Step [100/625], Loss: 1.4045\n",
      "Epoch [8/10], Step [101/625], Loss: 1.5885\n",
      "Epoch [8/10], Step [102/625], Loss: 1.6391\n",
      "Epoch [8/10], Step [103/625], Loss: 1.1551\n",
      "Epoch [8/10], Step [104/625], Loss: 1.4019\n",
      "Epoch [8/10], Step [105/625], Loss: 1.6383\n",
      "Epoch [8/10], Step [106/625], Loss: 1.9011\n",
      "Epoch [8/10], Step [107/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [108/625], Loss: 1.6375\n",
      "Epoch [8/10], Step [109/625], Loss: 1.1726\n",
      "Epoch [8/10], Step [110/625], Loss: 1.6848\n",
      "Epoch [8/10], Step [111/625], Loss: 0.9102\n",
      "Epoch [8/10], Step [112/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [113/625], Loss: 0.9052\n",
      "Epoch [8/10], Step [114/625], Loss: 1.1596\n",
      "Epoch [8/10], Step [115/625], Loss: 0.9066\n",
      "Epoch [8/10], Step [116/625], Loss: 1.3169\n",
      "Epoch [8/10], Step [117/625], Loss: 1.4032\n",
      "Epoch [8/10], Step [118/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [119/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [120/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [121/625], Loss: 1.1546\n",
      "Epoch [8/10], Step [122/625], Loss: 1.1483\n",
      "Epoch [8/10], Step [123/625], Loss: 1.4029\n",
      "Epoch [8/10], Step [124/625], Loss: 1.4055\n",
      "Epoch [8/10], Step [125/625], Loss: 1.5771\n",
      "Epoch [8/10], Step [126/625], Loss: 0.9286\n",
      "Epoch [8/10], Step [127/625], Loss: 1.5822\n",
      "Epoch [8/10], Step [128/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [129/625], Loss: 1.2926\n",
      "Epoch [8/10], Step [130/625], Loss: 0.9057\n",
      "Epoch [8/10], Step [131/625], Loss: 1.6523\n",
      "Epoch [8/10], Step [132/625], Loss: 1.4009\n",
      "Epoch [8/10], Step [133/625], Loss: 1.3952\n",
      "Epoch [8/10], Step [134/625], Loss: 1.1594\n",
      "Epoch [8/10], Step [135/625], Loss: 1.1320\n",
      "Epoch [8/10], Step [136/625], Loss: 1.4534\n",
      "Epoch [8/10], Step [137/625], Loss: 1.3954\n",
      "Epoch [8/10], Step [138/625], Loss: 1.4042\n",
      "Epoch [8/10], Step [139/625], Loss: 1.6541\n",
      "Epoch [8/10], Step [140/625], Loss: 1.1544\n",
      "Epoch [8/10], Step [141/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [142/625], Loss: 1.8887\n",
      "Epoch [8/10], Step [143/625], Loss: 1.2860\n",
      "Epoch [8/10], Step [144/625], Loss: 1.5660\n",
      "Epoch [8/10], Step [145/625], Loss: 0.9676\n",
      "Epoch [8/10], Step [146/625], Loss: 1.1579\n",
      "Epoch [8/10], Step [147/625], Loss: 0.9183\n",
      "Epoch [8/10], Step [148/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [149/625], Loss: 1.1507\n",
      "Epoch [8/10], Step [150/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [151/625], Loss: 0.9068\n",
      "Epoch [8/10], Step [152/625], Loss: 1.4053\n",
      "Epoch [8/10], Step [153/625], Loss: 1.0609\n",
      "Epoch [8/10], Step [154/625], Loss: 1.4055\n",
      "Epoch [8/10], Step [155/625], Loss: 0.9370\n",
      "Epoch [8/10], Step [156/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [157/625], Loss: 1.4728\n",
      "Epoch [8/10], Step [158/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [159/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [160/625], Loss: 1.1404\n",
      "Epoch [8/10], Step [161/625], Loss: 1.1558\n",
      "Epoch [8/10], Step [162/625], Loss: 1.3861\n",
      "Epoch [8/10], Step [163/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [164/625], Loss: 1.6345\n",
      "Epoch [8/10], Step [165/625], Loss: 1.1558\n",
      "Epoch [8/10], Step [166/625], Loss: 1.4028\n",
      "Epoch [8/10], Step [167/625], Loss: 1.4043\n",
      "Epoch [8/10], Step [168/625], Loss: 1.1558\n",
      "Epoch [8/10], Step [169/625], Loss: 1.1524\n",
      "Epoch [8/10], Step [170/625], Loss: 1.4016\n",
      "Epoch [8/10], Step [171/625], Loss: 1.1448\n",
      "Epoch [8/10], Step [172/625], Loss: 1.6565\n",
      "Epoch [8/10], Step [173/625], Loss: 1.2749\n",
      "Epoch [8/10], Step [174/625], Loss: 1.3712\n",
      "Epoch [8/10], Step [175/625], Loss: 1.4059\n",
      "Epoch [8/10], Step [176/625], Loss: 1.1568\n",
      "Epoch [8/10], Step [177/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [178/625], Loss: 1.4275\n",
      "Epoch [8/10], Step [179/625], Loss: 1.1684\n",
      "Epoch [8/10], Step [180/625], Loss: 1.4030\n",
      "Epoch [8/10], Step [181/625], Loss: 1.4194\n",
      "Epoch [8/10], Step [182/625], Loss: 1.9042\n",
      "Epoch [8/10], Step [183/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [184/625], Loss: 1.6248\n",
      "Epoch [8/10], Step [185/625], Loss: 1.3827\n",
      "Epoch [8/10], Step [186/625], Loss: 1.6528\n",
      "Epoch [8/10], Step [187/625], Loss: 1.5756\n",
      "Epoch [8/10], Step [188/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [189/625], Loss: 1.1559\n",
      "Epoch [8/10], Step [190/625], Loss: 1.4134\n",
      "Epoch [8/10], Step [191/625], Loss: 1.4082\n",
      "Epoch [8/10], Step [192/625], Loss: 0.9082\n",
      "Epoch [8/10], Step [193/625], Loss: 1.5386\n",
      "Epoch [8/10], Step [194/625], Loss: 0.9110\n",
      "Epoch [8/10], Step [195/625], Loss: 1.4453\n",
      "Epoch [8/10], Step [196/625], Loss: 1.5443\n",
      "Epoch [8/10], Step [197/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [198/625], Loss: 1.1474\n",
      "Epoch [8/10], Step [199/625], Loss: 1.3908\n",
      "Epoch [8/10], Step [200/625], Loss: 1.4041\n",
      "Epoch [8/10], Step [201/625], Loss: 1.1625\n",
      "Epoch [8/10], Step [202/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [203/625], Loss: 1.3953\n",
      "Epoch [8/10], Step [204/625], Loss: 1.1421\n",
      "Epoch [8/10], Step [205/625], Loss: 1.6285\n",
      "Epoch [8/10], Step [206/625], Loss: 1.3887\n",
      "Epoch [8/10], Step [207/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [208/625], Loss: 1.3194\n",
      "Epoch [8/10], Step [209/625], Loss: 1.6155\n",
      "Epoch [8/10], Step [210/625], Loss: 1.6451\n",
      "Epoch [8/10], Step [211/625], Loss: 1.1542\n",
      "Epoch [8/10], Step [212/625], Loss: 1.4050\n",
      "Epoch [8/10], Step [213/625], Loss: 1.6493\n",
      "Epoch [8/10], Step [214/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [215/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [216/625], Loss: 1.6259\n",
      "Epoch [8/10], Step [217/625], Loss: 0.9163\n",
      "Epoch [8/10], Step [218/625], Loss: 1.6550\n",
      "Epoch [8/10], Step [219/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [220/625], Loss: 1.3899\n",
      "Epoch [8/10], Step [221/625], Loss: 1.1895\n",
      "Epoch [8/10], Step [222/625], Loss: 1.6025\n",
      "Epoch [8/10], Step [223/625], Loss: 1.5457\n",
      "Epoch [8/10], Step [224/625], Loss: 1.6510\n",
      "Epoch [8/10], Step [225/625], Loss: 1.5463\n",
      "Epoch [8/10], Step [226/625], Loss: 1.6476\n",
      "Epoch [8/10], Step [227/625], Loss: 0.9756\n",
      "Epoch [8/10], Step [228/625], Loss: 1.4272\n",
      "Epoch [8/10], Step [229/625], Loss: 1.5198\n",
      "Epoch [8/10], Step [230/625], Loss: 1.6375\n",
      "Epoch [8/10], Step [231/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [232/625], Loss: 1.0694\n",
      "Epoch [8/10], Step [233/625], Loss: 1.9044\n",
      "Epoch [8/10], Step [234/625], Loss: 1.4010\n",
      "Epoch [8/10], Step [235/625], Loss: 1.3719\n",
      "Epoch [8/10], Step [236/625], Loss: 1.4037\n",
      "Epoch [8/10], Step [237/625], Loss: 1.4914\n",
      "Epoch [8/10], Step [238/625], Loss: 1.9031\n",
      "Epoch [8/10], Step [239/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [240/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [241/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [242/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [243/625], Loss: 1.5072\n",
      "Epoch [8/10], Step [244/625], Loss: 1.6374\n",
      "Epoch [8/10], Step [245/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [246/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [247/625], Loss: 1.3827\n",
      "Epoch [8/10], Step [248/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [249/625], Loss: 1.4043\n",
      "Epoch [8/10], Step [250/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [251/625], Loss: 1.0564\n",
      "Epoch [8/10], Step [252/625], Loss: 1.1524\n",
      "Epoch [8/10], Step [253/625], Loss: 1.1533\n",
      "Epoch [8/10], Step [254/625], Loss: 1.5758\n",
      "Epoch [8/10], Step [255/625], Loss: 1.4026\n",
      "Epoch [8/10], Step [256/625], Loss: 1.6579\n",
      "Epoch [8/10], Step [257/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [258/625], Loss: 1.1554\n",
      "Epoch [8/10], Step [259/625], Loss: 1.4505\n",
      "Epoch [8/10], Step [260/625], Loss: 1.4331\n",
      "Epoch [8/10], Step [261/625], Loss: 1.6515\n",
      "Epoch [8/10], Step [262/625], Loss: 1.0809\n",
      "Epoch [8/10], Step [263/625], Loss: 1.3356\n",
      "Epoch [8/10], Step [264/625], Loss: 1.4628\n",
      "Epoch [8/10], Step [265/625], Loss: 1.4216\n",
      "Epoch [8/10], Step [266/625], Loss: 1.4045\n",
      "Epoch [8/10], Step [267/625], Loss: 1.1108\n",
      "Epoch [8/10], Step [268/625], Loss: 1.6552\n",
      "Epoch [8/10], Step [269/625], Loss: 1.4964\n",
      "Epoch [8/10], Step [270/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [271/625], Loss: 1.4044\n",
      "Epoch [8/10], Step [272/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [273/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [274/625], Loss: 1.3814\n",
      "Epoch [8/10], Step [275/625], Loss: 1.4038\n",
      "Epoch [8/10], Step [276/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [277/625], Loss: 1.2186\n",
      "Epoch [8/10], Step [278/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [279/625], Loss: 1.4976\n",
      "Epoch [8/10], Step [280/625], Loss: 1.4024\n",
      "Epoch [8/10], Step [281/625], Loss: 1.1497\n",
      "Epoch [8/10], Step [282/625], Loss: 1.3387\n",
      "Epoch [8/10], Step [283/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [284/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [285/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [286/625], Loss: 1.0346\n",
      "Epoch [8/10], Step [287/625], Loss: 1.3899\n",
      "Epoch [8/10], Step [288/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [289/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [290/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [291/625], Loss: 1.6397\n",
      "Epoch [8/10], Step [292/625], Loss: 0.9784\n",
      "Epoch [8/10], Step [293/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [294/625], Loss: 1.6525\n",
      "Epoch [8/10], Step [295/625], Loss: 1.6788\n",
      "Epoch [8/10], Step [296/625], Loss: 1.1588\n",
      "Epoch [8/10], Step [297/625], Loss: 1.1547\n",
      "Epoch [8/10], Step [298/625], Loss: 1.2364\n",
      "Epoch [8/10], Step [299/625], Loss: 1.1915\n",
      "Epoch [8/10], Step [300/625], Loss: 1.1546\n",
      "Epoch [8/10], Step [301/625], Loss: 1.6421\n",
      "Epoch [8/10], Step [302/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [303/625], Loss: 1.1540\n",
      "Epoch [8/10], Step [304/625], Loss: 1.3582\n",
      "Epoch [8/10], Step [305/625], Loss: 1.6536\n",
      "Epoch [8/10], Step [306/625], Loss: 1.0000\n",
      "Epoch [8/10], Step [307/625], Loss: 1.0961\n",
      "Epoch [8/10], Step [308/625], Loss: 1.3994\n",
      "Epoch [8/10], Step [309/625], Loss: 1.3510\n",
      "Epoch [8/10], Step [310/625], Loss: 1.1536\n",
      "Epoch [8/10], Step [311/625], Loss: 0.9878\n",
      "Epoch [8/10], Step [312/625], Loss: 1.6517\n",
      "Epoch [8/10], Step [313/625], Loss: 1.4028\n",
      "Epoch [8/10], Step [314/625], Loss: 1.8979\n",
      "Epoch [8/10], Step [315/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [316/625], Loss: 1.1592\n",
      "Epoch [8/10], Step [317/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [318/625], Loss: 1.4049\n",
      "Epoch [8/10], Step [319/625], Loss: 1.4074\n",
      "Epoch [8/10], Step [320/625], Loss: 1.4360\n",
      "Epoch [8/10], Step [321/625], Loss: 0.9076\n",
      "Epoch [8/10], Step [322/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [323/625], Loss: 1.6536\n",
      "Epoch [8/10], Step [324/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [325/625], Loss: 1.6384\n",
      "Epoch [8/10], Step [326/625], Loss: 0.9077\n",
      "Epoch [8/10], Step [327/625], Loss: 1.1167\n",
      "Epoch [8/10], Step [328/625], Loss: 1.1551\n",
      "Epoch [8/10], Step [329/625], Loss: 1.1542\n",
      "Epoch [8/10], Step [330/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [331/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [332/625], Loss: 1.6507\n",
      "Epoch [8/10], Step [333/625], Loss: 1.4046\n",
      "Epoch [8/10], Step [334/625], Loss: 1.1581\n",
      "Epoch [8/10], Step [335/625], Loss: 1.8714\n",
      "Epoch [8/10], Step [336/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [337/625], Loss: 1.3934\n",
      "Epoch [8/10], Step [338/625], Loss: 0.9074\n",
      "Epoch [8/10], Step [339/625], Loss: 1.8552\n",
      "Epoch [8/10], Step [340/625], Loss: 1.6387\n",
      "Epoch [8/10], Step [341/625], Loss: 1.6540\n",
      "Epoch [8/10], Step [342/625], Loss: 1.6398\n",
      "Epoch [8/10], Step [343/625], Loss: 1.4554\n",
      "Epoch [8/10], Step [344/625], Loss: 1.4020\n",
      "Epoch [8/10], Step [345/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [346/625], Loss: 1.9021\n",
      "Epoch [8/10], Step [347/625], Loss: 1.3035\n",
      "Epoch [8/10], Step [348/625], Loss: 1.3981\n",
      "Epoch [8/10], Step [349/625], Loss: 1.6404\n",
      "Epoch [8/10], Step [350/625], Loss: 1.6256\n",
      "Epoch [8/10], Step [351/625], Loss: 1.1563\n",
      "Epoch [8/10], Step [352/625], Loss: 1.1547\n",
      "Epoch [8/10], Step [353/625], Loss: 1.3337\n",
      "Epoch [8/10], Step [354/625], Loss: 1.1903\n",
      "Epoch [8/10], Step [355/625], Loss: 1.5849\n",
      "Epoch [8/10], Step [356/625], Loss: 1.3345\n",
      "Epoch [8/10], Step [357/625], Loss: 1.1558\n",
      "Epoch [8/10], Step [358/625], Loss: 1.8986\n",
      "Epoch [8/10], Step [359/625], Loss: 1.1558\n",
      "Epoch [8/10], Step [360/625], Loss: 1.4050\n",
      "Epoch [8/10], Step [361/625], Loss: 1.4060\n",
      "Epoch [8/10], Step [362/625], Loss: 1.4023\n",
      "Epoch [8/10], Step [363/625], Loss: 1.1395\n",
      "Epoch [8/10], Step [364/625], Loss: 1.6390\n",
      "Epoch [8/10], Step [365/625], Loss: 1.5429\n",
      "Epoch [8/10], Step [366/625], Loss: 1.4056\n",
      "Epoch [8/10], Step [367/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [368/625], Loss: 1.4063\n",
      "Epoch [8/10], Step [369/625], Loss: 1.8398\n",
      "Epoch [8/10], Step [370/625], Loss: 1.5622\n",
      "Epoch [8/10], Step [371/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [372/625], Loss: 1.4096\n",
      "Epoch [8/10], Step [373/625], Loss: 1.3317\n",
      "Epoch [8/10], Step [374/625], Loss: 1.3841\n",
      "Epoch [8/10], Step [375/625], Loss: 1.3739\n",
      "Epoch [8/10], Step [376/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [377/625], Loss: 1.6177\n",
      "Epoch [8/10], Step [378/625], Loss: 1.1802\n",
      "Epoch [8/10], Step [379/625], Loss: 1.6010\n",
      "Epoch [8/10], Step [380/625], Loss: 1.6341\n",
      "Epoch [8/10], Step [381/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [382/625], Loss: 1.6546\n",
      "Epoch [8/10], Step [383/625], Loss: 1.3033\n",
      "Epoch [8/10], Step [384/625], Loss: 1.1541\n",
      "Epoch [8/10], Step [385/625], Loss: 0.9376\n",
      "Epoch [8/10], Step [386/625], Loss: 1.3714\n",
      "Epoch [8/10], Step [387/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [388/625], Loss: 1.4149\n",
      "Epoch [8/10], Step [389/625], Loss: 1.4049\n",
      "Epoch [8/10], Step [390/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [391/625], Loss: 1.2056\n",
      "Epoch [8/10], Step [392/625], Loss: 1.4112\n",
      "Epoch [8/10], Step [393/625], Loss: 1.3962\n",
      "Epoch [8/10], Step [394/625], Loss: 1.4036\n",
      "Epoch [8/10], Step [395/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [396/625], Loss: 1.8357\n",
      "Epoch [8/10], Step [397/625], Loss: 1.4031\n",
      "Epoch [8/10], Step [398/625], Loss: 1.1582\n",
      "Epoch [8/10], Step [399/625], Loss: 0.9051\n",
      "Epoch [8/10], Step [400/625], Loss: 1.1578\n",
      "Epoch [8/10], Step [401/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [402/625], Loss: 1.6510\n",
      "Epoch [8/10], Step [403/625], Loss: 0.9163\n",
      "Epoch [8/10], Step [404/625], Loss: 1.3983\n",
      "Epoch [8/10], Step [405/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [406/625], Loss: 1.1789\n",
      "Epoch [8/10], Step [407/625], Loss: 1.4395\n",
      "Epoch [8/10], Step [408/625], Loss: 1.9045\n",
      "Epoch [8/10], Step [409/625], Loss: 1.4069\n",
      "Epoch [8/10], Step [410/625], Loss: 1.8445\n",
      "Epoch [8/10], Step [411/625], Loss: 1.1547\n",
      "Epoch [8/10], Step [412/625], Loss: 1.6542\n",
      "Epoch [8/10], Step [413/625], Loss: 1.4030\n",
      "Epoch [8/10], Step [414/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [415/625], Loss: 1.6395\n",
      "Epoch [8/10], Step [416/625], Loss: 1.6794\n",
      "Epoch [8/10], Step [417/625], Loss: 1.1573\n",
      "Epoch [8/10], Step [418/625], Loss: 1.6421\n",
      "Epoch [8/10], Step [419/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [420/625], Loss: 1.3941\n",
      "Epoch [8/10], Step [421/625], Loss: 1.5030\n",
      "Epoch [8/10], Step [422/625], Loss: 1.3136\n",
      "Epoch [8/10], Step [423/625], Loss: 1.2510\n",
      "Epoch [8/10], Step [424/625], Loss: 1.1613\n",
      "Epoch [8/10], Step [425/625], Loss: 1.4993\n",
      "Epoch [8/10], Step [426/625], Loss: 1.1497\n",
      "Epoch [8/10], Step [427/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [428/625], Loss: 1.1486\n",
      "Epoch [8/10], Step [429/625], Loss: 1.4042\n",
      "Epoch [8/10], Step [430/625], Loss: 1.6545\n",
      "Epoch [8/10], Step [431/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [432/625], Loss: 1.4051\n",
      "Epoch [8/10], Step [433/625], Loss: 0.9793\n",
      "Epoch [8/10], Step [434/625], Loss: 1.6385\n",
      "Epoch [8/10], Step [435/625], Loss: 1.6449\n",
      "Epoch [8/10], Step [436/625], Loss: 1.1551\n",
      "Epoch [8/10], Step [437/625], Loss: 1.4042\n",
      "Epoch [8/10], Step [438/625], Loss: 0.9074\n",
      "Epoch [8/10], Step [439/625], Loss: 1.1069\n",
      "Epoch [8/10], Step [440/625], Loss: 1.6454\n",
      "Epoch [8/10], Step [441/625], Loss: 1.6340\n",
      "Epoch [8/10], Step [442/625], Loss: 1.9046\n",
      "Epoch [8/10], Step [443/625], Loss: 1.4638\n",
      "Epoch [8/10], Step [444/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [445/625], Loss: 1.6544\n",
      "Epoch [8/10], Step [446/625], Loss: 0.9223\n",
      "Epoch [8/10], Step [447/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [448/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [449/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [450/625], Loss: 1.1541\n",
      "Epoch [8/10], Step [451/625], Loss: 1.4986\n",
      "Epoch [8/10], Step [452/625], Loss: 1.3157\n",
      "Epoch [8/10], Step [453/625], Loss: 1.4042\n",
      "Epoch [8/10], Step [454/625], Loss: 0.9702\n",
      "Epoch [8/10], Step [455/625], Loss: 1.1569\n",
      "Epoch [8/10], Step [456/625], Loss: 1.4049\n",
      "Epoch [8/10], Step [457/625], Loss: 1.2009\n",
      "Epoch [8/10], Step [458/625], Loss: 1.1560\n",
      "Epoch [8/10], Step [459/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [460/625], Loss: 1.5959\n",
      "Epoch [8/10], Step [461/625], Loss: 1.6545\n",
      "Epoch [8/10], Step [462/625], Loss: 1.4049\n",
      "Epoch [8/10], Step [463/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [464/625], Loss: 1.5424\n",
      "Epoch [8/10], Step [465/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [466/625], Loss: 1.6381\n",
      "Epoch [8/10], Step [467/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [468/625], Loss: 1.6062\n",
      "Epoch [8/10], Step [469/625], Loss: 1.8485\n",
      "Epoch [8/10], Step [470/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [471/625], Loss: 1.6544\n",
      "Epoch [8/10], Step [472/625], Loss: 1.4049\n",
      "Epoch [8/10], Step [473/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [474/625], Loss: 1.6541\n",
      "Epoch [8/10], Step [475/625], Loss: 1.1546\n",
      "Epoch [8/10], Step [476/625], Loss: 1.4072\n",
      "Epoch [8/10], Step [477/625], Loss: 1.4880\n",
      "Epoch [8/10], Step [478/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [479/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [480/625], Loss: 1.3593\n",
      "Epoch [8/10], Step [481/625], Loss: 1.3336\n",
      "Epoch [8/10], Step [482/625], Loss: 1.3178\n",
      "Epoch [8/10], Step [483/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [484/625], Loss: 1.9048\n",
      "Epoch [8/10], Step [485/625], Loss: 1.1510\n",
      "Epoch [8/10], Step [486/625], Loss: 0.9067\n",
      "Epoch [8/10], Step [487/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [488/625], Loss: 0.9110\n",
      "Epoch [8/10], Step [489/625], Loss: 1.6310\n",
      "Epoch [8/10], Step [490/625], Loss: 1.6547\n",
      "Epoch [8/10], Step [491/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [492/625], Loss: 1.1559\n",
      "Epoch [8/10], Step [493/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [494/625], Loss: 1.2174\n",
      "Epoch [8/10], Step [495/625], Loss: 1.6290\n",
      "Epoch [8/10], Step [496/625], Loss: 1.1569\n",
      "Epoch [8/10], Step [497/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [498/625], Loss: 1.2458\n",
      "Epoch [8/10], Step [499/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [500/625], Loss: 1.3650\n",
      "Epoch [8/10], Step [501/625], Loss: 1.1445\n",
      "Epoch [8/10], Step [502/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [503/625], Loss: 1.1263\n",
      "Epoch [8/10], Step [504/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [505/625], Loss: 1.4693\n",
      "Epoch [8/10], Step [506/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [507/625], Loss: 0.9051\n",
      "Epoch [8/10], Step [508/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [509/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [510/625], Loss: 1.2412\n",
      "Epoch [8/10], Step [511/625], Loss: 0.9251\n",
      "Epoch [8/10], Step [512/625], Loss: 1.5138\n",
      "Epoch [8/10], Step [513/625], Loss: 1.3203\n",
      "Epoch [8/10], Step [514/625], Loss: 1.5586\n",
      "Epoch [8/10], Step [515/625], Loss: 1.2659\n",
      "Epoch [8/10], Step [516/625], Loss: 1.6337\n",
      "Epoch [8/10], Step [517/625], Loss: 1.3473\n",
      "Epoch [8/10], Step [518/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [519/625], Loss: 1.1446\n",
      "Epoch [8/10], Step [520/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [521/625], Loss: 1.1612\n",
      "Epoch [8/10], Step [522/625], Loss: 1.1752\n",
      "Epoch [8/10], Step [523/625], Loss: 1.0722\n",
      "Epoch [8/10], Step [524/625], Loss: 0.9051\n",
      "Epoch [8/10], Step [525/625], Loss: 1.1545\n",
      "Epoch [8/10], Step [526/625], Loss: 1.4011\n",
      "Epoch [8/10], Step [527/625], Loss: 1.1546\n",
      "Epoch [8/10], Step [528/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [529/625], Loss: 1.4050\n",
      "Epoch [8/10], Step [530/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [531/625], Loss: 1.4290\n",
      "Epoch [8/10], Step [532/625], Loss: 1.3969\n",
      "Epoch [8/10], Step [533/625], Loss: 1.5289\n",
      "Epoch [8/10], Step [534/625], Loss: 1.1307\n",
      "Epoch [8/10], Step [535/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [536/625], Loss: 1.4009\n",
      "Epoch [8/10], Step [537/625], Loss: 0.9085\n",
      "Epoch [8/10], Step [538/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [539/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [540/625], Loss: 1.2289\n",
      "Epoch [8/10], Step [541/625], Loss: 0.9670\n",
      "Epoch [8/10], Step [542/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [543/625], Loss: 1.3792\n",
      "Epoch [8/10], Step [544/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [545/625], Loss: 1.6288\n",
      "Epoch [8/10], Step [546/625], Loss: 1.4088\n",
      "Epoch [8/10], Step [547/625], Loss: 1.1549\n",
      "Epoch [8/10], Step [548/625], Loss: 1.3723\n",
      "Epoch [8/10], Step [549/625], Loss: 1.1101\n",
      "Epoch [8/10], Step [550/625], Loss: 0.9472\n",
      "Epoch [8/10], Step [551/625], Loss: 0.9250\n",
      "Epoch [8/10], Step [552/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [553/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [554/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [555/625], Loss: 1.4000\n",
      "Epoch [8/10], Step [556/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [557/625], Loss: 1.6277\n",
      "Epoch [8/10], Step [558/625], Loss: 1.3974\n",
      "Epoch [8/10], Step [559/625], Loss: 1.6529\n",
      "Epoch [8/10], Step [560/625], Loss: 1.6496\n",
      "Epoch [8/10], Step [561/625], Loss: 1.6507\n",
      "Epoch [8/10], Step [562/625], Loss: 1.1736\n",
      "Epoch [8/10], Step [563/625], Loss: 1.8994\n",
      "Epoch [8/10], Step [564/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [565/625], Loss: 1.1427\n",
      "Epoch [8/10], Step [566/625], Loss: 1.8475\n",
      "Epoch [8/10], Step [567/625], Loss: 1.3616\n",
      "Epoch [8/10], Step [568/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [569/625], Loss: 1.3950\n",
      "Epoch [8/10], Step [570/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [571/625], Loss: 1.1509\n",
      "Epoch [8/10], Step [572/625], Loss: 0.9070\n",
      "Epoch [8/10], Step [573/625], Loss: 0.9062\n",
      "Epoch [8/10], Step [574/625], Loss: 1.3235\n",
      "Epoch [8/10], Step [575/625], Loss: 1.1594\n",
      "Epoch [8/10], Step [576/625], Loss: 1.5749\n",
      "Epoch [8/10], Step [577/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [578/625], Loss: 1.4059\n",
      "Epoch [8/10], Step [579/625], Loss: 1.3745\n",
      "Epoch [8/10], Step [580/625], Loss: 1.1998\n",
      "Epoch [8/10], Step [581/625], Loss: 1.5969\n",
      "Epoch [8/10], Step [582/625], Loss: 1.3429\n",
      "Epoch [8/10], Step [583/625], Loss: 1.1577\n",
      "Epoch [8/10], Step [584/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [585/625], Loss: 1.4984\n",
      "Epoch [8/10], Step [586/625], Loss: 1.6509\n",
      "Epoch [8/10], Step [587/625], Loss: 1.4865\n",
      "Epoch [8/10], Step [588/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [589/625], Loss: 1.3949\n",
      "Epoch [8/10], Step [590/625], Loss: 1.4013\n",
      "Epoch [8/10], Step [591/625], Loss: 1.4077\n",
      "Epoch [8/10], Step [592/625], Loss: 1.4048\n",
      "Epoch [8/10], Step [593/625], Loss: 0.9076\n",
      "Epoch [8/10], Step [594/625], Loss: 1.2285\n",
      "Epoch [8/10], Step [595/625], Loss: 1.1605\n",
      "Epoch [8/10], Step [596/625], Loss: 1.1555\n",
      "Epoch [8/10], Step [597/625], Loss: 0.9136\n",
      "Epoch [8/10], Step [598/625], Loss: 1.1506\n",
      "Epoch [8/10], Step [599/625], Loss: 0.9213\n",
      "Epoch [8/10], Step [600/625], Loss: 1.1540\n",
      "Epoch [8/10], Step [601/625], Loss: 1.1646\n",
      "Epoch [8/10], Step [602/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [603/625], Loss: 1.4118\n",
      "Epoch [8/10], Step [604/625], Loss: 1.1560\n",
      "Epoch [8/10], Step [605/625], Loss: 1.1550\n",
      "Epoch [8/10], Step [606/625], Loss: 0.9532\n",
      "Epoch [8/10], Step [607/625], Loss: 1.1450\n",
      "Epoch [8/10], Step [608/625], Loss: 1.2073\n",
      "Epoch [8/10], Step [609/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [610/625], Loss: 1.3543\n",
      "Epoch [8/10], Step [611/625], Loss: 1.6428\n",
      "Epoch [8/10], Step [612/625], Loss: 1.4047\n",
      "Epoch [8/10], Step [613/625], Loss: 1.1541\n",
      "Epoch [8/10], Step [614/625], Loss: 1.6548\n",
      "Epoch [8/10], Step [615/625], Loss: 1.1650\n",
      "Epoch [8/10], Step [616/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [617/625], Loss: 1.6538\n",
      "Epoch [8/10], Step [618/625], Loss: 1.4026\n",
      "Epoch [8/10], Step [619/625], Loss: 1.3869\n",
      "Epoch [8/10], Step [620/625], Loss: 0.9048\n",
      "Epoch [8/10], Step [621/625], Loss: 1.3389\n",
      "Epoch [8/10], Step [622/625], Loss: 1.0236\n",
      "Epoch [8/10], Step [623/625], Loss: 1.1548\n",
      "Epoch [8/10], Step [624/625], Loss: 0.9049\n",
      "Epoch [8/10], Step [625/625], Loss: 1.1565\n",
      "Epoch [9/10], Step [1/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [2/625], Loss: 1.3556\n",
      "Epoch [9/10], Step [3/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [4/625], Loss: 1.0828\n",
      "Epoch [9/10], Step [5/625], Loss: 1.6398\n",
      "Epoch [9/10], Step [6/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [7/625], Loss: 1.6389\n",
      "Epoch [9/10], Step [8/625], Loss: 1.1071\n",
      "Epoch [9/10], Step [9/625], Loss: 1.2064\n",
      "Epoch [9/10], Step [10/625], Loss: 1.1572\n",
      "Epoch [9/10], Step [11/625], Loss: 1.4745\n",
      "Epoch [9/10], Step [12/625], Loss: 1.1430\n",
      "Epoch [9/10], Step [13/625], Loss: 1.8620\n",
      "Epoch [9/10], Step [14/625], Loss: 1.4063\n",
      "Epoch [9/10], Step [15/625], Loss: 1.1673\n",
      "Epoch [9/10], Step [16/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [17/625], Loss: 1.6316\n",
      "Epoch [9/10], Step [18/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [19/625], Loss: 1.6547\n",
      "Epoch [9/10], Step [20/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [21/625], Loss: 1.3500\n",
      "Epoch [9/10], Step [22/625], Loss: 1.5784\n",
      "Epoch [9/10], Step [23/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [24/625], Loss: 1.0105\n",
      "Epoch [9/10], Step [25/625], Loss: 1.4032\n",
      "Epoch [9/10], Step [26/625], Loss: 1.4119\n",
      "Epoch [9/10], Step [27/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [28/625], Loss: 1.4039\n",
      "Epoch [9/10], Step [29/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [30/625], Loss: 1.6425\n",
      "Epoch [9/10], Step [31/625], Loss: 1.6350\n",
      "Epoch [9/10], Step [32/625], Loss: 1.6451\n",
      "Epoch [9/10], Step [33/625], Loss: 1.1329\n",
      "Epoch [9/10], Step [34/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [35/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [36/625], Loss: 0.9091\n",
      "Epoch [9/10], Step [37/625], Loss: 1.0959\n",
      "Epoch [9/10], Step [38/625], Loss: 1.6610\n",
      "Epoch [9/10], Step [39/625], Loss: 1.1551\n",
      "Epoch [9/10], Step [40/625], Loss: 1.6314\n",
      "Epoch [9/10], Step [41/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [42/625], Loss: 0.9052\n",
      "Epoch [9/10], Step [43/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [44/625], Loss: 1.6557\n",
      "Epoch [9/10], Step [45/625], Loss: 1.6298\n",
      "Epoch [9/10], Step [46/625], Loss: 1.3887\n",
      "Epoch [9/10], Step [47/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [48/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [49/625], Loss: 1.5164\n",
      "Epoch [9/10], Step [50/625], Loss: 0.9077\n",
      "Epoch [9/10], Step [51/625], Loss: 1.3421\n",
      "Epoch [9/10], Step [52/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [53/625], Loss: 1.1643\n",
      "Epoch [9/10], Step [54/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [55/625], Loss: 1.4911\n",
      "Epoch [9/10], Step [56/625], Loss: 0.9071\n",
      "Epoch [9/10], Step [57/625], Loss: 1.1577\n",
      "Epoch [9/10], Step [58/625], Loss: 1.6367\n",
      "Epoch [9/10], Step [59/625], Loss: 1.4086\n",
      "Epoch [9/10], Step [60/625], Loss: 1.2594\n",
      "Epoch [9/10], Step [61/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [62/625], Loss: 1.1588\n",
      "Epoch [9/10], Step [63/625], Loss: 1.4043\n",
      "Epoch [9/10], Step [64/625], Loss: 1.3409\n",
      "Epoch [9/10], Step [65/625], Loss: 1.1467\n",
      "Epoch [9/10], Step [66/625], Loss: 1.1467\n",
      "Epoch [9/10], Step [67/625], Loss: 1.3921\n",
      "Epoch [9/10], Step [68/625], Loss: 1.2025\n",
      "Epoch [9/10], Step [69/625], Loss: 1.1541\n",
      "Epoch [9/10], Step [70/625], Loss: 1.8653\n",
      "Epoch [9/10], Step [71/625], Loss: 1.3344\n",
      "Epoch [9/10], Step [72/625], Loss: 1.1729\n",
      "Epoch [9/10], Step [73/625], Loss: 1.1772\n",
      "Epoch [9/10], Step [74/625], Loss: 1.8889\n",
      "Epoch [9/10], Step [75/625], Loss: 1.4149\n",
      "Epoch [9/10], Step [76/625], Loss: 0.9051\n",
      "Epoch [9/10], Step [77/625], Loss: 1.4039\n",
      "Epoch [9/10], Step [78/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [79/625], Loss: 1.4037\n",
      "Epoch [9/10], Step [80/625], Loss: 1.1536\n",
      "Epoch [9/10], Step [81/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [82/625], Loss: 1.1562\n",
      "Epoch [9/10], Step [83/625], Loss: 1.0992\n",
      "Epoch [9/10], Step [84/625], Loss: 1.1463\n",
      "Epoch [9/10], Step [85/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [86/625], Loss: 1.1555\n",
      "Epoch [9/10], Step [87/625], Loss: 0.9050\n",
      "Epoch [9/10], Step [88/625], Loss: 0.9175\n",
      "Epoch [9/10], Step [89/625], Loss: 1.2628\n",
      "Epoch [9/10], Step [90/625], Loss: 1.3277\n",
      "Epoch [9/10], Step [91/625], Loss: 1.8093\n",
      "Epoch [9/10], Step [92/625], Loss: 1.1551\n",
      "Epoch [9/10], Step [93/625], Loss: 0.9647\n",
      "Epoch [9/10], Step [94/625], Loss: 1.4013\n",
      "Epoch [9/10], Step [95/625], Loss: 1.8461\n",
      "Epoch [9/10], Step [96/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [97/625], Loss: 1.1495\n",
      "Epoch [9/10], Step [98/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [99/625], Loss: 0.9054\n",
      "Epoch [9/10], Step [100/625], Loss: 1.5841\n",
      "Epoch [9/10], Step [101/625], Loss: 0.9112\n",
      "Epoch [9/10], Step [102/625], Loss: 1.2035\n",
      "Epoch [9/10], Step [103/625], Loss: 1.1276\n",
      "Epoch [9/10], Step [104/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [105/625], Loss: 1.1568\n",
      "Epoch [9/10], Step [106/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [107/625], Loss: 1.0321\n",
      "Epoch [9/10], Step [108/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [109/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [110/625], Loss: 1.2037\n",
      "Epoch [9/10], Step [111/625], Loss: 1.3994\n",
      "Epoch [9/10], Step [112/625], Loss: 1.3880\n",
      "Epoch [9/10], Step [113/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [114/625], Loss: 1.9047\n",
      "Epoch [9/10], Step [115/625], Loss: 1.4054\n",
      "Epoch [9/10], Step [116/625], Loss: 1.5907\n",
      "Epoch [9/10], Step [117/625], Loss: 1.5835\n",
      "Epoch [9/10], Step [118/625], Loss: 1.3855\n",
      "Epoch [9/10], Step [119/625], Loss: 0.9056\n",
      "Epoch [9/10], Step [120/625], Loss: 1.1552\n",
      "Epoch [9/10], Step [121/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [122/625], Loss: 1.6545\n",
      "Epoch [9/10], Step [123/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [124/625], Loss: 1.2779\n",
      "Epoch [9/10], Step [125/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [126/625], Loss: 0.9392\n",
      "Epoch [9/10], Step [127/625], Loss: 1.6085\n",
      "Epoch [9/10], Step [128/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [129/625], Loss: 1.1663\n",
      "Epoch [9/10], Step [130/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [131/625], Loss: 1.3921\n",
      "Epoch [9/10], Step [132/625], Loss: 1.1563\n",
      "Epoch [9/10], Step [133/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [134/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [135/625], Loss: 1.4950\n",
      "Epoch [9/10], Step [136/625], Loss: 1.1555\n",
      "Epoch [9/10], Step [137/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [138/625], Loss: 1.1429\n",
      "Epoch [9/10], Step [139/625], Loss: 1.6546\n",
      "Epoch [9/10], Step [140/625], Loss: 1.0874\n",
      "Epoch [9/10], Step [141/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [142/625], Loss: 1.1546\n",
      "Epoch [9/10], Step [143/625], Loss: 1.1560\n",
      "Epoch [9/10], Step [144/625], Loss: 1.3333\n",
      "Epoch [9/10], Step [145/625], Loss: 1.6539\n",
      "Epoch [9/10], Step [146/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [147/625], Loss: 1.1107\n",
      "Epoch [9/10], Step [148/625], Loss: 1.8870\n",
      "Epoch [9/10], Step [149/625], Loss: 1.6542\n",
      "Epoch [9/10], Step [150/625], Loss: 0.9156\n",
      "Epoch [9/10], Step [151/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [152/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [153/625], Loss: 1.4057\n",
      "Epoch [9/10], Step [154/625], Loss: 1.4005\n",
      "Epoch [9/10], Step [155/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [156/625], Loss: 1.1572\n",
      "Epoch [9/10], Step [157/625], Loss: 1.1499\n",
      "Epoch [9/10], Step [158/625], Loss: 1.1742\n",
      "Epoch [9/10], Step [159/625], Loss: 1.1483\n",
      "Epoch [9/10], Step [160/625], Loss: 1.4094\n",
      "Epoch [9/10], Step [161/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [162/625], Loss: 1.0283\n",
      "Epoch [9/10], Step [163/625], Loss: 1.6549\n",
      "Epoch [9/10], Step [164/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [165/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [166/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [167/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [168/625], Loss: 1.2676\n",
      "Epoch [9/10], Step [169/625], Loss: 1.7518\n",
      "Epoch [9/10], Step [170/625], Loss: 1.6670\n",
      "Epoch [9/10], Step [171/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [172/625], Loss: 1.0828\n",
      "Epoch [9/10], Step [173/625], Loss: 1.4014\n",
      "Epoch [9/10], Step [174/625], Loss: 1.6517\n",
      "Epoch [9/10], Step [175/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [176/625], Loss: 1.1680\n",
      "Epoch [9/10], Step [177/625], Loss: 1.1550\n",
      "Epoch [9/10], Step [178/625], Loss: 0.9051\n",
      "Epoch [9/10], Step [179/625], Loss: 1.3625\n",
      "Epoch [9/10], Step [180/625], Loss: 1.3667\n",
      "Epoch [9/10], Step [181/625], Loss: 1.1557\n",
      "Epoch [9/10], Step [182/625], Loss: 1.4001\n",
      "Epoch [9/10], Step [183/625], Loss: 1.3989\n",
      "Epoch [9/10], Step [184/625], Loss: 1.8759\n",
      "Epoch [9/10], Step [185/625], Loss: 1.1551\n",
      "Epoch [9/10], Step [186/625], Loss: 1.4011\n",
      "Epoch [9/10], Step [187/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [188/625], Loss: 1.1545\n",
      "Epoch [9/10], Step [189/625], Loss: 1.8619\n",
      "Epoch [9/10], Step [190/625], Loss: 1.1583\n",
      "Epoch [9/10], Step [191/625], Loss: 1.8978\n",
      "Epoch [9/10], Step [192/625], Loss: 1.0484\n",
      "Epoch [9/10], Step [193/625], Loss: 1.4322\n",
      "Epoch [9/10], Step [194/625], Loss: 1.0809\n",
      "Epoch [9/10], Step [195/625], Loss: 1.1660\n",
      "Epoch [9/10], Step [196/625], Loss: 1.7849\n",
      "Epoch [9/10], Step [197/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [198/625], Loss: 1.6541\n",
      "Epoch [9/10], Step [199/625], Loss: 1.2574\n",
      "Epoch [9/10], Step [200/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [201/625], Loss: 1.1538\n",
      "Epoch [9/10], Step [202/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [203/625], Loss: 0.9238\n",
      "Epoch [9/10], Step [204/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [205/625], Loss: 1.6529\n",
      "Epoch [9/10], Step [206/625], Loss: 1.2671\n",
      "Epoch [9/10], Step [207/625], Loss: 1.4843\n",
      "Epoch [9/10], Step [208/625], Loss: 1.6553\n",
      "Epoch [9/10], Step [209/625], Loss: 1.1420\n",
      "Epoch [9/10], Step [210/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [211/625], Loss: 1.3953\n",
      "Epoch [9/10], Step [212/625], Loss: 1.1568\n",
      "Epoch [9/10], Step [213/625], Loss: 1.4050\n",
      "Epoch [9/10], Step [214/625], Loss: 1.6541\n",
      "Epoch [9/10], Step [215/625], Loss: 1.3179\n",
      "Epoch [9/10], Step [216/625], Loss: 1.4065\n",
      "Epoch [9/10], Step [217/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [218/625], Loss: 1.6539\n",
      "Epoch [9/10], Step [219/625], Loss: 1.3593\n",
      "Epoch [9/10], Step [220/625], Loss: 1.1540\n",
      "Epoch [9/10], Step [221/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [222/625], Loss: 1.4017\n",
      "Epoch [9/10], Step [223/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [224/625], Loss: 1.2738\n",
      "Epoch [9/10], Step [225/625], Loss: 1.3969\n",
      "Epoch [9/10], Step [226/625], Loss: 1.3832\n",
      "Epoch [9/10], Step [227/625], Loss: 1.6155\n",
      "Epoch [9/10], Step [228/625], Loss: 0.9176\n",
      "Epoch [9/10], Step [229/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [230/625], Loss: 1.6655\n",
      "Epoch [9/10], Step [231/625], Loss: 1.6529\n",
      "Epoch [9/10], Step [232/625], Loss: 0.9119\n",
      "Epoch [9/10], Step [233/625], Loss: 1.2386\n",
      "Epoch [9/10], Step [234/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [235/625], Loss: 1.3346\n",
      "Epoch [9/10], Step [236/625], Loss: 0.9228\n",
      "Epoch [9/10], Step [237/625], Loss: 1.4746\n",
      "Epoch [9/10], Step [238/625], Loss: 1.4843\n",
      "Epoch [9/10], Step [239/625], Loss: 1.1582\n",
      "Epoch [9/10], Step [240/625], Loss: 0.9852\n",
      "Epoch [9/10], Step [241/625], Loss: 1.0534\n",
      "Epoch [9/10], Step [242/625], Loss: 1.1538\n",
      "Epoch [9/10], Step [243/625], Loss: 1.2233\n",
      "Epoch [9/10], Step [244/625], Loss: 1.3974\n",
      "Epoch [9/10], Step [245/625], Loss: 1.4130\n",
      "Epoch [9/10], Step [246/625], Loss: 1.6708\n",
      "Epoch [9/10], Step [247/625], Loss: 1.4037\n",
      "Epoch [9/10], Step [248/625], Loss: 1.1201\n",
      "Epoch [9/10], Step [249/625], Loss: 0.9105\n",
      "Epoch [9/10], Step [250/625], Loss: 1.4024\n",
      "Epoch [9/10], Step [251/625], Loss: 1.1583\n",
      "Epoch [9/10], Step [252/625], Loss: 1.1515\n",
      "Epoch [9/10], Step [253/625], Loss: 0.9310\n",
      "Epoch [9/10], Step [254/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [255/625], Loss: 1.4005\n",
      "Epoch [9/10], Step [256/625], Loss: 1.2513\n",
      "Epoch [9/10], Step [257/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [258/625], Loss: 1.6657\n",
      "Epoch [9/10], Step [259/625], Loss: 1.1553\n",
      "Epoch [9/10], Step [260/625], Loss: 1.9045\n",
      "Epoch [9/10], Step [261/625], Loss: 1.1607\n",
      "Epoch [9/10], Step [262/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [263/625], Loss: 1.1530\n",
      "Epoch [9/10], Step [264/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [265/625], Loss: 1.4045\n",
      "Epoch [9/10], Step [266/625], Loss: 1.3775\n",
      "Epoch [9/10], Step [267/625], Loss: 1.4043\n",
      "Epoch [9/10], Step [268/625], Loss: 1.4029\n",
      "Epoch [9/10], Step [269/625], Loss: 1.8848\n",
      "Epoch [9/10], Step [270/625], Loss: 1.2449\n",
      "Epoch [9/10], Step [271/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [272/625], Loss: 1.1971\n",
      "Epoch [9/10], Step [273/625], Loss: 1.2913\n",
      "Epoch [9/10], Step [274/625], Loss: 1.4002\n",
      "Epoch [9/10], Step [275/625], Loss: 1.1547\n",
      "Epoch [9/10], Step [276/625], Loss: 0.9111\n",
      "Epoch [9/10], Step [277/625], Loss: 1.1828\n",
      "Epoch [9/10], Step [278/625], Loss: 1.4033\n",
      "Epoch [9/10], Step [279/625], Loss: 1.3891\n",
      "Epoch [9/10], Step [280/625], Loss: 1.1470\n",
      "Epoch [9/10], Step [281/625], Loss: 1.1938\n",
      "Epoch [9/10], Step [282/625], Loss: 1.4007\n",
      "Epoch [9/10], Step [283/625], Loss: 0.9059\n",
      "Epoch [9/10], Step [284/625], Loss: 1.0016\n",
      "Epoch [9/10], Step [285/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [286/625], Loss: 1.6547\n",
      "Epoch [9/10], Step [287/625], Loss: 1.6475\n",
      "Epoch [9/10], Step [288/625], Loss: 0.9103\n",
      "Epoch [9/10], Step [289/625], Loss: 0.9053\n",
      "Epoch [9/10], Step [290/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [291/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [292/625], Loss: 1.3461\n",
      "Epoch [9/10], Step [293/625], Loss: 1.1550\n",
      "Epoch [9/10], Step [294/625], Loss: 1.4044\n",
      "Epoch [9/10], Step [295/625], Loss: 1.3641\n",
      "Epoch [9/10], Step [296/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [297/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [298/625], Loss: 1.6546\n",
      "Epoch [9/10], Step [299/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [300/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [301/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [302/625], Loss: 1.1640\n",
      "Epoch [9/10], Step [303/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [304/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [305/625], Loss: 1.1828\n",
      "Epoch [9/10], Step [306/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [307/625], Loss: 1.5979\n",
      "Epoch [9/10], Step [308/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [309/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [310/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [311/625], Loss: 1.4038\n",
      "Epoch [9/10], Step [312/625], Loss: 0.9069\n",
      "Epoch [9/10], Step [313/625], Loss: 0.9057\n",
      "Epoch [9/10], Step [314/625], Loss: 1.4642\n",
      "Epoch [9/10], Step [315/625], Loss: 1.6493\n",
      "Epoch [9/10], Step [316/625], Loss: 1.2889\n",
      "Epoch [9/10], Step [317/625], Loss: 0.9052\n",
      "Epoch [9/10], Step [318/625], Loss: 1.6510\n",
      "Epoch [9/10], Step [319/625], Loss: 1.4144\n",
      "Epoch [9/10], Step [320/625], Loss: 1.2575\n",
      "Epoch [9/10], Step [321/625], Loss: 1.3823\n",
      "Epoch [9/10], Step [322/625], Loss: 1.1547\n",
      "Epoch [9/10], Step [323/625], Loss: 0.9334\n",
      "Epoch [9/10], Step [324/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [325/625], Loss: 1.3970\n",
      "Epoch [9/10], Step [326/625], Loss: 1.1487\n",
      "Epoch [9/10], Step [327/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [328/625], Loss: 1.4035\n",
      "Epoch [9/10], Step [329/625], Loss: 1.6341\n",
      "Epoch [9/10], Step [330/625], Loss: 1.3939\n",
      "Epoch [9/10], Step [331/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [332/625], Loss: 1.1442\n",
      "Epoch [9/10], Step [333/625], Loss: 1.1599\n",
      "Epoch [9/10], Step [334/625], Loss: 1.3661\n",
      "Epoch [9/10], Step [335/625], Loss: 1.1560\n",
      "Epoch [9/10], Step [336/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [337/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [338/625], Loss: 1.1687\n",
      "Epoch [9/10], Step [339/625], Loss: 1.4051\n",
      "Epoch [9/10], Step [340/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [341/625], Loss: 1.3617\n",
      "Epoch [9/10], Step [342/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [343/625], Loss: 1.4021\n",
      "Epoch [9/10], Step [344/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [345/625], Loss: 1.5735\n",
      "Epoch [9/10], Step [346/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [347/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [348/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [349/625], Loss: 1.2151\n",
      "Epoch [9/10], Step [350/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [351/625], Loss: 1.3498\n",
      "Epoch [9/10], Step [352/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [353/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [354/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [355/625], Loss: 1.4051\n",
      "Epoch [9/10], Step [356/625], Loss: 1.3998\n",
      "Epoch [9/10], Step [357/625], Loss: 1.1565\n",
      "Epoch [9/10], Step [358/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [359/625], Loss: 1.3949\n",
      "Epoch [9/10], Step [360/625], Loss: 1.1539\n",
      "Epoch [9/10], Step [361/625], Loss: 1.2048\n",
      "Epoch [9/10], Step [362/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [363/625], Loss: 1.4043\n",
      "Epoch [9/10], Step [364/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [365/625], Loss: 1.3757\n",
      "Epoch [9/10], Step [366/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [367/625], Loss: 1.4210\n",
      "Epoch [9/10], Step [368/625], Loss: 1.4041\n",
      "Epoch [9/10], Step [369/625], Loss: 1.2082\n",
      "Epoch [9/10], Step [370/625], Loss: 1.6464\n",
      "Epoch [9/10], Step [371/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [372/625], Loss: 0.9146\n",
      "Epoch [9/10], Step [373/625], Loss: 0.9062\n",
      "Epoch [9/10], Step [374/625], Loss: 1.4026\n",
      "Epoch [9/10], Step [375/625], Loss: 1.3568\n",
      "Epoch [9/10], Step [376/625], Loss: 1.6305\n",
      "Epoch [9/10], Step [377/625], Loss: 1.4033\n",
      "Epoch [9/10], Step [378/625], Loss: 1.1590\n",
      "Epoch [9/10], Step [379/625], Loss: 1.3798\n",
      "Epoch [9/10], Step [380/625], Loss: 1.1540\n",
      "Epoch [9/10], Step [381/625], Loss: 1.6382\n",
      "Epoch [9/10], Step [382/625], Loss: 1.4057\n",
      "Epoch [9/10], Step [383/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [384/625], Loss: 1.4045\n",
      "Epoch [9/10], Step [385/625], Loss: 1.1489\n",
      "Epoch [9/10], Step [386/625], Loss: 1.4036\n",
      "Epoch [9/10], Step [387/625], Loss: 1.4021\n",
      "Epoch [9/10], Step [388/625], Loss: 0.9349\n",
      "Epoch [9/10], Step [389/625], Loss: 1.4021\n",
      "Epoch [9/10], Step [390/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [391/625], Loss: 1.3154\n",
      "Epoch [9/10], Step [392/625], Loss: 1.4201\n",
      "Epoch [9/10], Step [393/625], Loss: 1.1513\n",
      "Epoch [9/10], Step [394/625], Loss: 1.4690\n",
      "Epoch [9/10], Step [395/625], Loss: 1.6532\n",
      "Epoch [9/10], Step [396/625], Loss: 1.3245\n",
      "Epoch [9/10], Step [397/625], Loss: 1.7501\n",
      "Epoch [9/10], Step [398/625], Loss: 1.4102\n",
      "Epoch [9/10], Step [399/625], Loss: 0.9213\n",
      "Epoch [9/10], Step [400/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [401/625], Loss: 1.1695\n",
      "Epoch [9/10], Step [402/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [403/625], Loss: 1.6412\n",
      "Epoch [9/10], Step [404/625], Loss: 1.3121\n",
      "Epoch [9/10], Step [405/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [406/625], Loss: 1.5967\n",
      "Epoch [9/10], Step [407/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [408/625], Loss: 1.4059\n",
      "Epoch [9/10], Step [409/625], Loss: 1.1551\n",
      "Epoch [9/10], Step [410/625], Loss: 1.8862\n",
      "Epoch [9/10], Step [411/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [412/625], Loss: 1.1755\n",
      "Epoch [9/10], Step [413/625], Loss: 0.9171\n",
      "Epoch [9/10], Step [414/625], Loss: 1.4046\n",
      "Epoch [9/10], Step [415/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [416/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [417/625], Loss: 1.1483\n",
      "Epoch [9/10], Step [418/625], Loss: 0.9567\n",
      "Epoch [9/10], Step [419/625], Loss: 1.2414\n",
      "Epoch [9/10], Step [420/625], Loss: 1.4001\n",
      "Epoch [9/10], Step [421/625], Loss: 1.0339\n",
      "Epoch [9/10], Step [422/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [423/625], Loss: 1.1429\n",
      "Epoch [9/10], Step [424/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [425/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [426/625], Loss: 1.6547\n",
      "Epoch [9/10], Step [427/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [428/625], Loss: 1.5824\n",
      "Epoch [9/10], Step [429/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [430/625], Loss: 1.1553\n",
      "Epoch [9/10], Step [431/625], Loss: 1.6236\n",
      "Epoch [9/10], Step [432/625], Loss: 1.6548\n",
      "Epoch [9/10], Step [433/625], Loss: 1.4028\n",
      "Epoch [9/10], Step [434/625], Loss: 1.1640\n",
      "Epoch [9/10], Step [435/625], Loss: 1.1559\n",
      "Epoch [9/10], Step [436/625], Loss: 0.9050\n",
      "Epoch [9/10], Step [437/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [438/625], Loss: 1.4046\n",
      "Epoch [9/10], Step [439/625], Loss: 0.9111\n",
      "Epoch [9/10], Step [440/625], Loss: 1.1652\n",
      "Epoch [9/10], Step [441/625], Loss: 0.9423\n",
      "Epoch [9/10], Step [442/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [443/625], Loss: 1.0821\n",
      "Epoch [9/10], Step [444/625], Loss: 1.9048\n",
      "Epoch [9/10], Step [445/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [446/625], Loss: 1.1340\n",
      "Epoch [9/10], Step [447/625], Loss: 1.6546\n",
      "Epoch [9/10], Step [448/625], Loss: 0.9132\n",
      "Epoch [9/10], Step [449/625], Loss: 1.1341\n",
      "Epoch [9/10], Step [450/625], Loss: 1.1500\n",
      "Epoch [9/10], Step [451/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [452/625], Loss: 1.3533\n",
      "Epoch [9/10], Step [453/625], Loss: 1.6480\n",
      "Epoch [9/10], Step [454/625], Loss: 1.3918\n",
      "Epoch [9/10], Step [455/625], Loss: 1.1510\n",
      "Epoch [9/10], Step [456/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [457/625], Loss: 1.1154\n",
      "Epoch [9/10], Step [458/625], Loss: 1.4052\n",
      "Epoch [9/10], Step [459/625], Loss: 1.1456\n",
      "Epoch [9/10], Step [460/625], Loss: 1.3029\n",
      "Epoch [9/10], Step [461/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [462/625], Loss: 1.1551\n",
      "Epoch [9/10], Step [463/625], Loss: 1.9048\n",
      "Epoch [9/10], Step [464/625], Loss: 1.3790\n",
      "Epoch [9/10], Step [465/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [466/625], Loss: 1.4045\n",
      "Epoch [9/10], Step [467/625], Loss: 1.4242\n",
      "Epoch [9/10], Step [468/625], Loss: 1.1540\n",
      "Epoch [9/10], Step [469/625], Loss: 1.1615\n",
      "Epoch [9/10], Step [470/625], Loss: 1.1550\n",
      "Epoch [9/10], Step [471/625], Loss: 1.5111\n",
      "Epoch [9/10], Step [472/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [473/625], Loss: 1.1369\n",
      "Epoch [9/10], Step [474/625], Loss: 1.4012\n",
      "Epoch [9/10], Step [475/625], Loss: 1.3972\n",
      "Epoch [9/10], Step [476/625], Loss: 1.3964\n",
      "Epoch [9/10], Step [477/625], Loss: 1.1536\n",
      "Epoch [9/10], Step [478/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [479/625], Loss: 1.2249\n",
      "Epoch [9/10], Step [480/625], Loss: 1.4038\n",
      "Epoch [9/10], Step [481/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [482/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [483/625], Loss: 1.1570\n",
      "Epoch [9/10], Step [484/625], Loss: 1.6239\n",
      "Epoch [9/10], Step [485/625], Loss: 1.1557\n",
      "Epoch [9/10], Step [486/625], Loss: 1.1467\n",
      "Epoch [9/10], Step [487/625], Loss: 1.3926\n",
      "Epoch [9/10], Step [488/625], Loss: 0.9110\n",
      "Epoch [9/10], Step [489/625], Loss: 1.3893\n",
      "Epoch [9/10], Step [490/625], Loss: 1.1637\n",
      "Epoch [9/10], Step [491/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [492/625], Loss: 1.3982\n",
      "Epoch [9/10], Step [493/625], Loss: 1.3867\n",
      "Epoch [9/10], Step [494/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [495/625], Loss: 1.1546\n",
      "Epoch [9/10], Step [496/625], Loss: 1.1725\n",
      "Epoch [9/10], Step [497/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [498/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [499/625], Loss: 1.4084\n",
      "Epoch [9/10], Step [500/625], Loss: 1.3713\n",
      "Epoch [9/10], Step [501/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [502/625], Loss: 1.0405\n",
      "Epoch [9/10], Step [503/625], Loss: 1.6524\n",
      "Epoch [9/10], Step [504/625], Loss: 1.6391\n",
      "Epoch [9/10], Step [505/625], Loss: 1.3905\n",
      "Epoch [9/10], Step [506/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [507/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [508/625], Loss: 1.1530\n",
      "Epoch [9/10], Step [509/625], Loss: 0.9062\n",
      "Epoch [9/10], Step [510/625], Loss: 1.4221\n",
      "Epoch [9/10], Step [511/625], Loss: 1.1524\n",
      "Epoch [9/10], Step [512/625], Loss: 1.1568\n",
      "Epoch [9/10], Step [513/625], Loss: 0.9052\n",
      "Epoch [9/10], Step [514/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [515/625], Loss: 1.4084\n",
      "Epoch [9/10], Step [516/625], Loss: 1.1588\n",
      "Epoch [9/10], Step [517/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [518/625], Loss: 1.4115\n",
      "Epoch [9/10], Step [519/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [520/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [521/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [522/625], Loss: 1.3749\n",
      "Epoch [9/10], Step [523/625], Loss: 1.1813\n",
      "Epoch [9/10], Step [524/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [525/625], Loss: 1.4024\n",
      "Epoch [9/10], Step [526/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [527/625], Loss: 1.4043\n",
      "Epoch [9/10], Step [528/625], Loss: 1.3796\n",
      "Epoch [9/10], Step [529/625], Loss: 1.4697\n",
      "Epoch [9/10], Step [530/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [531/625], Loss: 1.6551\n",
      "Epoch [9/10], Step [532/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [533/625], Loss: 1.6042\n",
      "Epoch [9/10], Step [534/625], Loss: 1.4036\n",
      "Epoch [9/10], Step [535/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [536/625], Loss: 1.2697\n",
      "Epoch [9/10], Step [537/625], Loss: 1.3995\n",
      "Epoch [9/10], Step [538/625], Loss: 1.5367\n",
      "Epoch [9/10], Step [539/625], Loss: 1.6373\n",
      "Epoch [9/10], Step [540/625], Loss: 1.3969\n",
      "Epoch [9/10], Step [541/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [542/625], Loss: 1.4039\n",
      "Epoch [9/10], Step [543/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [544/625], Loss: 0.9596\n",
      "Epoch [9/10], Step [545/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [546/625], Loss: 1.6116\n",
      "Epoch [9/10], Step [547/625], Loss: 1.3953\n",
      "Epoch [9/10], Step [548/625], Loss: 1.0703\n",
      "Epoch [9/10], Step [549/625], Loss: 1.6424\n",
      "Epoch [9/10], Step [550/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [551/625], Loss: 1.4062\n",
      "Epoch [9/10], Step [552/625], Loss: 1.1526\n",
      "Epoch [9/10], Step [553/625], Loss: 1.3063\n",
      "Epoch [9/10], Step [554/625], Loss: 0.9425\n",
      "Epoch [9/10], Step [555/625], Loss: 1.1549\n",
      "Epoch [9/10], Step [556/625], Loss: 0.9053\n",
      "Epoch [9/10], Step [557/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [558/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [559/625], Loss: 1.2927\n",
      "Epoch [9/10], Step [560/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [561/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [562/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [563/625], Loss: 1.0118\n",
      "Epoch [9/10], Step [564/625], Loss: 1.3886\n",
      "Epoch [9/10], Step [565/625], Loss: 0.9089\n",
      "Epoch [9/10], Step [566/625], Loss: 1.4355\n",
      "Epoch [9/10], Step [567/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [568/625], Loss: 1.3906\n",
      "Epoch [9/10], Step [569/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [570/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [571/625], Loss: 1.6424\n",
      "Epoch [9/10], Step [572/625], Loss: 1.8967\n",
      "Epoch [9/10], Step [573/625], Loss: 1.6545\n",
      "Epoch [9/10], Step [574/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [575/625], Loss: 1.6191\n",
      "Epoch [9/10], Step [576/625], Loss: 0.9056\n",
      "Epoch [9/10], Step [577/625], Loss: 1.1725\n",
      "Epoch [9/10], Step [578/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [579/625], Loss: 0.9540\n",
      "Epoch [9/10], Step [580/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [581/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [582/625], Loss: 1.0543\n",
      "Epoch [9/10], Step [583/625], Loss: 0.9048\n",
      "Epoch [9/10], Step [584/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [585/625], Loss: 1.3305\n",
      "Epoch [9/10], Step [586/625], Loss: 1.4047\n",
      "Epoch [9/10], Step [587/625], Loss: 1.6311\n",
      "Epoch [9/10], Step [588/625], Loss: 1.3873\n",
      "Epoch [9/10], Step [589/625], Loss: 1.4202\n",
      "Epoch [9/10], Step [590/625], Loss: 1.0825\n",
      "Epoch [9/10], Step [591/625], Loss: 1.4049\n",
      "Epoch [9/10], Step [592/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [593/625], Loss: 1.2619\n",
      "Epoch [9/10], Step [594/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [595/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [596/625], Loss: 1.2471\n",
      "Epoch [9/10], Step [597/625], Loss: 1.3944\n",
      "Epoch [9/10], Step [598/625], Loss: 1.1643\n",
      "Epoch [9/10], Step [599/625], Loss: 1.4005\n",
      "Epoch [9/10], Step [600/625], Loss: 1.9034\n",
      "Epoch [9/10], Step [601/625], Loss: 1.8923\n",
      "Epoch [9/10], Step [602/625], Loss: 1.4493\n",
      "Epoch [9/10], Step [603/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [604/625], Loss: 1.1548\n",
      "Epoch [9/10], Step [605/625], Loss: 1.2079\n",
      "Epoch [9/10], Step [606/625], Loss: 1.3935\n",
      "Epoch [9/10], Step [607/625], Loss: 1.1520\n",
      "Epoch [9/10], Step [608/625], Loss: 1.4034\n",
      "Epoch [9/10], Step [609/625], Loss: 1.1496\n",
      "Epoch [9/10], Step [610/625], Loss: 1.1501\n",
      "Epoch [9/10], Step [611/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [612/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [613/625], Loss: 1.6398\n",
      "Epoch [9/10], Step [614/625], Loss: 1.6541\n",
      "Epoch [9/10], Step [615/625], Loss: 1.3977\n",
      "Epoch [9/10], Step [616/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [617/625], Loss: 1.4048\n",
      "Epoch [9/10], Step [618/625], Loss: 1.4016\n",
      "Epoch [9/10], Step [619/625], Loss: 1.3891\n",
      "Epoch [9/10], Step [620/625], Loss: 1.4045\n",
      "Epoch [9/10], Step [621/625], Loss: 1.3012\n",
      "Epoch [9/10], Step [622/625], Loss: 1.4179\n",
      "Epoch [9/10], Step [623/625], Loss: 0.9049\n",
      "Epoch [9/10], Step [624/625], Loss: 1.3735\n",
      "Epoch [9/10], Step [625/625], Loss: 0.9303\n",
      "Epoch [10/10], Step [1/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [2/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [3/625], Loss: 0.9070\n",
      "Epoch [10/10], Step [4/625], Loss: 1.4045\n",
      "Epoch [10/10], Step [5/625], Loss: 1.1760\n",
      "Epoch [10/10], Step [6/625], Loss: 1.0954\n",
      "Epoch [10/10], Step [7/625], Loss: 1.1591\n",
      "Epoch [10/10], Step [8/625], Loss: 1.1542\n",
      "Epoch [10/10], Step [9/625], Loss: 1.0543\n",
      "Epoch [10/10], Step [10/625], Loss: 1.4042\n",
      "Epoch [10/10], Step [11/625], Loss: 1.3983\n",
      "Epoch [10/10], Step [12/625], Loss: 1.4046\n",
      "Epoch [10/10], Step [13/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [14/625], Loss: 1.3700\n",
      "Epoch [10/10], Step [15/625], Loss: 0.9940\n",
      "Epoch [10/10], Step [16/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [17/625], Loss: 1.1518\n",
      "Epoch [10/10], Step [18/625], Loss: 1.4055\n",
      "Epoch [10/10], Step [19/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [20/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [21/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [22/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [23/625], Loss: 1.1514\n",
      "Epoch [10/10], Step [24/625], Loss: 1.6547\n",
      "Epoch [10/10], Step [25/625], Loss: 1.4141\n",
      "Epoch [10/10], Step [26/625], Loss: 0.9053\n",
      "Epoch [10/10], Step [27/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [28/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [29/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [30/625], Loss: 1.4081\n",
      "Epoch [10/10], Step [31/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [32/625], Loss: 1.1529\n",
      "Epoch [10/10], Step [33/625], Loss: 1.3655\n",
      "Epoch [10/10], Step [34/625], Loss: 1.1398\n",
      "Epoch [10/10], Step [35/625], Loss: 1.5230\n",
      "Epoch [10/10], Step [36/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [37/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [38/625], Loss: 1.4049\n",
      "Epoch [10/10], Step [39/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [40/625], Loss: 1.4036\n",
      "Epoch [10/10], Step [41/625], Loss: 1.1588\n",
      "Epoch [10/10], Step [42/625], Loss: 1.4472\n",
      "Epoch [10/10], Step [43/625], Loss: 1.0945\n",
      "Epoch [10/10], Step [44/625], Loss: 1.1529\n",
      "Epoch [10/10], Step [45/625], Loss: 1.1560\n",
      "Epoch [10/10], Step [46/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [47/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [48/625], Loss: 1.1488\n",
      "Epoch [10/10], Step [49/625], Loss: 1.4268\n",
      "Epoch [10/10], Step [50/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [51/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [52/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [53/625], Loss: 1.1476\n",
      "Epoch [10/10], Step [54/625], Loss: 1.6257\n",
      "Epoch [10/10], Step [55/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [56/625], Loss: 1.4057\n",
      "Epoch [10/10], Step [57/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [58/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [59/625], Loss: 1.1315\n",
      "Epoch [10/10], Step [60/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [61/625], Loss: 1.6523\n",
      "Epoch [10/10], Step [62/625], Loss: 1.1241\n",
      "Epoch [10/10], Step [63/625], Loss: 1.6095\n",
      "Epoch [10/10], Step [64/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [65/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [66/625], Loss: 1.3430\n",
      "Epoch [10/10], Step [67/625], Loss: 1.0371\n",
      "Epoch [10/10], Step [68/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [69/625], Loss: 1.2557\n",
      "Epoch [10/10], Step [70/625], Loss: 0.9239\n",
      "Epoch [10/10], Step [71/625], Loss: 1.1539\n",
      "Epoch [10/10], Step [72/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [73/625], Loss: 1.3924\n",
      "Epoch [10/10], Step [74/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [75/625], Loss: 1.1439\n",
      "Epoch [10/10], Step [76/625], Loss: 0.9050\n",
      "Epoch [10/10], Step [77/625], Loss: 1.1728\n",
      "Epoch [10/10], Step [78/625], Loss: 1.1381\n",
      "Epoch [10/10], Step [79/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [80/625], Loss: 1.7307\n",
      "Epoch [10/10], Step [81/625], Loss: 1.2595\n",
      "Epoch [10/10], Step [82/625], Loss: 1.1600\n",
      "Epoch [10/10], Step [83/625], Loss: 1.3772\n",
      "Epoch [10/10], Step [84/625], Loss: 1.1547\n",
      "Epoch [10/10], Step [85/625], Loss: 1.1577\n",
      "Epoch [10/10], Step [86/625], Loss: 1.5169\n",
      "Epoch [10/10], Step [87/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [88/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [89/625], Loss: 1.7605\n",
      "Epoch [10/10], Step [90/625], Loss: 1.4046\n",
      "Epoch [10/10], Step [91/625], Loss: 1.4030\n",
      "Epoch [10/10], Step [92/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [93/625], Loss: 1.4031\n",
      "Epoch [10/10], Step [94/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [95/625], Loss: 1.4049\n",
      "Epoch [10/10], Step [96/625], Loss: 1.1332\n",
      "Epoch [10/10], Step [97/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [98/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [99/625], Loss: 1.6444\n",
      "Epoch [10/10], Step [100/625], Loss: 1.1808\n",
      "Epoch [10/10], Step [101/625], Loss: 0.9129\n",
      "Epoch [10/10], Step [102/625], Loss: 1.6517\n",
      "Epoch [10/10], Step [103/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [104/625], Loss: 0.9864\n",
      "Epoch [10/10], Step [105/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [106/625], Loss: 1.6529\n",
      "Epoch [10/10], Step [107/625], Loss: 1.3973\n",
      "Epoch [10/10], Step [108/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [109/625], Loss: 1.5593\n",
      "Epoch [10/10], Step [110/625], Loss: 1.3924\n",
      "Epoch [10/10], Step [111/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [112/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [113/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [114/625], Loss: 1.1611\n",
      "Epoch [10/10], Step [115/625], Loss: 1.4045\n",
      "Epoch [10/10], Step [116/625], Loss: 1.4025\n",
      "Epoch [10/10], Step [117/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [118/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [119/625], Loss: 1.4218\n",
      "Epoch [10/10], Step [120/625], Loss: 1.4009\n",
      "Epoch [10/10], Step [121/625], Loss: 1.3547\n",
      "Epoch [10/10], Step [122/625], Loss: 1.1633\n",
      "Epoch [10/10], Step [123/625], Loss: 1.1481\n",
      "Epoch [10/10], Step [124/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [125/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [126/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [127/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [128/625], Loss: 0.9078\n",
      "Epoch [10/10], Step [129/625], Loss: 1.1443\n",
      "Epoch [10/10], Step [130/625], Loss: 1.1553\n",
      "Epoch [10/10], Step [131/625], Loss: 1.4032\n",
      "Epoch [10/10], Step [132/625], Loss: 1.4037\n",
      "Epoch [10/10], Step [133/625], Loss: 1.4013\n",
      "Epoch [10/10], Step [134/625], Loss: 1.2808\n",
      "Epoch [10/10], Step [135/625], Loss: 1.6398\n",
      "Epoch [10/10], Step [136/625], Loss: 1.6342\n",
      "Epoch [10/10], Step [137/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [138/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [139/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [140/625], Loss: 1.1815\n",
      "Epoch [10/10], Step [141/625], Loss: 1.3785\n",
      "Epoch [10/10], Step [142/625], Loss: 1.1591\n",
      "Epoch [10/10], Step [143/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [144/625], Loss: 1.4122\n",
      "Epoch [10/10], Step [145/625], Loss: 1.4018\n",
      "Epoch [10/10], Step [146/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [147/625], Loss: 1.3921\n",
      "Epoch [10/10], Step [148/625], Loss: 1.5110\n",
      "Epoch [10/10], Step [149/625], Loss: 1.2364\n",
      "Epoch [10/10], Step [150/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [151/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [152/625], Loss: 1.8612\n",
      "Epoch [10/10], Step [153/625], Loss: 1.4055\n",
      "Epoch [10/10], Step [154/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [155/625], Loss: 1.6330\n",
      "Epoch [10/10], Step [156/625], Loss: 1.1552\n",
      "Epoch [10/10], Step [157/625], Loss: 0.9052\n",
      "Epoch [10/10], Step [158/625], Loss: 1.3908\n",
      "Epoch [10/10], Step [159/625], Loss: 1.1429\n",
      "Epoch [10/10], Step [160/625], Loss: 1.4016\n",
      "Epoch [10/10], Step [161/625], Loss: 1.6541\n",
      "Epoch [10/10], Step [162/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [163/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [164/625], Loss: 1.1334\n",
      "Epoch [10/10], Step [165/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [166/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [167/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [168/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [169/625], Loss: 1.1629\n",
      "Epoch [10/10], Step [170/625], Loss: 1.1820\n",
      "Epoch [10/10], Step [171/625], Loss: 1.4068\n",
      "Epoch [10/10], Step [172/625], Loss: 1.1761\n",
      "Epoch [10/10], Step [173/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [174/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [175/625], Loss: 1.1444\n",
      "Epoch [10/10], Step [176/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [177/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [178/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [179/625], Loss: 0.9061\n",
      "Epoch [10/10], Step [180/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [181/625], Loss: 1.3084\n",
      "Epoch [10/10], Step [182/625], Loss: 1.4049\n",
      "Epoch [10/10], Step [183/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [184/625], Loss: 1.1540\n",
      "Epoch [10/10], Step [185/625], Loss: 1.6625\n",
      "Epoch [10/10], Step [186/625], Loss: 1.3949\n",
      "Epoch [10/10], Step [187/625], Loss: 1.4045\n",
      "Epoch [10/10], Step [188/625], Loss: 1.3643\n",
      "Epoch [10/10], Step [189/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [190/625], Loss: 1.6193\n",
      "Epoch [10/10], Step [191/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [192/625], Loss: 1.1859\n",
      "Epoch [10/10], Step [193/625], Loss: 1.4128\n",
      "Epoch [10/10], Step [194/625], Loss: 1.3952\n",
      "Epoch [10/10], Step [195/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [196/625], Loss: 1.8985\n",
      "Epoch [10/10], Step [197/625], Loss: 1.5446\n",
      "Epoch [10/10], Step [198/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [199/625], Loss: 0.9051\n",
      "Epoch [10/10], Step [200/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [201/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [202/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [203/625], Loss: 1.1616\n",
      "Epoch [10/10], Step [204/625], Loss: 1.4103\n",
      "Epoch [10/10], Step [205/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [206/625], Loss: 1.1557\n",
      "Epoch [10/10], Step [207/625], Loss: 1.1661\n",
      "Epoch [10/10], Step [208/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [209/625], Loss: 1.1719\n",
      "Epoch [10/10], Step [210/625], Loss: 1.6202\n",
      "Epoch [10/10], Step [211/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [212/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [213/625], Loss: 1.4049\n",
      "Epoch [10/10], Step [214/625], Loss: 1.1543\n",
      "Epoch [10/10], Step [215/625], Loss: 1.5926\n",
      "Epoch [10/10], Step [216/625], Loss: 0.9051\n",
      "Epoch [10/10], Step [217/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [218/625], Loss: 1.1543\n",
      "Epoch [10/10], Step [219/625], Loss: 1.2074\n",
      "Epoch [10/10], Step [220/625], Loss: 0.9110\n",
      "Epoch [10/10], Step [221/625], Loss: 0.9454\n",
      "Epoch [10/10], Step [222/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [223/625], Loss: 1.6544\n",
      "Epoch [10/10], Step [224/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [225/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [226/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [227/625], Loss: 1.7907\n",
      "Epoch [10/10], Step [228/625], Loss: 1.4050\n",
      "Epoch [10/10], Step [229/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [230/625], Loss: 1.1547\n",
      "Epoch [10/10], Step [231/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [232/625], Loss: 1.1597\n",
      "Epoch [10/10], Step [233/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [234/625], Loss: 1.4038\n",
      "Epoch [10/10], Step [235/625], Loss: 1.4052\n",
      "Epoch [10/10], Step [236/625], Loss: 1.1556\n",
      "Epoch [10/10], Step [237/625], Loss: 1.1542\n",
      "Epoch [10/10], Step [238/625], Loss: 1.4032\n",
      "Epoch [10/10], Step [239/625], Loss: 1.1848\n",
      "Epoch [10/10], Step [240/625], Loss: 1.6392\n",
      "Epoch [10/10], Step [241/625], Loss: 1.6513\n",
      "Epoch [10/10], Step [242/625], Loss: 1.4038\n",
      "Epoch [10/10], Step [243/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [244/625], Loss: 1.4051\n",
      "Epoch [10/10], Step [245/625], Loss: 1.3976\n",
      "Epoch [10/10], Step [246/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [247/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [248/625], Loss: 1.1955\n",
      "Epoch [10/10], Step [249/625], Loss: 0.9083\n",
      "Epoch [10/10], Step [250/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [251/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [252/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [253/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [254/625], Loss: 1.4068\n",
      "Epoch [10/10], Step [255/625], Loss: 1.1695\n",
      "Epoch [10/10], Step [256/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [257/625], Loss: 1.6544\n",
      "Epoch [10/10], Step [258/625], Loss: 1.1512\n",
      "Epoch [10/10], Step [259/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [260/625], Loss: 1.1407\n",
      "Epoch [10/10], Step [261/625], Loss: 1.1198\n",
      "Epoch [10/10], Step [262/625], Loss: 1.1989\n",
      "Epoch [10/10], Step [263/625], Loss: 1.1537\n",
      "Epoch [10/10], Step [264/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [265/625], Loss: 1.6520\n",
      "Epoch [10/10], Step [266/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [267/625], Loss: 1.4024\n",
      "Epoch [10/10], Step [268/625], Loss: 1.6468\n",
      "Epoch [10/10], Step [269/625], Loss: 1.3725\n",
      "Epoch [10/10], Step [270/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [271/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [272/625], Loss: 1.6540\n",
      "Epoch [10/10], Step [273/625], Loss: 1.3465\n",
      "Epoch [10/10], Step [274/625], Loss: 1.6399\n",
      "Epoch [10/10], Step [275/625], Loss: 1.6429\n",
      "Epoch [10/10], Step [276/625], Loss: 1.6382\n",
      "Epoch [10/10], Step [277/625], Loss: 1.6349\n",
      "Epoch [10/10], Step [278/625], Loss: 1.6528\n",
      "Epoch [10/10], Step [279/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [280/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [281/625], Loss: 1.1502\n",
      "Epoch [10/10], Step [282/625], Loss: 0.9104\n",
      "Epoch [10/10], Step [283/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [284/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [285/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [286/625], Loss: 1.3958\n",
      "Epoch [10/10], Step [287/625], Loss: 1.4045\n",
      "Epoch [10/10], Step [288/625], Loss: 1.6450\n",
      "Epoch [10/10], Step [289/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [290/625], Loss: 1.1717\n",
      "Epoch [10/10], Step [291/625], Loss: 1.1609\n",
      "Epoch [10/10], Step [292/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [293/625], Loss: 1.6543\n",
      "Epoch [10/10], Step [294/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [295/625], Loss: 1.4003\n",
      "Epoch [10/10], Step [296/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [297/625], Loss: 1.6357\n",
      "Epoch [10/10], Step [298/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [299/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [300/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [301/625], Loss: 1.1629\n",
      "Epoch [10/10], Step [302/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [303/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [304/625], Loss: 0.9053\n",
      "Epoch [10/10], Step [305/625], Loss: 1.1826\n",
      "Epoch [10/10], Step [306/625], Loss: 0.9157\n",
      "Epoch [10/10], Step [307/625], Loss: 1.4044\n",
      "Epoch [10/10], Step [308/625], Loss: 1.4815\n",
      "Epoch [10/10], Step [309/625], Loss: 1.1214\n",
      "Epoch [10/10], Step [310/625], Loss: 1.3945\n",
      "Epoch [10/10], Step [311/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [312/625], Loss: 1.0746\n",
      "Epoch [10/10], Step [313/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [314/625], Loss: 1.4627\n",
      "Epoch [10/10], Step [315/625], Loss: 1.6539\n",
      "Epoch [10/10], Step [316/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [317/625], Loss: 1.6542\n",
      "Epoch [10/10], Step [318/625], Loss: 1.4076\n",
      "Epoch [10/10], Step [319/625], Loss: 1.6544\n",
      "Epoch [10/10], Step [320/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [321/625], Loss: 1.4036\n",
      "Epoch [10/10], Step [322/625], Loss: 0.9085\n",
      "Epoch [10/10], Step [323/625], Loss: 1.2269\n",
      "Epoch [10/10], Step [324/625], Loss: 1.4027\n",
      "Epoch [10/10], Step [325/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [326/625], Loss: 1.6006\n",
      "Epoch [10/10], Step [327/625], Loss: 1.6437\n",
      "Epoch [10/10], Step [328/625], Loss: 1.4064\n",
      "Epoch [10/10], Step [329/625], Loss: 1.4031\n",
      "Epoch [10/10], Step [330/625], Loss: 1.1463\n",
      "Epoch [10/10], Step [331/625], Loss: 1.1536\n",
      "Epoch [10/10], Step [332/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [333/625], Loss: 1.4013\n",
      "Epoch [10/10], Step [334/625], Loss: 1.1538\n",
      "Epoch [10/10], Step [335/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [336/625], Loss: 1.1545\n",
      "Epoch [10/10], Step [337/625], Loss: 1.3664\n",
      "Epoch [10/10], Step [338/625], Loss: 1.4029\n",
      "Epoch [10/10], Step [339/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [340/625], Loss: 1.1744\n",
      "Epoch [10/10], Step [341/625], Loss: 1.8942\n",
      "Epoch [10/10], Step [342/625], Loss: 1.5222\n",
      "Epoch [10/10], Step [343/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [344/625], Loss: 1.8324\n",
      "Epoch [10/10], Step [345/625], Loss: 1.6762\n",
      "Epoch [10/10], Step [346/625], Loss: 1.4142\n",
      "Epoch [10/10], Step [347/625], Loss: 1.4037\n",
      "Epoch [10/10], Step [348/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [349/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [350/625], Loss: 1.1551\n",
      "Epoch [10/10], Step [351/625], Loss: 1.1542\n",
      "Epoch [10/10], Step [352/625], Loss: 1.6544\n",
      "Epoch [10/10], Step [353/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [354/625], Loss: 1.6238\n",
      "Epoch [10/10], Step [355/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [356/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [357/625], Loss: 1.4038\n",
      "Epoch [10/10], Step [358/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [359/625], Loss: 1.3303\n",
      "Epoch [10/10], Step [360/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [361/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [362/625], Loss: 1.8843\n",
      "Epoch [10/10], Step [363/625], Loss: 1.4089\n",
      "Epoch [10/10], Step [364/625], Loss: 1.6387\n",
      "Epoch [10/10], Step [365/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [366/625], Loss: 1.4823\n",
      "Epoch [10/10], Step [367/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [368/625], Loss: 1.4096\n",
      "Epoch [10/10], Step [369/625], Loss: 1.4050\n",
      "Epoch [10/10], Step [370/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [371/625], Loss: 1.0661\n",
      "Epoch [10/10], Step [372/625], Loss: 1.1545\n",
      "Epoch [10/10], Step [373/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [374/625], Loss: 1.4078\n",
      "Epoch [10/10], Step [375/625], Loss: 1.6403\n",
      "Epoch [10/10], Step [376/625], Loss: 1.1771\n",
      "Epoch [10/10], Step [377/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [378/625], Loss: 1.4488\n",
      "Epoch [10/10], Step [379/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [380/625], Loss: 1.6550\n",
      "Epoch [10/10], Step [381/625], Loss: 1.0412\n",
      "Epoch [10/10], Step [382/625], Loss: 1.1948\n",
      "Epoch [10/10], Step [383/625], Loss: 1.5207\n",
      "Epoch [10/10], Step [384/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [385/625], Loss: 1.2146\n",
      "Epoch [10/10], Step [386/625], Loss: 1.6547\n",
      "Epoch [10/10], Step [387/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [388/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [389/625], Loss: 1.4570\n",
      "Epoch [10/10], Step [390/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [391/625], Loss: 1.6483\n",
      "Epoch [10/10], Step [392/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [393/625], Loss: 1.8870\n",
      "Epoch [10/10], Step [394/625], Loss: 1.1465\n",
      "Epoch [10/10], Step [395/625], Loss: 1.1419\n",
      "Epoch [10/10], Step [396/625], Loss: 1.1618\n",
      "Epoch [10/10], Step [397/625], Loss: 1.4138\n",
      "Epoch [10/10], Step [398/625], Loss: 1.0105\n",
      "Epoch [10/10], Step [399/625], Loss: 1.1529\n",
      "Epoch [10/10], Step [400/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [401/625], Loss: 1.4050\n",
      "Epoch [10/10], Step [402/625], Loss: 1.3465\n",
      "Epoch [10/10], Step [403/625], Loss: 1.4075\n",
      "Epoch [10/10], Step [404/625], Loss: 1.1520\n",
      "Epoch [10/10], Step [405/625], Loss: 1.6337\n",
      "Epoch [10/10], Step [406/625], Loss: 1.6547\n",
      "Epoch [10/10], Step [407/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [408/625], Loss: 1.1666\n",
      "Epoch [10/10], Step [409/625], Loss: 1.1598\n",
      "Epoch [10/10], Step [410/625], Loss: 1.1494\n",
      "Epoch [10/10], Step [411/625], Loss: 1.4037\n",
      "Epoch [10/10], Step [412/625], Loss: 1.1608\n",
      "Epoch [10/10], Step [413/625], Loss: 1.4017\n",
      "Epoch [10/10], Step [414/625], Loss: 1.3125\n",
      "Epoch [10/10], Step [415/625], Loss: 1.1402\n",
      "Epoch [10/10], Step [416/625], Loss: 1.1433\n",
      "Epoch [10/10], Step [417/625], Loss: 1.3714\n",
      "Epoch [10/10], Step [418/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [419/625], Loss: 1.1160\n",
      "Epoch [10/10], Step [420/625], Loss: 1.3937\n",
      "Epoch [10/10], Step [421/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [422/625], Loss: 1.3061\n",
      "Epoch [10/10], Step [423/625], Loss: 1.1475\n",
      "Epoch [10/10], Step [424/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [425/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [426/625], Loss: 1.6296\n",
      "Epoch [10/10], Step [427/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [428/625], Loss: 0.9119\n",
      "Epoch [10/10], Step [429/625], Loss: 1.4035\n",
      "Epoch [10/10], Step [430/625], Loss: 1.6541\n",
      "Epoch [10/10], Step [431/625], Loss: 1.6539\n",
      "Epoch [10/10], Step [432/625], Loss: 0.9303\n",
      "Epoch [10/10], Step [433/625], Loss: 1.4330\n",
      "Epoch [10/10], Step [434/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [435/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [436/625], Loss: 1.4643\n",
      "Epoch [10/10], Step [437/625], Loss: 1.6541\n",
      "Epoch [10/10], Step [438/625], Loss: 1.4073\n",
      "Epoch [10/10], Step [439/625], Loss: 0.9599\n",
      "Epoch [10/10], Step [440/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [441/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [442/625], Loss: 1.4046\n",
      "Epoch [10/10], Step [443/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [444/625], Loss: 1.3503\n",
      "Epoch [10/10], Step [445/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [446/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [447/625], Loss: 1.3749\n",
      "Epoch [10/10], Step [448/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [449/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [450/625], Loss: 0.9753\n",
      "Epoch [10/10], Step [451/625], Loss: 0.9063\n",
      "Epoch [10/10], Step [452/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [453/625], Loss: 1.4044\n",
      "Epoch [10/10], Step [454/625], Loss: 1.4052\n",
      "Epoch [10/10], Step [455/625], Loss: 1.1558\n",
      "Epoch [10/10], Step [456/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [457/625], Loss: 1.1552\n",
      "Epoch [10/10], Step [458/625], Loss: 1.3972\n",
      "Epoch [10/10], Step [459/625], Loss: 1.3240\n",
      "Epoch [10/10], Step [460/625], Loss: 1.4050\n",
      "Epoch [10/10], Step [461/625], Loss: 1.1488\n",
      "Epoch [10/10], Step [462/625], Loss: 1.1547\n",
      "Epoch [10/10], Step [463/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [464/625], Loss: 1.3967\n",
      "Epoch [10/10], Step [465/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [466/625], Loss: 0.9534\n",
      "Epoch [10/10], Step [467/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [468/625], Loss: 1.3888\n",
      "Epoch [10/10], Step [469/625], Loss: 1.3972\n",
      "Epoch [10/10], Step [470/625], Loss: 1.5258\n",
      "Epoch [10/10], Step [471/625], Loss: 1.6540\n",
      "Epoch [10/10], Step [472/625], Loss: 1.1555\n",
      "Epoch [10/10], Step [473/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [474/625], Loss: 1.6412\n",
      "Epoch [10/10], Step [475/625], Loss: 1.1570\n",
      "Epoch [10/10], Step [476/625], Loss: 1.4042\n",
      "Epoch [10/10], Step [477/625], Loss: 1.3869\n",
      "Epoch [10/10], Step [478/625], Loss: 1.6541\n",
      "Epoch [10/10], Step [479/625], Loss: 0.9077\n",
      "Epoch [10/10], Step [480/625], Loss: 1.6539\n",
      "Epoch [10/10], Step [481/625], Loss: 1.6432\n",
      "Epoch [10/10], Step [482/625], Loss: 1.9020\n",
      "Epoch [10/10], Step [483/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [484/625], Loss: 1.2190\n",
      "Epoch [10/10], Step [485/625], Loss: 1.6564\n",
      "Epoch [10/10], Step [486/625], Loss: 1.4103\n",
      "Epoch [10/10], Step [487/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [488/625], Loss: 1.1563\n",
      "Epoch [10/10], Step [489/625], Loss: 1.3758\n",
      "Epoch [10/10], Step [490/625], Loss: 1.4070\n",
      "Epoch [10/10], Step [491/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [492/625], Loss: 1.1600\n",
      "Epoch [10/10], Step [493/625], Loss: 1.4013\n",
      "Epoch [10/10], Step [494/625], Loss: 1.1541\n",
      "Epoch [10/10], Step [495/625], Loss: 1.4032\n",
      "Epoch [10/10], Step [496/625], Loss: 1.3896\n",
      "Epoch [10/10], Step [497/625], Loss: 1.1546\n",
      "Epoch [10/10], Step [498/625], Loss: 1.4036\n",
      "Epoch [10/10], Step [499/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [500/625], Loss: 0.9733\n",
      "Epoch [10/10], Step [501/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [502/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [503/625], Loss: 1.3477\n",
      "Epoch [10/10], Step [504/625], Loss: 0.9055\n",
      "Epoch [10/10], Step [505/625], Loss: 1.1571\n",
      "Epoch [10/10], Step [506/625], Loss: 0.9053\n",
      "Epoch [10/10], Step [507/625], Loss: 1.4541\n",
      "Epoch [10/10], Step [508/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [509/625], Loss: 1.3883\n",
      "Epoch [10/10], Step [510/625], Loss: 1.3140\n",
      "Epoch [10/10], Step [511/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [512/625], Loss: 1.4064\n",
      "Epoch [10/10], Step [513/625], Loss: 1.4076\n",
      "Epoch [10/10], Step [514/625], Loss: 1.2447\n",
      "Epoch [10/10], Step [515/625], Loss: 1.6504\n",
      "Epoch [10/10], Step [516/625], Loss: 0.9236\n",
      "Epoch [10/10], Step [517/625], Loss: 1.3932\n",
      "Epoch [10/10], Step [518/625], Loss: 1.1556\n",
      "Epoch [10/10], Step [519/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [520/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [521/625], Loss: 1.3115\n",
      "Epoch [10/10], Step [522/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [523/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [524/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [525/625], Loss: 1.3832\n",
      "Epoch [10/10], Step [526/625], Loss: 1.1644\n",
      "Epoch [10/10], Step [527/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [528/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [529/625], Loss: 1.1552\n",
      "Epoch [10/10], Step [530/625], Loss: 1.4092\n",
      "Epoch [10/10], Step [531/625], Loss: 1.2851\n",
      "Epoch [10/10], Step [532/625], Loss: 0.9191\n",
      "Epoch [10/10], Step [533/625], Loss: 1.0069\n",
      "Epoch [10/10], Step [534/625], Loss: 1.4046\n",
      "Epoch [10/10], Step [535/625], Loss: 0.9352\n",
      "Epoch [10/10], Step [536/625], Loss: 1.1522\n",
      "Epoch [10/10], Step [537/625], Loss: 0.9069\n",
      "Epoch [10/10], Step [538/625], Loss: 1.1896\n",
      "Epoch [10/10], Step [539/625], Loss: 1.3707\n",
      "Epoch [10/10], Step [540/625], Loss: 1.1641\n",
      "Epoch [10/10], Step [541/625], Loss: 1.4004\n",
      "Epoch [10/10], Step [542/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [543/625], Loss: 1.1550\n",
      "Epoch [10/10], Step [544/625], Loss: 1.1551\n",
      "Epoch [10/10], Step [545/625], Loss: 1.1228\n",
      "Epoch [10/10], Step [546/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [547/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [548/625], Loss: 1.3896\n",
      "Epoch [10/10], Step [549/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [550/625], Loss: 1.1542\n",
      "Epoch [10/10], Step [551/625], Loss: 0.9188\n",
      "Epoch [10/10], Step [552/625], Loss: 1.1544\n",
      "Epoch [10/10], Step [553/625], Loss: 1.4048\n",
      "Epoch [10/10], Step [554/625], Loss: 1.4029\n",
      "Epoch [10/10], Step [555/625], Loss: 1.1547\n",
      "Epoch [10/10], Step [556/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [557/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [558/625], Loss: 1.6520\n",
      "Epoch [10/10], Step [559/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [560/625], Loss: 1.4008\n",
      "Epoch [10/10], Step [561/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [562/625], Loss: 1.1571\n",
      "Epoch [10/10], Step [563/625], Loss: 1.9042\n",
      "Epoch [10/10], Step [564/625], Loss: 1.6378\n",
      "Epoch [10/10], Step [565/625], Loss: 0.9080\n",
      "Epoch [10/10], Step [566/625], Loss: 1.1876\n",
      "Epoch [10/10], Step [567/625], Loss: 0.9096\n",
      "Epoch [10/10], Step [568/625], Loss: 1.5682\n",
      "Epoch [10/10], Step [569/625], Loss: 1.3883\n",
      "Epoch [10/10], Step [570/625], Loss: 1.6524\n",
      "Epoch [10/10], Step [571/625], Loss: 1.3995\n",
      "Epoch [10/10], Step [572/625], Loss: 1.6511\n",
      "Epoch [10/10], Step [573/625], Loss: 1.1720\n",
      "Epoch [10/10], Step [574/625], Loss: 1.1629\n",
      "Epoch [10/10], Step [575/625], Loss: 1.2324\n",
      "Epoch [10/10], Step [576/625], Loss: 1.2958\n",
      "Epoch [10/10], Step [577/625], Loss: 0.9970\n",
      "Epoch [10/10], Step [578/625], Loss: 1.4391\n",
      "Epoch [10/10], Step [579/625], Loss: 1.4031\n",
      "Epoch [10/10], Step [580/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [581/625], Loss: 1.1549\n",
      "Epoch [10/10], Step [582/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [583/625], Loss: 1.0189\n",
      "Epoch [10/10], Step [584/625], Loss: 1.2506\n",
      "Epoch [10/10], Step [585/625], Loss: 1.6313\n",
      "Epoch [10/10], Step [586/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [587/625], Loss: 1.4037\n",
      "Epoch [10/10], Step [588/625], Loss: 0.9049\n",
      "Epoch [10/10], Step [589/625], Loss: 1.3988\n",
      "Epoch [10/10], Step [590/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [591/625], Loss: 1.3819\n",
      "Epoch [10/10], Step [592/625], Loss: 1.3374\n",
      "Epoch [10/10], Step [593/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [594/625], Loss: 1.4082\n",
      "Epoch [10/10], Step [595/625], Loss: 0.9509\n",
      "Epoch [10/10], Step [596/625], Loss: 1.8887\n",
      "Epoch [10/10], Step [597/625], Loss: 0.9050\n",
      "Epoch [10/10], Step [598/625], Loss: 1.1562\n",
      "Epoch [10/10], Step [599/625], Loss: 1.3717\n",
      "Epoch [10/10], Step [600/625], Loss: 1.6537\n",
      "Epoch [10/10], Step [601/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [602/625], Loss: 1.1541\n",
      "Epoch [10/10], Step [603/625], Loss: 1.6385\n",
      "Epoch [10/10], Step [604/625], Loss: 1.3012\n",
      "Epoch [10/10], Step [605/625], Loss: 1.5954\n",
      "Epoch [10/10], Step [606/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [607/625], Loss: 1.4073\n",
      "Epoch [10/10], Step [608/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [609/625], Loss: 0.9078\n",
      "Epoch [10/10], Step [610/625], Loss: 1.4062\n",
      "Epoch [10/10], Step [611/625], Loss: 0.9088\n",
      "Epoch [10/10], Step [612/625], Loss: 1.1576\n",
      "Epoch [10/10], Step [613/625], Loss: 1.4047\n",
      "Epoch [10/10], Step [614/625], Loss: 1.4042\n",
      "Epoch [10/10], Step [615/625], Loss: 0.9048\n",
      "Epoch [10/10], Step [616/625], Loss: 1.4905\n",
      "Epoch [10/10], Step [617/625], Loss: 1.1548\n",
      "Epoch [10/10], Step [618/625], Loss: 1.6548\n",
      "Epoch [10/10], Step [619/625], Loss: 1.4095\n",
      "Epoch [10/10], Step [620/625], Loss: 1.6497\n",
      "Epoch [10/10], Step [621/625], Loss: 1.6148\n",
      "Epoch [10/10], Step [622/625], Loss: 1.4203\n",
      "Epoch [10/10], Step [623/625], Loss: 1.6244\n",
      "Epoch [10/10], Step [624/625], Loss: 1.9039\n",
      "Epoch [10/10], Step [625/625], Loss: 0.9053\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "figure_2 = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels, coarse_label) in enumerate(train_loader):\n",
    "            epochVec_2 = []\n",
    "            epochLoss_2 = []\n",
    "            \n",
    "            labels[labels == 12] = 0\n",
    "            labels[labels == 17] = 1\n",
    "            labels[labels == 37] = 2\n",
    "            labels[labels == 68] = 3\n",
    "            labels[labels == 76] = 4\n",
    "            \n",
    "            #forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epochLoss_2.append(loss.item())\n",
    "        \n",
    "\n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epochAvg_2 = sum(epochLoss_2)/len(epochLoss_2)\n",
    "    epochVec = [(epoch + 1), epochAvg_2]\n",
    "    figure_2.append(epochVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.        ,  3.        ,  4.        ,  5.        ,\n",
       "         6.        ,  7.        ,  8.        ,  9.        , 10.        ],\n",
       "       [ 1.15502894,  1.39703345,  1.26968288,  1.39878702,  1.40819848,\n",
       "         1.02916515,  1.40441942,  1.15648746,  0.93025935,  0.90533125]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure_2 = np.array(figure_2)\n",
    "figure_2 = figure_2.transpose()\n",
    "figure_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 10.0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax_2 = plt.subplots()\n",
    "\n",
    "ax_2.plot(figure_2[0], figure_2[1])\n",
    "ax_2.set_xlabel(\"Epoch\")\n",
    "ax_2.set_ylabel(\"Loss\")\n",
    "ax_2.set_title(\"Loss by Epoch\")\n",
    "plt.xlim([0,10])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47dc133e8ee860b7ee8fc99469f8ae846d4577bfb47e6172c19a4001a051aace"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('498A': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
